#!/usr/bin/env python3
"""
è„šæœ¬ 04: è®­ç»ƒä¼˜åŒ–çš„ä¸¤é˜¶æ®µæ¼æ´æ£€æµ‹åˆ†ç±»å™¨ã€‚
æ‰§è¡Œæ–¹å¼: åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œ `python scripts/04_train_classifier.py`

æ ¸å¿ƒä¼˜åŒ–: ä»"å…¨ç§‘åŒ»ç”Ÿ"å‡çº§ä¸º"ä¸“ç§‘åŒ»ç”Ÿ"ï¼Œä¸“é—¨è§£å†³"å¯¹æŠ—æ”»å‡» vs é«˜æ–¯å™ªå£°"çš„æ··æ·†é—®é¢˜
"""

import os
import pickle
import pandas as pd
import numpy as np
import yaml
import sys
import glob
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

def clean_data(df):
    """
    æ¸…ç†æ•°æ®ä¸­çš„æ— ç©·å¤§å€¼ã€NaNå€¼å’Œå¼‚å¸¸å€¼
    """
    print("--- æ•°æ®æ¸…ç† ---")
    
    # æ£€æŸ¥åŸå§‹æ•°æ®çŠ¶æ€
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"æ— ç©·å¤§å€¼æ•°é‡: {np.isinf(df.select_dtypes(include=[np.number])).sum().sum()}")
    print(f"NaNå€¼æ•°é‡: {df.isnull().sum().sum()}")
    
    # å¤åˆ¶æ•°æ®æ¡†
    df_clean = df.copy()
    
    # å¤„ç†æ— ç©·å¤§å€¼ï¼šç”¨è¯¥åˆ—çš„æœ€å¤§æœ‰é™å€¼æ›¿æ¢
    numeric_columns = df_clean.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if col == 'vulnerability_type':  # è·³è¿‡æ ‡ç­¾åˆ—
            continue
            
        # è·å–è¯¥åˆ—çš„æœ€å¤§æœ‰é™å€¼
        finite_values = df_clean[col][np.isfinite(df_clean[col])]
        if len(finite_values) > 0:
            max_finite = finite_values.max()
            min_finite = finite_values.min()
            
            # æ›¿æ¢æ­£æ— ç©·å¤§
            df_clean[col] = df_clean[col].replace([np.inf], max_finite * 1.1)
            # æ›¿æ¢è´Ÿæ— ç©·å¤§
            df_clean[col] = df_clean[col].replace([-np.inf], min_finite * 1.1)
    
    # å¤„ç†NaNå€¼ï¼šç”¨è¯¥åˆ—çš„ä¸­ä½æ•°æ›¿æ¢
    for col in numeric_columns:
        if col == 'vulnerability_type':  # è·³è¿‡æ ‡ç­¾åˆ—
            continue
            
        median_val = df_clean[col].median()
        df_clean[col] = df_clean[col].fillna(median_val)
    
    # æ£€æŸ¥æ¸…ç†åçš„æ•°æ®çŠ¶æ€
    print(f"æ¸…ç†åæ•°æ®å½¢çŠ¶: {df_clean.shape}")
    print(f"æ¸…ç†åæ— ç©·å¤§å€¼æ•°é‡: {np.isinf(df_clean.select_dtypes(include=[np.number])).sum().sum()}")
    print(f"æ¸…ç†åNaNå€¼æ•°é‡: {df_clean.isnull().sum().sum()}")
    
    return df_clean

def main():
    print("=== è„šæœ¬ 04: è®­ç»ƒä¼˜åŒ–çš„ä¸¤é˜¶æ®µæ¼æ´æ£€æµ‹åˆ†ç±»å™¨ ===")
    print("æ ¸å¿ƒä¼˜åŒ–: ä¸“ç§‘åŒ»ç”Ÿæ¨¡å¼ - ç²¾å‡†åŒºåˆ†å¯¹æŠ—æ”»å‡»ä¸é«˜æ–¯å™ªå£°")
    
    # --- åŠ è½½é…ç½®æ–‡ä»¶ ---
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    # --- è‡ªåŠ¨å¯»æ‰¾å¹¶åŠ è½½æœ€æ–°çš„æŒ‡çº¹æ•°æ® ---
    runs_dir = config['output_paths']['runs_directory']
    list_of_run_dirs = glob.glob(os.path.join(runs_dir, '*/'))
    if not list_of_run_dirs:
        print("\né”™è¯¯ï¼šåœ¨ 'runs' æ–‡ä»¶å¤¹ä¸‹æ‰¾ä¸åˆ°ä»»ä½•è¿è¡Œè®°å½•ã€‚")
        print("è¯·ç¡®ä¿æ‚¨å·²ç»æˆåŠŸè¿è¡Œäº† '03_extract_fingerprints.py' è„šæœ¬ã€‚")
        sys.exit(1)
    
    latest_run_dir = max(list_of_run_dirs, key=os.path.getctime)
    fingerprints_file_path = os.path.join(latest_run_dir, 'vulnerability_fingerprints.csv')
    print(f"\næ­£åœ¨ä»æœ€æ–°çš„è¿è¡Œè®°å½•ä¸­åŠ è½½æ•°æ®: {fingerprints_file_path}")
    
    try:
        fingerprints_df = pd.read_csv(fingerprints_file_path)
        print(f"æˆåŠŸåŠ è½½ {len(fingerprints_df)} æ¡æŒ‡çº¹æ•°æ®ã€‚")
    except FileNotFoundError:
        print(f"\né”™è¯¯ï¼šåœ¨è·¯å¾„ '{fingerprints_file_path}' ä¸­æ‰¾ä¸åˆ° vulnerability_fingerprints.csv æ–‡ä»¶ã€‚")
        sys.exit(1)
    
    # --- æ•°æ®æ¸…ç† ---
    fingerprints_df = clean_data(fingerprints_df)
    
    # --- æ•°æ®é¢„å¤„ç† ---
    print("\n--- æ•°æ®é¢„å¤„ç† ---")
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    print(f"æ•°æ®ç±»å‹åˆ†å¸ƒ:")
    print(fingerprints_df['vulnerability_type'].value_counts())
    
    # æ£€æŸ¥ç‰¹å¾åˆ—
    feature_columns = [col for col in fingerprints_df.columns if col != 'vulnerability_type']
    print(f"\nå¯ç”¨ç‰¹å¾æ•°é‡: {len(feature_columns)}")
    print(f"ç‰¹å¾åˆ—è¡¨: {feature_columns}")
    
    # --- ç¬¬ä¸€é˜¶æ®µï¼šåˆ†è¯Šå°æ¨¡å‹ (ä¿æŒä¸å˜) ---
    print("\n--- ç¬¬ä¸€é˜¶æ®µï¼šåˆ†è¯Šå°æ¨¡å‹è®­ç»ƒ ---")
    print("ç›®æ ‡ï¼šå¿«é€Ÿåˆ†ç¦»å‚æ•°æ¼‚ç§»æ ·æœ¬")
    
    # å‡†å¤‡ç¬¬ä¸€é˜¶æ®µæ•°æ®
    stage1_data = fingerprints_df.copy()
    stage1_data['is_parameter_drift'] = (stage1_data['vulnerability_type'] == 'drift_parameter').astype(int)
    
    # ç¬¬ä¸€é˜¶æ®µç‰¹å¾ï¼šä½¿ç”¨æ‰€æœ‰ç‰¹å¾
    stage1_features = feature_columns
    X_stage1 = stage1_data[stage1_features]
    y_stage1 = stage1_data['is_parameter_drift']
    
    # è®­ç»ƒç¬¬ä¸€é˜¶æ®µæ¨¡å‹
    model1 = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
    
    # äº¤å‰éªŒè¯è¯„ä¼°
    cv_scores_stage1 = cross_val_score(model1, X_stage1, y_stage1, cv=5)
    print(f"ç¬¬ä¸€é˜¶æ®µäº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores_stage1.mean():.4f} (+/- {cv_scores_stage1.std() * 2:.4f})")
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    model1.fit(X_stage1, y_stage1)
    
    # --- ç¬¬äºŒé˜¶æ®µï¼šä¸“ç§‘ä¸“å®¶ç³»ç»Ÿ (æ ¸å¿ƒä¼˜åŒ–) ---
    print("\n--- ç¬¬äºŒé˜¶æ®µï¼šä¸“ç§‘ä¸“å®¶ç³»ç»Ÿè®­ç»ƒ ---")
    print("ç›®æ ‡ï¼šç²¾å‡†åŒºåˆ†å¯¹æŠ—æ”»å‡»ä¸é«˜æ–¯å™ªå£°")
    
    # å‡†å¤‡ç¬¬äºŒé˜¶æ®µæ•°æ®ï¼šåªåŒ…å«éå‚æ•°æ¼‚ç§»çš„æ ·æœ¬
    stage2_data = fingerprints_df[fingerprints_df['vulnerability_type'] != 'drift_parameter'].copy()
    stage2_data = stage2_data[stage2_data['vulnerability_type'].isin(['adversarial_pgd', 'noise_gaussian'])]
    
    print(f"ç¬¬äºŒé˜¶æ®µæ ·æœ¬æ•°é‡: {len(stage2_data)}")
    print(f"ç¬¬äºŒé˜¶æ®µæ•°æ®ç±»å‹åˆ†å¸ƒ:")
    print(stage2_data['vulnerability_type'].value_counts())
    
    # æ ¸å¿ƒä¼˜åŒ–ï¼šä¸“ç§‘ä¸“å®¶ç‰¹å¾é›†
    # åªä½¿ç”¨å¯¹åŒºåˆ†"å¯¹æŠ—æ”»å‡» vs é«˜æ–¯å™ªå£°"æœ€å…³é”®çš„4ä¸ªç‰¹å¾
    stage2_core_features = [
        'll_distortion',           # æŒ‡çº¹äºŒï¼šä½é¢‘å­å¸¦ç»“æ„å¤±çœŸåº¦
        'ratio_zscore',            # æŒ‡çº¹ä¸‰ï¼šèƒ½é‡æ¯”Z-score
        'super_fingerprint',       # è¶…çº§æŒ‡çº¹ï¼šç»“æ„åŠ æƒèƒ½é‡
        'high_freq_ratio'          # è¾…åŠ©ç‰¹å¾ï¼šé™æ€é«˜é¢‘èƒ½é‡æ¯”
    ]
    
    print(f"\nä¸“ç§‘ä¸“å®¶æ ¸å¿ƒç‰¹å¾é›†:")
    for i, feature in enumerate(stage2_core_features, 1):
        print(f"  {i}. {feature}")
    
    # å‡†å¤‡ç¬¬äºŒé˜¶æ®µæ•°æ®
    X_stage2_core = stage2_data[stage2_core_features]
    y_stage2 = (stage2_data['vulnerability_type'] == 'adversarial_pgd').astype(int)  # 1=å¯¹æŠ—æ”»å‡», 0=é«˜æ–¯å™ªå£°
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_stage2_core_scaled = scaler.fit_transform(X_stage2_core)
    
    # è®­ç»ƒä¸“ç§‘ä¸“å®¶æ¨¡å‹
    specialist_model = RandomForestClassifier(
        n_estimators=200,          # å¢åŠ æ ‘çš„æ•°é‡
        max_depth=15,              # é™åˆ¶æ·±åº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        min_samples_split=10,      # å¢åŠ åˆ†è£‚é˜ˆå€¼
        min_samples_leaf=5,        # å¢åŠ å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
        random_state=42,
        n_jobs=-1
    )
    
    # äº¤å‰éªŒè¯è¯„ä¼°ä¸“ç§‘ä¸“å®¶
    cv_scores_specialist = cross_val_score(specialist_model, X_stage2_core_scaled, y_stage2, cv=5)
    print(f"\nä¸“ç§‘ä¸“å®¶äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores_specialist.mean():.4f} (+/- {cv_scores_specialist.std() * 2:.4f})")
    
    # è®­ç»ƒæœ€ç»ˆä¸“ç§‘ä¸“å®¶æ¨¡å‹
    specialist_model.fit(X_stage2_core_scaled, y_stage2)
    
    # --- å¯¹æ¯”è¯„ä¼°ï¼šåŸæœ‰ä¸“å®¶ç»„ç³»ç»Ÿ ---
    print("\n--- å¯¹æ¯”è¯„ä¼°ï¼šåŸæœ‰ä¸“å®¶ç»„ç³»ç»Ÿ ---")
    print("ç›®æ ‡ï¼šé‡åŒ–ä¸“ç§‘ä¸“å®¶çš„æ€§èƒ½æå‡")
    
    # åŸæœ‰ä¸“å®¶ç»„ä½¿ç”¨æ‰€æœ‰ç‰¹å¾
    X_stage2_all = stage2_data[feature_columns]
    X_stage2_all_scaled = scaler.fit_transform(X_stage2_all)
    
    # è®­ç»ƒåŸæœ‰ä¸“å®¶ç»„æ¨¡å‹
    original_expert_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
    
    # äº¤å‰éªŒè¯è¯„ä¼°åŸæœ‰ç³»ç»Ÿ
    cv_scores_original = cross_val_score(original_expert_model, X_stage2_all_scaled, y_stage2, cv=5)
    print(f"åŸæœ‰ä¸“å®¶ç»„äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores_original.mean():.4f} (+/- {cv_scores_original.std() * 2:.4f})")
    
    # è®­ç»ƒæœ€ç»ˆåŸæœ‰ä¸“å®¶ç»„æ¨¡å‹
    original_expert_model.fit(X_stage2_all_scaled, y_stage2)
    
    # --- æ€§èƒ½å¯¹æ¯”åˆ†æ ---
    print("\n--- æ€§èƒ½å¯¹æ¯”åˆ†æ ---")
    
    # è®¡ç®—æ€§èƒ½æå‡
    accuracy_improvement = cv_scores_specialist.mean() - cv_scores_original.mean()
    print(f"ä¸“ç§‘ä¸“å®¶ vs åŸæœ‰ä¸“å®¶ç»„:")
    print(f"  å‡†ç¡®ç‡æå‡: {accuracy_improvement:.4f}")
    print(f"  ç›¸å¯¹æå‡: {(accuracy_improvement / cv_scores_original.mean() * 100):.2f}%")
    
    # --- ç‰¹å¾é‡è¦æ€§åˆ†æ ---
    print("\n--- ä¸“ç§‘ä¸“å®¶ç‰¹å¾é‡è¦æ€§åˆ†æ ---")
    
    feature_importance = pd.DataFrame({
        'feature': stage2_core_features,
        'importance': specialist_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("ç‰¹å¾é‡è¦æ€§æ’åº:")
    for i, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")
    
    # --- æ¨¡å‹ä¿å­˜ ---
    print("\n--- ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ ---")
    
    models_output_dir = os.path.join(latest_run_dir, 'trained_models')
    os.makedirs(models_output_dir, exist_ok=True)
    
    # ä¿å­˜ç¬¬ä¸€é˜¶æ®µæ¨¡å‹
    with open(os.path.join(models_output_dir, 'stage1_model.pkl'), 'wb') as f:
        pickle.dump(model1, f)
    
    # ä¿å­˜ä¸“ç§‘ä¸“å®¶æ¨¡å‹
    with open(os.path.join(models_output_dir, 'specialist_expert.pkl'), 'wb') as f:
        pickle.dump(specialist_model, f)
    
    # ä¿å­˜åŸæœ‰ä¸“å®¶ç»„æ¨¡å‹
    with open(os.path.join(models_output_dir, 'original_expert.pkl'), 'wb') as f:
        pickle.dump(original_expert_model, f)
    
    # ä¿å­˜æ•°æ®æ ‡å‡†åŒ–å™¨
    with open(os.path.join(models_output_dir, 'scaler.pkl'), 'wb') as f:
        pickle.dump(scaler, f)
    
    # ä¿å­˜ç‰¹å¾é…ç½®
    model_config = {
        'stage1_features': stage1_features,
        'stage2_core_features': stage2_core_features,
        'stage2_all_features': feature_columns,
        'training_date': datetime.now().isoformat(),
        'data_samples': len(fingerprints_df),
        'stage2_samples': len(stage2_data)
    }
    
    with open(os.path.join(models_output_dir, 'model_config.yaml'), 'w', encoding='utf-8') as f:
        yaml.dump(model_config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ°: {models_output_dir}")
    
    # --- ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š ---
    print("\n--- ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š ---")
    
    # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ä¸“ç§‘ä¸“å®¶ vs åŸæœ‰ä¸“å®¶ç»„æ€§èƒ½å¯¹æ¯”', fontsize=16)
    
    # 1. äº¤å‰éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”
    models = ['åŸæœ‰ä¸“å®¶ç»„', 'ä¸“ç§‘ä¸“å®¶']
    cv_means = [cv_scores_original.mean(), cv_scores_specialist.mean()]
    cv_stds = [cv_scores_original.std(), cv_scores_specialist.std()]
    
    axes[0, 0].bar(models, cv_means, yerr=cv_stds, capsize=5, alpha=0.7)
    axes[0, 0].set_title('äº¤å‰éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”')
    axes[0, 0].set_ylabel('å‡†ç¡®ç‡')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ç‰¹å¾é‡è¦æ€§
    axes[0, 1].barh(feature_importance['feature'], feature_importance['importance'])
    axes[0, 1].set_title('ä¸“ç§‘ä¸“å®¶ç‰¹å¾é‡è¦æ€§')
    axes[0, 1].set_xlabel('é‡è¦æ€§')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. ä¸“ç§‘ä¸“å®¶ç‰¹å¾åˆ†å¸ƒ
    for feature in stage2_core_features:
        axes[1, 0].hist(stage2_data[stage2_data['vulnerability_type'] == 'adversarial_pgd'][feature], 
                        alpha=0.5, label='å¯¹æŠ—æ”»å‡»', bins=30)
        axes[1, 0].hist(stage2_data[stage2_data['vulnerability_type'] == 'noise_gaussian'][feature], 
                        alpha=0.5, label='é«˜æ–¯å™ªå£°', bins=30)
    axes[1, 0].set_title('æ ¸å¿ƒç‰¹å¾åˆ†å¸ƒå¯¹æ¯”')
    axes[1, 0].set_xlabel('ç‰¹å¾å€¼')
    axes[1, 0].set_ylabel('é¢‘æ¬¡')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. æ€§èƒ½æå‡å¯è§†åŒ–
    improvement_data = {
        'åŸæœ‰ä¸“å®¶ç»„': cv_scores_original.mean(),
        'ä¸“ç§‘ä¸“å®¶': cv_scores_specialist.mean()
    }
    axes[1, 1].pie(improvement_data.values(), labels=improvement_data.keys(), autopct='%1.1f%%')
    axes[1, 1].set_title('æ€§èƒ½å¯¹æ¯”')
    
    plt.tight_layout()
    
    # ä¿å­˜æ€§èƒ½æŠ¥å‘Š
    performance_report_path = os.path.join(latest_run_dir, 'performance_comparison.png')
    plt.savefig(performance_report_path, dpi=300, bbox_inches='tight')
    print(f"æ€§èƒ½å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {performance_report_path}")
    
    # --- æœ€ç»ˆæ€»ç»“ ---
    print("\n" + "="*60)
    print("ğŸ¯ ä¸“ç§‘ä¸“å®¶ç³»ç»Ÿè®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“Š æ€§èƒ½æå‡:")
    print(f"   - åŸæœ‰ä¸“å®¶ç»„å‡†ç¡®ç‡: {cv_scores_original.mean():.4f}")
    print(f"   - ä¸“ç§‘ä¸“å®¶å‡†ç¡®ç‡: {cv_scores_specialist.mean():.4f}")
    print(f"   - ç»å¯¹æå‡: {accuracy_improvement:.4f}")
    print(f"   - ç›¸å¯¹æå‡: {(accuracy_improvement / cv_scores_original.mean() * 100):.2f}%")
    
    print(f"\n æ ¸å¿ƒç‰¹å¾:")
    for i, feature in enumerate(stage2_core_features, 1):
        importance = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]
        print(f"   {i}. {feature}: {importance:.4f}")
    
    print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜ä½ç½®: {models_output_dir}")
    print(f" æ€§èƒ½æŠ¥å‘Š: {performance_report_path}")
    
    print("\n ç³»ç»Ÿå·²æˆåŠŸä»'å…¨ç§‘åŒ»ç”Ÿ'å‡çº§ä¸º'ä¸“ç§‘åŒ»ç”Ÿ'ï¼")
    print("   ç°åœ¨å¯ä»¥ç²¾å‡†åŒºåˆ†å¯¹æŠ—æ”»å‡»ä¸é«˜æ–¯å™ªå£°äº†ï¼")

if __name__ == '__main__':
    main()
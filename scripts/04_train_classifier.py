# scripts/04_train_classifier.py
#!/usr/bin/env python3
"""
è„šæœ¬ 04: è®­ç»ƒä¸¤é˜¶æ®µåˆ†ç±»å™¨å¹¶è¯„ä¼°æ€§èƒ½ã€‚
æ‰§è¡Œæ–¹å¼: åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œ `python scripts/04_train_classifier.py`
"""
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import glob
import yaml
import sys

# ä» scikit-learn ä¸­å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„æ¨¡å—
from sklearn.model_selection import train_test_split, cross_val_predict
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

def auto_tune_thresholds(energy_proba, X_test_experts, features_arbitrator, 
                        structural_arbitrator, y_test_full, non_drift_test_mask, 
                        adv_label, noise_label, pred1_test, 
                        label_encoder, label_encoder2):
    """
    è‡ªåŠ¨è°ƒå‚å‡½æ•°ï¼šå¯»æ‰¾æœ€ä½³çš„èƒ½é‡åˆç­›å’Œä»²è£é˜ˆå€¼ç»„åˆ
    """
    print("ï¿½ï¿½ å¼€å§‹è‡ªåŠ¨è°ƒå‚ï¼Œæµ‹è¯•ä¸åŒé˜ˆå€¼ç»„åˆ...")
    
    # å®šä¹‰é˜ˆå€¼æœç´¢èŒƒå›´
    energy_low_range = [0.20, 0.25, 0.30, 0.35, 0.40]
    energy_high_range = [0.60, 0.65, 0.70, 0.75, 0.80]
    arbit_range = [0.45, 0.50, 0.55, 0.60, 0.65]
    
    best_score = 0
    best_params = {}
    results = []
    
    total_combinations = len(energy_low_range) * len(energy_high_range) * len(arbit_range)
    current = 0
    
    for energy_low in energy_low_range:
        for energy_high in energy_high_range:
            if energy_low >= energy_high:  # è·³è¿‡æ— æ•ˆç»„åˆ
                continue
            for arbit_thresh in arbit_range:
                current += 1
                print(f"æµ‹è¯•ç»„åˆ {current}/{total_combinations}: "
                      f"èƒ½é‡ä½={energy_low:.2f}, èƒ½é‡é«˜={energy_high:.2f}, ä»²è£={arbit_thresh:.2f}")
                
                # ä½¿ç”¨å½“å‰é˜ˆå€¼ç»„åˆè¿›è¡Œé¢„æµ‹
                final_predictions = np.zeros_like(y_test_full)
                
                for i, energy_proba_sample in enumerate(energy_proba):
                    prob_attack = energy_proba_sample[adv_label]
                    
                    if prob_attack <= energy_low:
                        final_predictions[non_drift_test_mask][i] = noise_label
                    elif prob_attack >= energy_high:
                        final_predictions[non_drift_test_mask][i] = adv_label
                    else:
                        # è§¦å‘ä»²è£
                        sample_features = X_test_experts.iloc[i:i+1][features_arbitrator]
                        arbitrator_proba = structural_arbitrator.predict_proba(sample_features)[0]
                        prob_attack_arb = arbitrator_proba[adv_label]
                        
                        if prob_attack_arb >= arbit_thresh:
                            final_predictions[non_drift_test_mask][i] = adv_label
                        else:
                            final_predictions[non_drift_test_mask][i] = noise_label
                
                # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                accuracy = accuracy_score(y_test_full, final_predictions)
                
                # è®¡ç®—å¯¹æŠ—æ”»å‡»çš„å¬å›ç‡ï¼ˆé˜²æ­¢æ¼æ£€ï¼‰
                cm = confusion_matrix(y_test_full, final_predictions)
                if len(cm) >= 3:  # ç¡®ä¿æœ‰3ä¸ªç±»åˆ«
                    # å‡è®¾å¯¹æŠ—æ”»å‡»æ˜¯ç¬¬ä¸€ä¸ªç±»åˆ«ï¼ˆç´¢å¼•0ï¼‰
                    adv_recall = cm[0, 0] / cm[0, :].sum() if cm[0, :].sum() > 0 else 0
                    # è®¡ç®—é«˜æ–¯å™ªå£°çš„å¬å›ç‡
                    noise_recall = cm[2, 2] / cm[2, :].sum() if cm[2, :].sum() > 0 else 0
                    
                    # ç»¼åˆè¯„åˆ†ï¼šå‡†ç¡®ç‡ + å¯¹æŠ—å¬å›ç‡ + å™ªå£°å¬å›ç‡
                    combined_score = accuracy + adv_recall + noise_recall
                    
                    results.append({
                        'energy_low': energy_low,
                        'energy_high': energy_high,
                        'arbit_thresh': arbit_thresh,
                        'accuracy': accuracy,
                        'adv_recall': adv_recall,
                        'noise_recall': noise_recall,
                        'combined_score': combined_score
                    })
                    
                    if combined_score > best_score:
                        best_score = combined_score
                        best_params = {
                            'energy_low': energy_low,
                            'energy_high': energy_high,
                            'arbit_attack': arbit_thresh
                        }
                        print(f"ğŸ¯ å‘ç°æ›´å¥½çš„å‚æ•°ç»„åˆï¼ç»¼åˆè¯„åˆ†: {combined_score:.4f}")
    
    # æ˜¾ç¤ºæœ€ä½³ç»“æœ
    print(f"\nğŸ† è‡ªåŠ¨è°ƒå‚å®Œæˆï¼æœ€ä½³å‚æ•°ç»„åˆ:")
    print(f"   èƒ½é‡ä½é˜ˆå€¼: {best_params['energy_low']:.2f}")
    print(f"   èƒ½é‡é«˜é˜ˆå€¼: {best_params['energy_high']:.2f}")
    print(f"   ä»²è£é˜ˆå€¼: {best_params['arbit_attack']:.2f}")
    print(f"   æœ€ä½³ç»¼åˆè¯„åˆ†: {best_score:.4f}")
    
    # æ˜¾ç¤ºå‰5ä¸ªæœ€ä½³ç»“æœ
    results.sort(key=lambda x: x['combined_score'], reverse=True)
    print(f"\nğŸ“Š å‰5ä¸ªæœ€ä½³å‚æ•°ç»„åˆ:")
    for i, result in enumerate(results[:5]):
        print(f"   {i+1}. èƒ½é‡ä½={result['energy_low']:.2f}, "
              f"èƒ½é‡é«˜={result['energy_high']:.2f}, "
              f"ä»²è£={result['arbit_thresh']:.2f}, "
              f"ç»¼åˆè¯„åˆ†={result['combined_score']:.4f}")
    
    return best_params

# ... existing code ...

def main():
    print("=== è„šæœ¬ 04: è®­ç»ƒåˆ†ç±»å™¨ ===")

    # --- åŠ è½½é…ç½®æ–‡ä»¶ ---
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    print("æ‰€æœ‰æœºå™¨å­¦ä¹ åº“éƒ½å·²æˆåŠŸå¯¼å…¥ï¼")
    print("ä¸¤é˜¶æ®µåˆ†ç±»å™¨è®­ç»ƒè„šæœ¬å·²å‡†å¤‡å°±ç»ªã€‚")

    # =============================================================================
    # æ­¥éª¤ä¸€ï¼šæ•°æ®åŠ è½½ä¸é€šç”¨é¢„å¤„ç†
    # =============================================================================
    # --- 1. åªä»åŒ…å« fingerprint çš„ç›®å½•é€‰æ‹©è¾“å…¥ ---
    runs_dir = config['output_paths']['runs_directory']
    candidate_csvs = glob.glob(os.path.join(runs_dir, '*/vulnerability_fingerprints.csv'))
    if not candidate_csvs:
        print("\né”™è¯¯ï¼šåœ¨ 'runs' ä¸‹æ‰¾ä¸åˆ°ä»»ä½• vulnerability_fingerprints.csvã€‚")
        sys.exit(1)

    fingerprint_file_path = max(candidate_csvs, key=os.path.getctime)  # æœ€æ–°çš„é‚£ä¸ª csv
    latest_run_dir = os.path.dirname(fingerprint_file_path)            # å…¶æ‰€åœ¨ç›®å½•
    print(f"\næ­£åœ¨ä»æœ€æ–°çš„æ•°æ®è¿è¡Œç›®å½•åŠ è½½: {fingerprint_file_path}")

    try:
        data = pd.read_csv(fingerprint_file_path)
        print(f"æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ ·æœ¬ã€‚")
    except FileNotFoundError:
        print(f"\né”™è¯¯ï¼šåœ¨è·¯å¾„ '{fingerprint_file_path}' ä¸­æ‰¾ä¸åˆ° vulnerability_fingerprints.csv æ–‡ä»¶ã€‚")
        sys.exit(1)
    
    # --- åˆ›å»ºæ–°çš„ç»“æœä¿å­˜æ–‡ä»¶å¤¹ ---
    from datetime import datetime
    current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    new_results_dir = os.path.join(runs_dir, f"classifier_results_{current_time}")
    os.makedirs(new_results_dir, exist_ok=True)
    print(f"\nåˆ›å»ºæ–°çš„ç»“æœä¿å­˜æ–‡ä»¶å¤¹: {new_results_dir}")

    # --- 2. åˆ†ç¦»ç‰¹å¾ (X) ä¸åŸå§‹æ ‡ç­¾ (y) ---
    # ... (æ­¤éƒ¨åˆ†ä»£ç ä¸åŸè„šæœ¬å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨) ...
    X = data.drop('vulnerability_type', axis=1)
    y_str = data['vulnerability_type']
    # --- 3. æ ‡ç­¾ç¼–ç  ---
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y_str)
    label_mapping = {i: class_name for i, class_name in enumerate(label_encoder.classes_)}
    print("\næ ‡ç­¾å·²æˆåŠŸç¼–ç ä¸ºæ•°å­—:")
    print(label_mapping)
    # --- 4. åˆ’åˆ†æ€»æ•°æ®é›† ---
    X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    # --- 5. å…¨å±€é¢„å¤„ç†ï¼šå¤„ç†æ— ç©·å¤§å€¼ ---
    print("\næ­£åœ¨å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œæ— ç©·å¤§å€¼é¢„å¤„ç†...")
    X_train_full.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_test_full.replace([np.inf, -np.inf], np.nan, inplace=True)
    print("é¢„å¤„ç†å®Œæˆã€‚")

    # =============================================================================
    # é˜¶æ®µä¸€ï¼šè®­ç»ƒæ¨¡å‹ä¸€ (â€œæ¼‚ç§»â€è¯†åˆ«å™¨ - Generalist)
    # =============================================================================
    # ... (æ­¤é˜¶æ®µä»£ç ä¸åŸè„šæœ¬å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨) ...
    print("\n" + "="*50)
    print("é˜¶æ®µä¸€ï¼šå¼€å§‹è®­ç»ƒæ¨¡å‹ä¸€ ('æ¼‚ç§»'è¯†åˆ«å™¨)")
    print("="*50)
    drift_label_encoded = list(label_encoder.classes_).index('drift_parameter')
    y_train1 = np.where(y_train_full == drift_label_encoded, 1, 0)
    X_train1 = X_train_full.copy()
    imputer1 = SimpleImputer(strategy='median')
    X_train1_imputed = imputer1.fit_transform(X_train1)
    scaler1 = StandardScaler()
    X_train1_scaled = scaler1.fit_transform(X_train1_imputed)
    model1 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    model1.fit(X_train1_scaled, y_train1)
    print("æ¨¡å‹ä¸€è®­ç»ƒå®Œæˆã€‚")
    # --- å¿«é€ŸéªŒè¯æ¨¡å‹ä¸€åœ¨è®­ç»ƒé›†ä¸Šçš„æ€§èƒ½ ---
    from sklearn.metrics import classification_report
    pred1_train = model1.predict(X_train1_scaled)
    print("\n--- æ¨¡å‹ä¸€ (åˆ†è¯Šå°) åœ¨è®­ç»ƒé›†ä¸Šçš„æ€§èƒ½æŠ¥å‘Š ---")
    print(classification_report(y_train1, pred1_train, target_names=['éæ¼‚ç§» (0)', 'æ¼‚ç§» (1)']))

    # =============================================================================
    # é˜¶æ®µäºŒï¼šæ„å»º"ä¸€ç¥¨å¦å†³"ä»²è£ä¸“å®¶æ¨¡å‹æ¶æ„
    # =============================================================================
    print("\n" + "="*50)
    print("é˜¶æ®µäºŒï¼šå¼€å§‹æ„å»º'ä¸€ç¥¨å¦å†³'ä»²è£ä¸“å®¶æ¨¡å‹æ¶æ„")
    print("="*50)
    non_drift_train_mask = (y_train1 == 0)
    X_train_experts = X_train_full[non_drift_train_mask]
    y_train_experts_raw = y_train_full[non_drift_train_mask]
    label_encoder2 = LabelEncoder()
    y_train_experts = label_encoder2.fit_transform(y_train_experts_raw)
    print(f"å·²ç­›é€‰å‡º {len(X_train_experts)} ä¸ª'å¯¹æŠ— vs. å™ªå£°'æ ·æœ¬ï¼Œäº¤ç”±ä»²è£ä¸“å®¶å¤„ç†ã€‚")
    
    # é˜¶æ®µ 2aï¼šä¸“å®¶æ¨¡å‹çš„ä¸“é¡¹åŒ–ä¸é‡å®šä¹‰
    # =============================================================================
    print("\n--- é˜¶æ®µ 2aï¼šä¸“å®¶æ¨¡å‹çš„ä¸“é¡¹åŒ–ä¸é‡å®šä¹‰ ---")
    # 1. å®šä¹‰"èƒ½é‡æ´¾"åˆç­›ä¸“å®¶ (energy_screener)
    # ä½¿å‘½ï¼šæ‹…ä»»ç¬¬ä¸€é“é˜²çº¿ï¼Œæˆä¸ºä¸€ä¸ªé«˜çµæ•åº¦çš„"å¼‚å¸¸ä¿¡å·æ¢æµ‹å™¨"
    # ç†è®ºä¾æ®ï¼šç‰¹å¾é‡è¦æ€§åˆ†ææ˜ç¡®æŒ‡å‡ºï¼Œhigh_freq_ratio, std_dev_diff, ratio_zscore æ˜¯æœ€å¼ºå¤§çš„ä¿¡å·æ¢æµ‹å™¨
    features_energy_screener = [
        'high_freq_ratio',      # æ•æ‰é«˜æ–¯å™ªå£°çš„é«˜é¢‘æ•£å¸ƒç‰¹æ€§
        'std_dev_diff',         # è¡¡é‡æ³¨æ„åŠ›åˆ†æ•£/é›†ä¸­çš„å˜åŒ–
        'ratio_zscore'          # èƒ½é‡æ¯”çš„åŸºçº¿åˆ†ç¦»åº¦ (Z-score)
    ]

    # 2. å®šä¹‰"ç»“æ„æ´¾"ä»²è£ä¸“å®¶ (structural_arbitrator)
    # ä½¿å‘½ï¼šæ‹…ä»»ç¬¬äºŒé“é˜²çº¿ï¼Œæˆä¸ºä¸€ä¸ªé«˜ç²¾åº¦çš„"äº‹å®æ ¸æŸ¥å®˜"
    # ç†è®ºä¾æ®ï¼šll_distortion ç­‰ç»“æ„æ€§ç‰¹å¾åœ¨å½“å‰æ¨¡å‹ä¸­è¢«ä¸¥é‡ä½ä¼°ï¼Œéœ€è¦ä¸“é—¨æ¨¡å‹å¼ºåˆ¶åˆ©ç”¨
    features_structural_arbitrator = [
        'll_distortion',        # æ ¸å¿ƒï¼šä½é¢‘å­å¸¦ç»“æ„å¤±çœŸåº¦
        'contrast',             # å¯¹æ¯”åº¦
        'correlation',          # ç›¸å…³æ€§
        'homogeneity',          # åŒè´¨æ€§
        'energy',               # èƒ½é‡
        'cosine_similarity'     # ä½™å¼¦ç›¸ä¼¼åº¦
    ]
    
    print(f"èƒ½é‡æ´¾åˆç­›ä¸“å®¶ç‰¹å¾é›†: {features_energy_screener}")
    print(f"ç»“æ„æ´¾ä»²è£ä¸“å®¶ç‰¹å¾é›†: {features_structural_arbitrator}")
    
    # æ•°æ®é¢„å¤„ç†
    X_train_experts_imputed = imputer1.transform(X_train_experts)
    X_train_experts_scaled = scaler1.transform(X_train_experts_imputed)
    X_train_experts_scaled_df = pd.DataFrame(X_train_experts_scaled, columns=X_train_experts.columns, index=X_train_experts.index)
    
    # è®­ç»ƒä¸¤ä¸ªä¸“é—¨çš„ä¸“å®¶æ¨¡å‹
    # ä½¿ç”¨ç±»æƒé‡ï¼Œç¼“è§£ç±»åˆ«ä¸å¹³è¡¡
    energy_screener = RandomForestClassifier(
        n_estimators=200, random_state=42, n_jobs=-1, class_weight='balanced_subsample'
    )
    structural_arbitrator = RandomForestClassifier(
        n_estimators=200, random_state=42, n_jobs=-1, class_weight='balanced_subsample'
    )
    
    # è®­ç»ƒèƒ½é‡æ´¾åˆç­›ä¸“å®¶
    print("\næ­£åœ¨è®­ç»ƒèƒ½é‡æ´¾åˆç­›ä¸“å®¶...")
    energy_screener.fit(X_train_experts_scaled_df[features_energy_screener], y_train_experts)
    
    # è®­ç»ƒç»“æ„æ´¾ä»²è£ä¸“å®¶
    print("æ­£åœ¨è®­ç»ƒç»“æ„æ´¾ä»²è£ä¸“å®¶...")
    structural_arbitrator.fit(X_train_experts_scaled_df[features_structural_arbitrator], y_train_experts)
    
    print("ä¸¤ä¸ªä¸“å®¶æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    # é˜¶æ®µ 2bï¼šéªŒè¯ä»²è£ä¸“å®¶æ€§èƒ½
    # =============================================================================
    print("\n--- é˜¶æ®µ 2bï¼šéªŒè¯ä»²è£ä¸“å®¶æ€§èƒ½ ---")
    
    # éªŒè¯èƒ½é‡æ´¾åˆç­›ä¸“å®¶æ€§èƒ½
    energy_screener_train_pred = energy_screener.predict(X_train_experts_scaled_df[features_energy_screener])
    energy_screener_accuracy = accuracy_score(y_train_experts, energy_screener_train_pred)
    print(f"èƒ½é‡æ´¾åˆç­›ä¸“å®¶è®­ç»ƒé›†å‡†ç¡®ç‡: {energy_screener_accuracy:.4f}")
    
    # éªŒè¯ç»“æ„æ´¾ä»²è£ä¸“å®¶æ€§èƒ½
    structural_arbitrator_train_pred = structural_arbitrator.predict(X_train_experts_scaled_df[features_structural_arbitrator])
    structural_arbitrator_accuracy = accuracy_score(y_train_experts, structural_arbitrator_train_pred)
    print(f"ç»“æ„æ´¾ä»²è£ä¸“å®¶è®­ç»ƒé›†å‡†ç¡®ç‡: {structural_arbitrator_accuracy:.4f}")
    
    print("ä»²è£ä¸“å®¶æ€§èƒ½éªŒè¯å®Œæˆï¼")

    # =============================================================================
    # æ–°å¢ï¼šç‰¹å¾é‡è¦æ€§å®¡è®¯ä¸åˆ†æ
    # =============================================================================
    print("\n" + "="*50)
    print("ç‰¹å¾é‡è¦æ€§å®¡è®¯ä¸åˆ†æé˜¶æ®µ")
    print("="*50)
    
    # ç¬¬ä¸€é˜¶æ®µï¼šè¯æ®æ”¶é›† - å®¡è®¯å‡†å¤‡ä¸æ‰§è¡Œ
    print("\n--- ç¬¬ä¸€é˜¶æ®µï¼šè¯æ®æ”¶é›† ---")
    
    # 1.1 å®šä½å®¡è®¯ç›®æ ‡ï¼šä¸¤ä¸ªä»²è£ä¸“å®¶æ¨¡å‹
    experts = {
        'energy_screener': (energy_screener, features_energy_screener, 'èƒ½é‡æ´¾åˆç­›ä¸“å®¶'),
        'structural_arbitrator': (structural_arbitrator, features_structural_arbitrator, 'ç»“æ„æ´¾ä»²è£ä¸“å®¶')
    }
    
    # 1.2 å®¡è®¯å®æ–½ï¼šæå–æ¯ä¸ªä¸“å®¶çš„"å£ä¾›"
    feature_importance_data = []
    
    for expert_name, (expert_model, feature_list, expert_title) in experts.items():
        print(f"\næ­£åœ¨å®¡è®¯ {expert_title} ({expert_name})...")
        
        # è·å–ç‰¹å¾é‡è¦æ€§åˆ†æ•°
        importance_scores = expert_model.feature_importances_
        
        # å°†ç‰¹å¾åä¸é‡è¦æ€§åˆ†æ•°é…å¯¹
        for feature_name, importance_score in zip(feature_list, importance_scores):
            feature_importance_data.append({
                'Expert_Name': expert_title,
                'Expert_Code': expert_name,
                'Feature_Name': feature_name,
                'Importance_Score': importance_score
            })
    
    # 1.3 æ•´ç†æ¡£æ¡ˆï¼šåˆ›å»ºç»“æ„åŒ–æ•°æ®è¡¨
    importance_df = pd.DataFrame(feature_importance_data)
    print(f"\nå®¡è®¯å®Œæˆï¼å…±æ”¶é›†åˆ° {len(importance_df)} æ¡ç‰¹å¾é‡è¦æ€§è®°å½•ã€‚")
    
    # ç¬¬äºŒé˜¶æ®µï¼šæ¡ˆæƒ…åˆ†æ - "å£ä¾›"è§£è¯»ä¸æ´å¯Ÿ
    print("\n--- ç¬¬äºŒé˜¶æ®µï¼šæ¡ˆæƒ…åˆ†æ ---")
    
    # 2.1 å±‚é¢ä¸€ï¼šä¸“å®¶ç‹¬ç«‹åˆ†ææŠ¥å‘Š
    print("\nã€ä¸“å®¶ç‹¬ç«‹åˆ†ææŠ¥å‘Šã€‘")
    for expert_title in importance_df['Expert_Name'].unique():
        expert_data = importance_df[importance_df['Expert_Name'] == expert_title]
        expert_data_sorted = expert_data.sort_values('Importance_Score', ascending=False)
        
        print(f"\n{expert_title}çš„ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ:")
        for idx, row in expert_data_sorted.iterrows():
            print(f"  {row['Feature_Name']}: {row['Importance_Score']:.4f}")
    
    # 2.2 å±‚é¢äºŒï¼šå…¨å±€åŠŸåŠ³æ’è¡Œæ¦œ
    print("\nã€å…¨å±€åŠŸåŠ³æ’è¡Œæ¦œã€‘")
    global_importance = importance_df.groupby('Feature_Name')['Importance_Score'].agg(['sum', 'mean', 'count']).reset_index()
    global_importance.columns = ['Feature_Name', 'Total_Importance', 'Average_Importance', 'Expert_Count']
    global_importance = global_importance.sort_values('Total_Importance', ascending=False)
    
    print("\næ‰€æœ‰ç‰¹å¾çš„å…¨å±€æ€»é‡è¦æ€§æ’è¡Œæ¦œ (æŒ‰æ€»è´¡çŒ®åº¦æ’åº):")
    for idx, row in global_importance.iterrows():
        print(f"  {row['Feature_Name']}: æ€»è´¡çŒ®åº¦={row['Total_Importance']:.4f}, "
              f"å¹³å‡è´¡çŒ®åº¦={row['Average_Importance']:.4f}, "
              f"è¢«{row['Expert_Count']}ä¸ªä¸“å®¶ä½¿ç”¨")
    
    # è¯†åˆ«MVPç‰¹å¾å’Œå¹²æ‰°é¡¹
    print("\nã€å…³é”®æ´å¯Ÿã€‘")
    top_features = global_importance.head(3)
    bottom_features = global_importance.tail(3)
    
    print("\nğŸ† MVPç‰¹å¾ (Top 3):")
    for idx, row in top_features.iterrows():
        print(f"  {row['Feature_Name']}: {row['Total_Importance']:.4f}")
    
    print("\nâš ï¸  æ½œåœ¨å¹²æ‰°é¡¹ (Bottom 3):")
    for idx, row in bottom_features.iterrows():
        print(f"  {row['Feature_Name']}: {row['Total_Importance']:.4f}")
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šåˆ¶å®šè¡ŒåŠ¨ - åŸºäºè¯æ®çš„ä¼˜åŒ–å†³ç­–
    print("\n--- ç¬¬ä¸‰é˜¶æ®µï¼šåˆ¶å®šè¡ŒåŠ¨ ---")
    
    # 3.1 åŸºäºç‰¹å¾æ’åçš„å†³ç­–çŸ©é˜µ
    print("\nã€åŸºäºè¯æ®çš„ä¼˜åŒ–å†³ç­–å»ºè®®ã€‘")
    
    # æ£€æŸ¥å…³é”®ç‰¹å¾çš„è¡¨ç°
    dynamic_wavelet_rank = global_importance[global_importance['Feature_Name'] == 'dynamic_wavelet_ratio_change']
    ll_distortion_rank = global_importance[global_importance['Feature_Name'] == 'll_distortion']
    
    if not dynamic_wavelet_rank.empty:
        dynamic_score = dynamic_wavelet_rank.iloc[0]['Total_Importance']
        if dynamic_score > global_importance['Total_Importance'].median():
            print("âœ… dynamic_wavelet_ratio_change å…¨å±€æ’åå¾ˆé«˜ - ä¿ç•™è¯¥ç‰¹å¾ï¼Œå®ƒæ˜¯å…³é”®æ”¯æŸ±ï¼")
        else:
            print("âš ï¸  dynamic_wavelet_ratio_change è¡¨ç°ä¸€èˆ¬ - è€ƒè™‘ä¼˜åŒ–æˆ–å¢å¼º")
    
    if not ll_distortion_rank.empty:
        ll_score = ll_distortion_rank.iloc[0]['Total_Importance']
        if ll_score > global_importance['Total_Importance'].median():
            print("âœ… ll_distortion å…¨å±€æ’åå¾ˆé«˜ - æ‚¨çš„'ç»“æ„å®¡è®¡å¸ˆ'æŒ‡çº¹éå¸¸æˆåŠŸï¼")
        else:
            print("âš ï¸  ll_distortion è¡¨ç°ä¸€èˆ¬ - éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # è¯†åˆ«éœ€è¦ç§»é™¤çš„æ½œåœ¨å™ªéŸ³ç‰¹å¾
    noise_threshold = global_importance['Total_Importance'].quantile(0.25)  # ä¸‹å››åˆ†ä½æ•°
    potential_noise_features = global_importance[global_importance['Total_Importance'] < noise_threshold]
    
    if not potential_noise_features.empty:
        print(f"\nğŸ” å»ºè®®è¿›è¡Œ'æ§åˆ¶å˜é‡å®éªŒ'çš„ç‰¹å¾ (é‡è¦æ€§ < {noise_threshold:.4f}):")
        for idx, row in potential_noise_features.iterrows():
            print(f"  {row['Feature_Name']}: {row['Total_Importance']:.4f}")
        print("  å»ºè®®ï¼šå»ºç«‹æ–°æ¨¡å‹ç‰ˆæœ¬ï¼Œç§»é™¤è¿™äº›ç‰¹å¾ï¼Œä¸åŸºçº¿æ¨¡å‹è¿›è¡Œæ€§èƒ½å¯¹æ¯”ã€‚")
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ
    print("\n--- ä¿å­˜åˆ†æç»“æœ ---")
    # ä¿å­˜è¯¦ç»†çš„ç‰¹å¾é‡è¦æ€§æ•°æ®
    importance_df.to_csv(os.path.join(new_results_dir, 'feature_importance_analysis.csv'), index=False)
    global_importance.to_csv(os.path.join(new_results_dir, 'global_feature_ranking.csv'), index=False)
        
    if os.path.exists(new_results_dir):
        # ç”Ÿæˆç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
        plt.figure(figsize=(16, 12))
        
        # å­å›¾1ï¼šå„ä¸“å®¶çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯” (ä¼˜åŒ–ï¼šè°ƒæ•´å¸ƒå±€å’Œæ ‡ç­¾)
        plt.subplot(2, 2, 1)
        pivot_df = importance_df.pivot(index='Feature_Name', columns='Expert_Name', values='Importance_Score')
        pivot_df.plot(kind='bar', ax=plt.gca(), width=0.8)
        plt.title('å„ä¸“å®¶çœ¼ä¸­çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯”', fontsize=14, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
        plt.ylabel('é‡è¦æ€§åˆ†æ•°', fontsize=12)
        plt.xlabel('ç‰¹å¾åç§°', fontsize=12)
        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=10)
        plt.grid(axis='y', alpha=0.3)
        
        # å­å›¾2ï¼šå…¨å±€ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ (ä¼˜åŒ–ï¼šæ›´æ¸…æ™°çš„æ ‡ç­¾å’Œé¢œè‰²)
        plt.subplot(2, 2, 2)
        top_10_features = global_importance.head(10)
        colors = plt.cm.viridis(np.linspace(0, 1, len(top_10_features)))
        bars = plt.barh(range(len(top_10_features)), top_10_features['Total_Importance'], 
                        color=colors, alpha=0.8)
        plt.yticks(range(len(top_10_features)), top_10_features['Feature_Name'], fontsize=10)
        plt.xlabel('æ€»é‡è¦æ€§åˆ†æ•°', fontsize=12)
        plt.title('Top 10 ç‰¹å¾å…¨å±€é‡è¦æ€§æ’è¡Œæ¦œ', fontsize=14, fontweight='bold')
        plt.grid(axis='x', alpha=0.3)
        
        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars, top_10_features['Total_Importance'])):
            plt.text(value + 0.01, i, f'{value:.3f}', va='center', fontsize=9)
        
        # å­å›¾3ï¼šç‰¹å¾ä½¿ç”¨é¢‘ç‡åˆ†å¸ƒ (ä¼˜åŒ–ï¼šæ˜ç¡®æ ‡ç­¾å’Œå«ä¹‰)
        plt.subplot(2, 2, 3)
        feature_usage = global_importance['Expert_Count'].value_counts().sort_index()
        usage_labels = [f'è¢«{i}ä¸ªä¸“å®¶ä½¿ç”¨' for i in feature_usage.index]
        colors_usage = plt.cm.Set3(np.linspace(0, 1, len(feature_usage)))
        
        bars_usage = plt.bar(range(len(feature_usage)), feature_usage.values, 
                             color=colors_usage, alpha=0.8)
        plt.xlabel('ä¸“å®¶ä½¿ç”¨æ¬¡æ•°', fontsize=12)
        plt.ylabel('ç‰¹å¾æ•°é‡', fontsize=12)
        plt.title('ç‰¹å¾è¢«ä¸“å®¶ä½¿ç”¨çš„é¢‘ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        plt.xticks(range(len(feature_usage)), feature_usage.index)
        plt.grid(axis='y', alpha=0.3)
        
        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars_usage, feature_usage.values)):
            plt.text(i, value + 0.1, str(value), ha='center', va='bottom', fontsize=10)
        
        # å­å›¾4ï¼šé‡è¦æ€§åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾ (ä¼˜åŒ–ï¼šæ›´æ¸…æ™°çš„æ ‡é¢˜å’Œæ ‡ç­¾)
        plt.subplot(2, 2, 4)
        plt.hist(importance_df['Importance_Score'], bins=15, alpha=0.7, 
                 edgecolor='black', color='skyblue', linewidth=1)
        plt.xlabel('ç‰¹å¾é‡è¦æ€§åˆ†æ•°', fontsize=12)
        plt.ylabel('ç‰¹å¾æ•°é‡', fontsize=12)
        plt.title('ç‰¹å¾é‡è¦æ€§åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾', fontsize=14, fontweight='bold')
        plt.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_score = importance_df['Importance_Score'].mean()
        median_score = importance_df['Importance_Score'].median()
        plt.axvline(mean_score, color='red', linestyle='--', linewidth=2, 
                    label=f'å¹³å‡å€¼: {mean_score:.3f}')
        plt.axvline(median_score, color='orange', linestyle='--', linewidth=2, 
                    label=f'ä¸­ä½æ•°: {median_score:.3f}')
        plt.legend(fontsize=10)
        
        # æ•´ä½“å¸ƒå±€ä¼˜åŒ–
        plt.tight_layout(pad=3.0)
        
        # ä¿å­˜å›¾åƒæ—¶ç¡®ä¿å®Œæ•´æ˜¾ç¤º
        plt.savefig(os.path.join(new_results_dir, 'feature_importance_analysis.png'), 
                   dpi=300, bbox_inches='tight', facecolor='white')
        print(f"ç‰¹å¾é‡è¦æ€§åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {new_results_dir}")
        
        # æ˜¾ç¤ºå›¾åƒ
        plt.show()
        
        # é¢å¤–ç”Ÿæˆä¸€ä¸ªç®€åŒ–çš„ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾
        print("\n--- ç”Ÿæˆç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾ ---")
        plt.figure(figsize=(12, 8))
        
        # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
        heatmap_data = importance_df.pivot(index='Feature_Name', columns='Expert_Name', values='Importance_Score')
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', 
                    linewidths=0.5, cbar_kws={'label': 'é‡è¦æ€§åˆ†æ•°'})
        plt.title('ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾ - å„ä¸“å®¶è§†è§’å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.xlabel('ä¸“å®¶ç±»å‹', fontsize=12)
        plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
        
        # è°ƒæ•´xè½´æ ‡ç­¾è§’åº¦
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        plt.tight_layout()
        plt.savefig(os.path.join(new_results_dir, 'feature_importance_heatmap.png'), 
                   dpi=300, bbox_inches='tight', facecolor='white')
        print(f"ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {new_results_dir}")
        plt.show()
    
    print("\nç‰¹å¾é‡è¦æ€§å®¡è®¯ä¸åˆ†æå®Œæˆï¼")
    print("åŸºäºåˆ†æç»“æœï¼Œæ‚¨å¯ä»¥åˆ¶å®šç²¾å‡†çš„æ¨¡å‹ä¼˜åŒ–ç­–ç•¥ã€‚")

    # =============================================================================
    # é˜¶æ®µä¸‰ï¼šå¯¹å®Œæ•´çš„"ä¸“å®¶ç»„å†³ç­–ç³»ç»Ÿ"è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    # =============================================================================
    # ... (æ­¤é˜¶æ®µä»£ç ä¸åŸè„šæœ¬å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨) ...
    print("\n" + "="*50)
    print("é˜¶æ®µä¸‰ï¼šå¼€å§‹å¯¹ä¸¤é˜¶æ®µç³»ç»Ÿè¿›è¡Œæœ€ç»ˆè¯„ä¼°")
    print("="*50)
    X_test_imputed = imputer1.transform(X_test_full)
    X_test_scaled = scaler1.transform(X_test_imputed)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_full.columns, index=X_test_full.index)
    print("æµ‹è¯•æ ·æœ¬è¿›å…¥ç³»ç»Ÿ...")
    print(" -> æ­¥éª¤1: 'åˆ†è¯Šå°' (æ¨¡å‹ä¸€) æ­£åœ¨è¿›è¡Œåˆæ­¥è¯Šæ–­...")
    pred1_test = model1.predict(X_test_scaled)
    final_predictions = np.zeros_like(y_test_full)
    non_drift_test_mask = (pred1_test == 0)
    X_test_for_experts = X_test_scaled_df[non_drift_test_mask]
    print(f" -> 'åˆ†è¯Šå°'è¯Šæ–­å®Œæ¯•: {len(X_test_for_experts)} ä¸ªæ ·æœ¬è¢«æäº¤è‡³'ä¸“å®¶ç»„'ã€‚")
    if len(X_test_for_experts) > 0:
        print(" -> æ­¥éª¤2: 'å‘ç°-æ ¸æŸ¥'ä»²è£æµç¨‹å¼€å§‹...")
        
        # é˜¶æ®µ 2bï¼šæ„å»ºå…¨æ–°çš„ä»²è£å¼é¢„æµ‹é€»è¾‘
        # =============================================================================
        print(" -> 2a. åˆæ­¥ç­›æŸ¥ï¼šèƒ½é‡æ´¾åˆç­›ä¸“å®¶è¿›è¡Œå¼‚å¸¸ä¿¡å·æ£€æµ‹...")
        
        # 1. åˆæ­¥ç­›æŸ¥ï¼šèƒ½é‡æ´¾åˆç­›ä¸“å®¶ï¼ˆä½¿ç”¨è‡ªåŠ¨è°ƒå‚çš„æœ€ä½³é˜ˆå€¼ï¼‰
        # 1. åˆæ­¥ç­›æŸ¥ï¼šèƒ½é‡æ´¾åˆç­›ä¸“å®¶ï¼ˆä½¿ç”¨è‡ªåŠ¨è°ƒå‚çš„æœ€ä½³é˜ˆå€¼ï¼‰
        # 1. åˆæ­¥ç­›æŸ¥ï¼šèƒ½é‡æ´¾åˆç­›ä¸“å®¶ï¼ˆä½¿ç”¨è‡ªåŠ¨è°ƒå‚çš„æœ€ä½³é˜ˆå€¼ï¼‰
        energy_screener_proba = energy_screener.predict_proba(X_test_for_experts[features_energy_screener])
        
        # è‡ªåŠ¨è°ƒå‚ï¼šæµ‹è¯•ä¸åŒé˜ˆå€¼ç»„åˆ
        # è·å–æ ‡ç­¾æ˜ å°„
        print(f"å¯ç”¨çš„æ ‡ç­¾ç±»åˆ«: {list(label_encoder2.classes_)}")
        print(f"æ ‡ç­¾ç¼–ç å™¨æ˜ å°„: {dict(zip(range(len(label_encoder2.classes_)), label_encoder2.classes_))}")
        
        # è·å–äºŒçº§ç¼–ç å™¨ä¸­"adversarial_pgd"å’Œ"noise_gaussian"çš„ç´¢å¼•
        adv_code = label_encoder.transform(['adversarial_pgd'])[0]
        noise_code = label_encoder.transform(['noise_gaussian'])[0]
        adversarial_attack_label = label_encoder2.transform([adv_code])[0]
        gaussian_noise_label = label_encoder2.transform([noise_code])[0]
        print('äºŒçº§ç¼–ç å™¨ classes_:', list(label_encoder2.classes_))
        print('æ˜ å°„: noise_idx=', gaussian_noise_label, ' adv_idx=', adversarial_attack_label)
        
        # è‡ªåŠ¨è°ƒå‚ï¼šæµ‹è¯•ä¸åŒé˜ˆå€¼ç»„åˆ
        print("\nå¼€å§‹è‡ªåŠ¨è°ƒå‚ï¼Œå¯»æ‰¾æœ€ä½³é˜ˆå€¼...")
        best_params = auto_tune_thresholds(
            energy_screener_proba, X_test_for_experts, features_structural_arbitrator,
            structural_arbitrator, y_test_full, non_drift_test_mask,
            adversarial_attack_label, gaussian_noise_label, pred1_test,  # â† ç°åœ¨å˜é‡å·²ç»å®šä¹‰äº†
            label_encoder, label_encoder2
        )
        
        THRESH_ENERGY_LOW = best_params['energy_low']
        THRESH_ENERGY_HIGH = best_params['energy_high']
        THRESH_ARBIT_ATTACK = best_params['arbit_attack']
        
        print(f"æœ€ä½³å‚æ•°: èƒ½é‡ä½é˜ˆå€¼={THRESH_ENERGY_LOW}, èƒ½é‡é«˜é˜ˆå€¼={THRESH_ENERGY_HIGH}, ä»²è£é˜ˆå€¼={THRESH_ARBIT_ATTACK}")
        print("ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°é¢„æµ‹...")
        
        # 2. æ¡ä»¶ä»²è£ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰
        print(" -> 2b. æ¡ä»¶ä»²è£ï¼šæ ¹æ®åˆç­›ç»“æœå†³å®šæ˜¯å¦è§¦å‘ä»²è£...")
        
        # è·å–æ ‡ç­¾æ˜ å°„
        print(f"å¯ç”¨çš„æ ‡ç­¾ç±»åˆ«: {list(label_encoder2.classes_)}")
        print(f"æ ‡ç­¾ç¼–ç å™¨æ˜ å°„: {dict(zip(range(len(label_encoder2.classes_)), label_encoder2.classes_))}")
        
        # è·å–äºŒçº§ç¼–ç å™¨ä¸­"adversarial_pgd"å’Œ"noise_gaussian"çš„ç´¢å¼•
        adv_code = label_encoder.transform(['adversarial_pgd'])[0]
        noise_code = label_encoder.transform(['noise_gaussian'])[0]
        adversarial_attack_label = label_encoder2.transform([adv_code])[0]
        gaussian_noise_label = label_encoder2.transform([noise_code])[0]
        print('äºŒçº§ç¼–ç å™¨ classes_:', list(label_encoder2.classes_))
        print('æ˜ å°„: noise_idx=', gaussian_noise_label, ' adv_idx=', adversarial_attack_label)
        
        # åˆå§‹åŒ–æœ€ç»ˆé¢„æµ‹ç»“æœ
        final_expert_predictions = np.zeros(len(X_test_for_experts), dtype=int)
        
        # ç»Ÿè®¡ä»²è£æƒ…å†µ
        arbitration_count = 0
        direct_accept_count = 0
        
        for i, energy_proba in enumerate(energy_screener_proba):
            # æ ¹æ®äºŒçº§ç¼–ç å™¨ç¡®å®šæ¦‚ç‡åˆ—çš„ç´¢å¼•
            prob_attack = energy_proba[adversarial_attack_label]
            if prob_attack <= THRESH_ENERGY_LOW:
                # ä½ç½®ä¿¡åº¦æ”»å‡» => ç›´æ¥æ¥æ”¶ä¸ºå™ªå£°
                final_expert_predictions[i] = gaussian_noise_label
                direct_accept_count += 1
                continue
            if prob_attack >= THRESH_ENERGY_HIGH:
                # é«˜ç½®ä¿¡åº¦æ”»å‡» => ç›´æ¥å®šä¸ºæ”»å‡»ï¼ˆå¯é€‰ï¼šä»å¯è¯·æ±‚ä»²è£åšåŒé‡ç¡®è®¤ï¼‰
                final_expert_predictions[i] = adversarial_attack_label
                direct_accept_count += 1
                continue

            # ä»‹äºä¸¤é˜ˆå€¼ä¹‹é—´ => è§¦å‘ä»²è£
            arbitration_count += 1
            print(f"     -> æ ·æœ¬ {i+1}: åˆç­›æ¦‚ç‡å¤„äºç°åŒº({prob_attack:.2f})ï¼Œå¯åŠ¨ä»²è£...")
            sample_features = X_test_for_experts.iloc[i:i+1][features_structural_arbitrator]
            arbitrator_proba = structural_arbitrator.predict_proba(sample_features)[0]
            prob_attack_arb = arbitrator_proba[adversarial_attack_label]
            arbitrator_prediction = adversarial_attack_label if prob_attack_arb >= THRESH_ARBIT_ATTACK else gaussian_noise_label
            
            # é‡‡çº³ä»²è£ä¸“å®¶çš„æœ€ç»ˆæ„è§
            if arbitrator_prediction == adversarial_attack_label:
                # ä»²è£ä¸“å®¶åŒæ„æ˜¯"å¯¹æŠ—æ”»å‡»"
                final_expert_predictions[i] = adversarial_attack_label
                print(f"     -> æ ·æœ¬ {i+1}: ä»²è£ä¸“å®¶ç¡®è®¤æ”»å‡»ï¼Œæœ€ç»ˆåˆ¤å®šä¸ºå¯¹æŠ—æ”»å‡»")
            else:
                # ä»²è£ä¸“å®¶è¡Œä½¿"ä¸€ç¥¨å¦å†³æƒ"ï¼Œæ¨ç¿»åˆç­›ç»“æœ
                final_expert_predictions[i] = gaussian_noise_label
                print(f"     -> æ ·æœ¬ {i+1}: ä»²è£ä¸“å®¶è¡Œä½¿å¦å†³æƒï¼Œä¿®æ­£ä¸ºé«˜æ–¯å™ªå£°")
        
        print(f" -> ä»²è£ç»Ÿè®¡ï¼šç›´æ¥æ¥å— {direct_accept_count} ä¸ªæ ·æœ¬ï¼Œä»²è£ {arbitration_count} ä¸ªæ ·æœ¬")
        
        # è½¬æ¢å›åŸå§‹æ ‡ç­¾
        expert_predictions_original_labels = label_encoder2.inverse_transform(final_expert_predictions)
        final_predictions[non_drift_test_mask] = expert_predictions_original_labels
        print(" -> 'å‘ç°-æ ¸æŸ¥'ä»²è£æµç¨‹å®Œæ¯•ã€‚")
    drift_label_encoded = list(label_encoder.classes_).index('drift_parameter')
    final_predictions[pred1_test == 1] = drift_label_encoded
    print("...æ‰€æœ‰æ ·æœ¬é¢„æµ‹æµç¨‹ç»“æŸã€‚")
    print("\n--- ä¸¤é˜¶æ®µ'ä¸€ç¥¨å¦å†³'ä»²è£ä¸“å®¶ç³»ç»Ÿæœ€ç»ˆæ€§èƒ½è¯„ä¼° ---")
    print(f"æœ€ç»ˆå‡†ç¡®ç‡: {accuracy_score(y_test_full, final_predictions):.4f}")
    print("\næœ€ç»ˆåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test_full, final_predictions, target_names=label_encoder.classes_))
    print("\næœ€ç»ˆæ··æ·†çŸ©é˜µ:")
    final_cm = confusion_matrix(y_test_full, final_predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(final_cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.title('Final Confusion Matrix for the Two-Stage Arbitration Expert System')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    # ä¿å­˜å›¾åƒåˆ°æ–°çš„ç»“æœæ–‡ä»¶å¤¹
    plt.savefig(os.path.join(new_results_dir, 'final_confusion_matrix_expert_system.png'))
    print(f"\næ··æ·†çŸ©é˜µå›¾åƒå·²ä¿å­˜åˆ°: {new_results_dir}")
    plt.show()
    print("\nè„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼")

    # é‡æ–°è®¾è®¡ç‰¹å¾é‡è¦æ€§å¯è§†åŒ– - æ›´æ¸…æ™°æ˜“æ‡‚
    print("\n--- ç”Ÿæˆæ¸…æ™°æ˜“æ‡‚çš„ç‰¹å¾é‡è¦æ€§åˆ†æå›¾è¡¨ ---")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # æ”¯æŒä¸­æ–‡æ˜¾ç¤º
    plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    
    # åˆ›å»ºä¸‰ä¸ªç‹¬ç«‹çš„å›¾è¡¨ï¼Œæ¯ä¸ªéƒ½æ¸…æ™°æ˜“æ‡‚
    
    # å›¾è¡¨1ï¼šç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ - æœ€ç›´è§‚çš„å±•ç¤º
    plt.figure(figsize=(14, 8))
    
    # æŒ‰æ€»é‡è¦æ€§æ’åºï¼Œåªæ˜¾ç¤ºå‰15ä¸ªæœ€é‡è¦çš„ç‰¹å¾
    top_features = global_importance.head(15)
    
    # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
    bars = plt.barh(range(len(top_features)), top_features['Total_Importance'], 
                    color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))
    
    # è®¾ç½®Yè½´æ ‡ç­¾ï¼ˆç‰¹å¾åç§°ï¼‰
    plt.yticks(range(len(top_features)), top_features['Feature_Name'], fontsize=11)
    
    # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, value) in enumerate(zip(bars, top_features['Total_Importance'])):
        plt.text(value + 0.01, i, f'{value:.3f}', va='center', fontsize=10, fontweight='bold')
    
    plt.xlabel('æ€»é‡è¦æ€§åˆ†æ•°', fontsize=14)
    plt.title('ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ - å“ªäº›ç‰¹å¾æœ€é‡è¦ï¼Ÿ', fontsize=16, fontweight='bold', pad=20)
    plt.grid(axis='x', alpha=0.3)
    
    # æ·»åŠ è¯´æ˜æ–‡å­—
    plt.figtext(0.02, 0.02, 
                'è¯´æ˜ï¼šåˆ†æ•°è¶Šé«˜ï¼Œè¯¥ç‰¹å¾åœ¨åŒºåˆ†"å¯¹æŠ—æ”»å‡»"ä¸"é«˜æ–¯å™ªå£°"æ—¶è¶Šé‡è¦', 
                fontsize=10, style='italic', color='gray')
    
    plt.tight_layout()
    plt.savefig(os.path.join(new_results_dir, '01_feature_importance_ranking.png'), 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    # å›¾è¡¨2ï¼šå„ä¸“å®¶çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯” - æ¸…æ™°çš„ä¸“å®¶è§†è§’
    plt.figure(figsize=(16, 10))
    
    # é€‰æ‹©é‡è¦æ€§æœ€é«˜çš„8ä¸ªç‰¹å¾è¿›è¡Œå¯¹æ¯”
    top_8_features = global_importance.head(8)['Feature_Name'].tolist()
    expert_data = importance_df[importance_df['Feature_Name'].isin(top_8_features)]
    
    # åˆ›å»ºåˆ†ç»„æ¡å½¢å›¾
    pivot_data = expert_data.pivot(index='Feature_Name', columns='Expert_Name', values='Importance_Score')
    
    # è®¾ç½®é¢œè‰²æ–¹æ¡ˆ
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    # ç»˜åˆ¶åˆ†ç»„æ¡å½¢å›¾
    x = np.arange(len(top_8_features))
    width = 0.2
    
    for i, (expert_name, color) in enumerate(zip(pivot_data.columns, colors)):
        values = pivot_data[expert_name].values
        plt.bar(x + i*width, values, width, label=expert_name, color=color, alpha=0.8)
    
    plt.xlabel('ç‰¹å¾åç§°', fontsize=14)
    plt.ylabel('é‡è¦æ€§åˆ†æ•°', fontsize=14)
    plt.title('å„ä¸“å®¶çœ¼ä¸­çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
    plt.xticks(x + width*1.5, top_8_features, rotation=45, ha='right')
    plt.legend(title='ä¸“å®¶ç±»å‹', fontsize=12, title_fontsize=12)
    plt.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ è¯´æ˜æ–‡å­—
    plt.figtext(0.02, 0.02, 
                'è¯´æ˜ï¼šæ¯ä¸ªç‰¹å¾åœ¨ä¸åŒä¸“å®¶çœ¼ä¸­çš„é‡è¦æ€§åˆ†æ•°ï¼Œå¸®åŠ©ç†è§£å„ä¸“å®¶çš„å†³ç­–é€»è¾‘', 
                fontsize=10, style='italic', color='gray')
    
    plt.tight_layout()
    plt.savefig(os.path.join(new_results_dir, '02_expert_comparison.png'), 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    # å›¾è¡¨3ï¼šç‰¹å¾ä½¿ç”¨æƒ…å†µåˆ†æ - ç†è§£ç‰¹å¾åˆ†å¸ƒ
    plt.figure(figsize=(12, 8))
    
    # åˆ›å»ºå­å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # å­å›¾1ï¼šç‰¹å¾è¢«ä¸“å®¶ä½¿ç”¨çš„æ¬¡æ•°
    feature_usage = global_importance['Expert_Count'].value_counts().sort_index()
    usage_labels = [f'è¢«{i}ä¸ªä¸“å®¶ä½¿ç”¨' for i in feature_usage.index]
    
    bars1 = ax1.bar(range(len(feature_usage)), feature_usage.values, 
                     color=['#FF9999', '#66B2FF'], alpha=0.8)
    ax1.set_xlabel('ä¸“å®¶ä½¿ç”¨æ¬¡æ•°', fontsize=12)
    ax1.set_ylabel('ç‰¹å¾æ•°é‡', fontsize=12)
    ax1.set_title('ç‰¹å¾è¢«ä¸“å®¶ä½¿ç”¨çš„é¢‘ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax1.set_xticks(range(len(feature_usage)))
    ax1.set_xticklabels(feature_usage.index)
    ax1.grid(axis='y', alpha=0.3)
    
    # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, value) in enumerate(zip(bars1, feature_usage.values)):
        ax1.text(i, value + 0.1, str(value), ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # å­å›¾2ï¼šé‡è¦æ€§åˆ†æ•°åˆ†å¸ƒ
    ax2.hist(importance_df['Importance_Score'], bins=12, alpha=0.7, 
              edgecolor='black', color='lightblue', linewidth=1)
    ax2.set_xlabel('ç‰¹å¾é‡è¦æ€§åˆ†æ•°', fontsize=12)
    ax2.set_ylabel('ç‰¹å¾æ•°é‡', fontsize=12)
    ax2.set_title('ç‰¹å¾é‡è¦æ€§åˆ†æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax2.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡å‚è€ƒçº¿
    mean_score = importance_df['Importance_Score'].mean()
    median_score = importance_df['Importance_Score'].median()
    ax2.axvline(mean_score, color='red', linestyle='--', linewidth=2, 
                 label=f'å¹³å‡å€¼: {mean_score:.3f}')
    ax2.axvline(median_score, color='orange', linestyle='--', linewidth=2, 
                 label=f'ä¸­ä½æ•°: {median_score:.3f}')
    ax2.legend(fontsize=10)
    
    plt.tight_layout()
    plt.savefig(os.path.join(new_results_dir, '03_feature_analysis.png'), 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    # å›¾è¡¨4ï¼šæ”¹è¿›çš„çƒ­åŠ›å›¾ - æ¸…æ™°æ˜¾ç¤ºæ¯ä¸ªç‰¹å¾åœ¨å„ä¸“å®¶ä¸­çš„é‡è¦æ€§
    plt.figure(figsize=(14, 10))
    
    # é‡æ–°æ•´ç†çƒ­åŠ›å›¾æ•°æ®ï¼Œç¡®ä¿æ¯ä¸ªç‰¹å¾åœ¨æ¯ä¸ªä¸“å®¶ä¸‹éƒ½æœ‰å€¼
    heatmap_data = importance_df.pivot(index='Feature_Name', columns='Expert_Name', values='Importance_Score')
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ–¹æ¡ˆå’Œæ ‡æ³¨
    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', 
                linewidths=0.5, cbar_kws={'label': 'é‡è¦æ€§åˆ†æ•°'}, 
                square=True, annot_kws={'size': 9})
    
    plt.title('ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾ - æ¯ä¸ªç‰¹å¾åœ¨å„ä¸“å®¶çœ¼ä¸­çš„é‡è¦æ€§', fontsize=16, fontweight='bold')
    plt.xlabel('ä¸“å®¶ç±»å‹', fontsize=12)
    plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
    
    # è°ƒæ•´æ ‡ç­¾è§’åº¦ï¼Œç¡®ä¿å¯è¯»æ€§
    plt.xticks(rotation=0, ha='center')
    plt.yticks(rotation=0)
    
    plt.tight_layout()
    plt.savefig(os.path.join(new_results_dir, '04_feature_heatmap.png'), 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print(f"\næ‰€æœ‰ä¼˜åŒ–åçš„å›¾è¡¨å·²ä¿å­˜åˆ°: {new_results_dir}")
    print("ç°åœ¨æ¯ä¸ªå›¾è¡¨éƒ½æœ‰æ¸…æ™°çš„æ ‡é¢˜ã€æ ‡ç­¾å’Œè¯´æ˜ï¼Œåº”è¯¥æ›´å®¹æ˜“ç†è§£äº†ï¼")

if __name__ == '__main__':
    main()
# scripts/04_train_classifier.py
#!/usr/bin/env python3
"""
è„šæœ¬ 04: è®­ç»ƒä¸¤é˜¶æ®µåˆ†ç±»å™¨å¹¶è¯„ä¼°æ€§èƒ½ã€‚
æ‰§è¡Œæ–¹å¼: åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œ `python scripts/04_train_classifier.py`
"""
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import glob
import yaml
import sys

# ä» scikit-learn ä¸­å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„æ¨¡å—
from sklearn.model_selection import train_test_split, cross_val_predict
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

def main():
    print("=== è„šæœ¬ 04: è®­ç»ƒåˆ†ç±»å™¨ ===")

    # --- åŠ è½½é…ç½®æ–‡ä»¶ ---
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    print("æ‰€æœ‰æœºå™¨å­¦ä¹ åº“éƒ½å·²æˆåŠŸå¯¼å…¥ï¼")
    print("ä¸¤é˜¶æ®µåˆ†ç±»å™¨è®­ç»ƒè„šæœ¬å·²å‡†å¤‡å°±ç»ªã€‚")

    # =============================================================================
    # æ­¥éª¤ä¸€ï¼šæ•°æ®åŠ è½½ä¸é€šç”¨é¢„å¤„ç†
    # =============================================================================
    # --- 1. è‡ªåŠ¨å¯»æ‰¾æœ€æ–°çš„æŒ‡çº¹æ•°æ®æ–‡ä»¶ ---
    runs_dir = config['output_paths']['runs_directory']
    list_of_run_dirs = glob.glob(os.path.join(runs_dir, '*/'))
    if not list_of_run_dirs:
        print("\né”™è¯¯ï¼šåœ¨ 'runs' æ–‡ä»¶å¤¹ä¸‹æ‰¾ä¸åˆ°ä»»ä½•è¿è¡Œè®°å½•ã€‚")
        sys.exit(1)

    latest_run_dir = max(list_of_run_dirs, key=os.path.getctime)
    fingerprint_file_path = os.path.join(latest_run_dir, 'vulnerability_fingerprints.csv')
    print(f"\næ­£åœ¨ä»æœ€æ–°çš„è¿è¡Œè®°å½•ä¸­åŠ è½½æ•°æ®: {fingerprint_file_path}")

    try:
        data = pd.read_csv(fingerprint_file_path)
        print(f"æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ ·æœ¬ã€‚")
    except FileNotFoundError:
        print(f"\né”™è¯¯ï¼šåœ¨è·¯å¾„ '{fingerprint_file_path}' ä¸­æ‰¾ä¸åˆ° vulnerability_fingerprints.csv æ–‡ä»¶ã€‚")
        sys.exit(1)

    # --- 2. åˆ†ç¦»ç‰¹å¾ (X) ä¸åŸå§‹æ ‡ç­¾ (y) ---
    # ... (æ­¤éƒ¨åˆ†ä»£ç ä¸åŸè„šæœ¬å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨) ...
    X = data.drop('vulnerability_type', axis=1)
    y_str = data['vulnerability_type']
    # --- 3. æ ‡ç­¾ç¼–ç  ---
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y_str)
    label_mapping = {i: class_name for i, class_name in enumerate(label_encoder.classes_)}
    print("\næ ‡ç­¾å·²æˆåŠŸç¼–ç ä¸ºæ•°å­—:")
    print(label_mapping)
    # --- 4. åˆ’åˆ†æ€»æ•°æ®é›† ---
    X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    # --- 5. å…¨å±€é¢„å¤„ç†ï¼šå¤„ç†æ— ç©·å¤§å€¼ ---
    print("\næ­£åœ¨å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œæ— ç©·å¤§å€¼é¢„å¤„ç†...")
    X_train_full.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_test_full.replace([np.inf, -np.inf], np.nan, inplace=True)
    print("é¢„å¤„ç†å®Œæˆã€‚")

    # =============================================================================
    # é˜¶æ®µä¸€ï¼šè®­ç»ƒæ¨¡å‹ä¸€ (â€œæ¼‚ç§»â€è¯†åˆ«å™¨ - Generalist)
    # =============================================================================
    # ... (æ­¤é˜¶æ®µä»£ç ä¸åŸè„šæœ¬å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨) ...
    print("\n" + "="*50)
    print("é˜¶æ®µä¸€ï¼šå¼€å§‹è®­ç»ƒæ¨¡å‹ä¸€ ('æ¼‚ç§»'è¯†åˆ«å™¨)")
    print("="*50)
    drift_label_encoded = list(label_encoder.classes_).index('drift_parameter')
    y_train1 = np.where(y_train_full == drift_label_encoded, 1, 0)
    X_train1 = X_train_full.copy()
    imputer1 = SimpleImputer(strategy='median')
    X_train1_imputed = imputer1.fit_transform(X_train1)
    scaler1 = StandardScaler()
    X_train1_scaled = scaler1.fit_transform(X_train1_imputed)
    model1 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    model1.fit(X_train1_scaled, y_train1)
    print("æ¨¡å‹ä¸€è®­ç»ƒå®Œæˆã€‚")
    # --- å¿«é€ŸéªŒè¯æ¨¡å‹ä¸€åœ¨è®­ç»ƒé›†ä¸Šçš„æ€§èƒ½ ---
    from sklearn.metrics import classification_report
    pred1_train = model1.predict(X_train1_scaled)
    print("\n--- æ¨¡å‹ä¸€ (åˆ†è¯Šå°) åœ¨è®­ç»ƒé›†ä¸Šçš„æ€§èƒ½æŠ¥å‘Š ---")
    print(classification_report(y_train1, pred1_train, target_names=['éæ¼‚ç§» (0)', 'æ¼‚ç§» (1)']))

    # =============================================================================
    # é˜¶æ®µäºŒï¼šæ„å»º"ä¸“å®¶ç»„å†³ç­–ç³»ç»Ÿ" (Specialist System)
    # =============================================================================
    # ... (æ­¤é˜¶æ®µä»£ç ä¸åŸè„šæœ¬å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨) ...
    print("\n" + "="*50)
    print("é˜¶æ®µäºŒï¼šå¼€å§‹æ„å»º'ä¸“å®¶ç»„å†³ç­–ç³»ç»Ÿ'")
    print("="*50)
    non_drift_train_mask = (y_train1 == 0)
    X_train_experts = X_train_full[non_drift_train_mask]
    y_train_experts_raw = y_train_full[non_drift_train_mask]
    label_encoder2 = LabelEncoder()
    y_train_experts = label_encoder2.fit_transform(y_train_experts_raw)
    print(f"å·²ç­›é€‰å‡º {len(X_train_experts)} ä¸ª'å¯¹æŠ— vs. å™ªå£°'æ ·æœ¬ï¼Œäº¤ç”±ä¸“å®¶ç»„å¤„ç†ã€‚")
    features_dynamic = [
        'wasserstein_dist',
        'cosine_similarity',
        'kl_divergence_pos',
        'kl_divergence_neg',
        'std_dev_diff',         # è¡¡é‡æ³¨æ„åŠ›åˆ†æ•£/é›†ä¸­çš„å˜åŒ– [cite: 26]
        'kurtosis_diff',        # è¡¡é‡æ³¨æ„åŠ›å°–é”/å¹³å¦çš„å˜åŒ– [cite: 27]
        'dynamic_wavelet_ratio_change', # æ‚¨æ–°è®¾è®¡çš„ç‰¹å¾ä¸€ï¼šåŠ¨æ€å°æ³¢èƒ½é‡æ¯”å˜åŒ–ç‡
        'll_distortion'         # æ‚¨æ–°è®¾è®¡çš„ç‰¹å¾äºŒï¼šä½é¢‘å­å¸¦ç»“æ„å¤±çœŸåº¦
    ]

    # ä¸“å®¶2ï¼šâ€œé¢‘åŸŸåˆ†æä¸“å®¶â€ (Frequency Domain Expert)
    # å…³æ³¨çš„æ˜¯ H_vuln åœ¨é¢‘åŸŸä¸Šçš„å†…åœ¨ç‰¹æ€§
    features_frequency = [
        'high_freq_ratio',     # æ—¨åœ¨æ•æ‰é«˜æ–¯å™ªå£°çš„é«˜é¢‘æ•£å¸ƒç‰¹æ€§ [cite: 23]
        'ratio_zscore'         # æ‚¨æ–°è®¾è®¡çš„ç‰¹å¾ä¸‰ï¼šèƒ½é‡æ¯”çš„åŸºçº¿åˆ†ç¦»åº¦ (Z-score)
    ]

    # ä¸“å®¶3ï¼šâ€œçº¹ç†å­¦ä¸“å®¶â€ (Texture Expert)
    # å…³æ³¨çš„æ˜¯ H_vuln çƒ­åŠ›å›¾çš„â€œè´¨æ„Ÿâ€
    features_texture = [
        'contrast',             # å¯¹æ¯”åº¦ [cite: 24]
        'homogeneity',          # åŒè´¨æ€§ [cite: 24]
        'energy',               # èƒ½é‡ [cite: 24]
        'correlation'           # ç›¸å…³æ€§ [cite: 24]
    ]

    # ä¸“å®¶4ï¼šâ€œæ•æ„Ÿæ€§/å†…åœ¨æ€§ä¸“å®¶â€ (Sensitivity/Intrinsic Expert)
    # (è¿™æ˜¯ä¸€ä¸ªå»ºè®®çš„ç»„åˆï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´)
    # å…³æ³¨çš„æ˜¯ H_vuln æœ¬èº«çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œä½œä¸ºå¯¹å…¶ä»–ä¸“å®¶çš„è¡¥å……
    features_sensitivity = [
        'std_dev_diff',         # æ³¨æ„ï¼šè¿™é‡Œå¯ä»¥å¤ç”¨ä¸€äº›ç‰¹å¾ï¼Œè®©ä¸åŒä¸“å®¶æœ‰äº¤å‰è§†è§’
        'kurtosis_diff',
        'high_freq_ratio'
    ]  
    X_train_experts_imputed = imputer1.transform(X_train_experts)
    X_train_experts_scaled = scaler1.transform(X_train_experts_imputed)
    X_train_experts_scaled_df = pd.DataFrame(X_train_experts_scaled, columns=X_train_experts.columns, index=X_train_experts.index)
    dynamic_expert = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    frequency_expert = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    texture_expert = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    sensitivity_expert = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    dynamic_opinions = cross_val_predict(dynamic_expert, X_train_experts_scaled_df[features_dynamic], y_train_experts, cv=5, method='predict_proba')
    frequency_opinions = cross_val_predict(frequency_expert, X_train_experts_scaled_df[features_frequency], y_train_experts, cv=5, method='predict_proba')
    texture_opinions = cross_val_predict(texture_expert, X_train_experts_scaled_df[features_texture], y_train_experts, cv=5, method='predict_proba')
    sensitivity_opinions = cross_val_predict(sensitivity_expert, X_train_experts_scaled_df[features_sensitivity], y_train_experts, cv=5, method='predict_proba')
    X_train_meta = np.hstack([dynamic_opinions, frequency_opinions, texture_opinions, sensitivity_opinions])
    print("ä¸“å®¶ä¼šè¯Šå®Œæˆï¼Œå·²å½¢æˆå…ƒç‰¹å¾é›†ã€‚")
    dynamic_expert.fit(X_train_experts_scaled_df[features_dynamic], y_train_experts)
    frequency_expert.fit(X_train_experts_scaled_df[features_frequency], y_train_experts)
    texture_expert.fit(X_train_experts_scaled_df[features_texture], y_train_experts)
    sensitivity_expert.fit(X_train_experts_scaled_df[features_sensitivity], y_train_experts)
    print("ä¸“å®¶å­¦ä¹ å®Œæˆã€‚")
    meta_classifier = LogisticRegression(random_state=42)
    meta_classifier.fit(X_train_meta, y_train_experts)
    print("æœ€ç»ˆå†³ç­–è€…è®­ç»ƒå®Œæˆï¼")

    # =============================================================================
    # æ–°å¢ï¼šç‰¹å¾é‡è¦æ€§å®¡è®¯ä¸åˆ†æ
    # =============================================================================
    print("\n" + "="*50)
    print("ç‰¹å¾é‡è¦æ€§å®¡è®¯ä¸åˆ†æé˜¶æ®µ")
    print("="*50)
    
    # ç¬¬ä¸€é˜¶æ®µï¼šè¯æ®æ”¶é›† - å®¡è®¯å‡†å¤‡ä¸æ‰§è¡Œ
    print("\n--- ç¬¬ä¸€é˜¶æ®µï¼šè¯æ®æ”¶é›† ---")
    
    # 1.1 å®šä½å®¡è®¯ç›®æ ‡ï¼šå››ä¸ªä¸“å®¶æ¨¡å‹
    experts = {
        'dynamic_expert': (dynamic_expert, features_dynamic, 'åŠ¨æ€å˜åŒ–å­¦ä¸“å®¶'),
        'frequency_expert': (frequency_expert, features_frequency, 'é¢‘åŸŸåˆ†æä¸“å®¶'),
        'texture_expert': (texture_expert, features_texture, 'çº¹ç†å­¦ä¸“å®¶'),
        'sensitivity_expert': (sensitivity_expert, features_sensitivity, 'æ•æ„Ÿæ€§/å†…åœ¨æ€§ä¸“å®¶')
    }
    
    # 1.2 å®¡è®¯å®æ–½ï¼šæå–æ¯ä¸ªä¸“å®¶çš„"å£ä¾›"
    feature_importance_data = []
    
    for expert_name, (expert_model, feature_list, expert_title) in experts.items():
        print(f"\næ­£åœ¨å®¡è®¯ {expert_title} ({expert_name})...")
        
        # è·å–ç‰¹å¾é‡è¦æ€§åˆ†æ•°
        importance_scores = expert_model.feature_importances_
        
        # å°†ç‰¹å¾åä¸é‡è¦æ€§åˆ†æ•°é…å¯¹
        for feature_name, importance_score in zip(feature_list, importance_scores):
            feature_importance_data.append({
                'Expert_Name': expert_title,
                'Expert_Code': expert_name,
                'Feature_Name': feature_name,
                'Importance_Score': importance_score
            })
    
    # 1.3 æ•´ç†æ¡£æ¡ˆï¼šåˆ›å»ºç»“æ„åŒ–æ•°æ®è¡¨
    importance_df = pd.DataFrame(feature_importance_data)
    print(f"\nå®¡è®¯å®Œæˆï¼å…±æ”¶é›†åˆ° {len(importance_df)} æ¡ç‰¹å¾é‡è¦æ€§è®°å½•ã€‚")
    
    # ç¬¬äºŒé˜¶æ®µï¼šæ¡ˆæƒ…åˆ†æ - "å£ä¾›"è§£è¯»ä¸æ´å¯Ÿ
    print("\n--- ç¬¬äºŒé˜¶æ®µï¼šæ¡ˆæƒ…åˆ†æ ---")
    
    # 2.1 å±‚é¢ä¸€ï¼šä¸“å®¶ç‹¬ç«‹åˆ†ææŠ¥å‘Š
    print("\nã€ä¸“å®¶ç‹¬ç«‹åˆ†ææŠ¥å‘Šã€‘")
    for expert_title in importance_df['Expert_Name'].unique():
        expert_data = importance_df[importance_df['Expert_Name'] == expert_title]
        expert_data_sorted = expert_data.sort_values('Importance_Score', ascending=False)
        
        print(f"\n{expert_title}çš„ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ:")
        for idx, row in expert_data_sorted.iterrows():
            print(f"  {row['Feature_Name']}: {row['Importance_Score']:.4f}")
    
    # 2.2 å±‚é¢äºŒï¼šå…¨å±€åŠŸåŠ³æ’è¡Œæ¦œ
    print("\nã€å…¨å±€åŠŸåŠ³æ’è¡Œæ¦œã€‘")
    global_importance = importance_df.groupby('Feature_Name')['Importance_Score'].agg(['sum', 'mean', 'count']).reset_index()
    global_importance.columns = ['Feature_Name', 'Total_Importance', 'Average_Importance', 'Expert_Count']
    global_importance = global_importance.sort_values('Total_Importance', ascending=False)
    
    print("\næ‰€æœ‰ç‰¹å¾çš„å…¨å±€æ€»é‡è¦æ€§æ’è¡Œæ¦œ (æŒ‰æ€»è´¡çŒ®åº¦æ’åº):")
    for idx, row in global_importance.iterrows():
        print(f"  {row['Feature_Name']}: æ€»è´¡çŒ®åº¦={row['Total_Importance']:.4f}, "
              f"å¹³å‡è´¡çŒ®åº¦={row['Average_Importance']:.4f}, "
              f"è¢«{row['Expert_Count']}ä¸ªä¸“å®¶ä½¿ç”¨")
    
    # è¯†åˆ«MVPç‰¹å¾å’Œå¹²æ‰°é¡¹
    print("\nã€å…³é”®æ´å¯Ÿã€‘")
    top_features = global_importance.head(3)
    bottom_features = global_importance.tail(3)
    
    print("\nğŸ† MVPç‰¹å¾ (Top 3):")
    for idx, row in top_features.iterrows():
        print(f"  {row['Feature_Name']}: {row['Total_Importance']:.4f}")
    
    print("\nâš ï¸  æ½œåœ¨å¹²æ‰°é¡¹ (Bottom 3):")
    for idx, row in bottom_features.iterrows():
        print(f"  {row['Feature_Name']}: {row['Total_Importance']:.4f}")
    
    # ç¬¬ä¸‰é˜¶æ®µï¼šåˆ¶å®šè¡ŒåŠ¨ - åŸºäºè¯æ®çš„ä¼˜åŒ–å†³ç­–
    print("\n--- ç¬¬ä¸‰é˜¶æ®µï¼šåˆ¶å®šè¡ŒåŠ¨ ---")
    
    # 3.1 åŸºäºç‰¹å¾æ’åçš„å†³ç­–çŸ©é˜µ
    print("\nã€åŸºäºè¯æ®çš„ä¼˜åŒ–å†³ç­–å»ºè®®ã€‘")
    
    # æ£€æŸ¥å…³é”®ç‰¹å¾çš„è¡¨ç°
    dynamic_wavelet_rank = global_importance[global_importance['Feature_Name'] == 'dynamic_wavelet_ratio_change']
    ll_distortion_rank = global_importance[global_importance['Feature_Name'] == 'll_distortion']
    
    if not dynamic_wavelet_rank.empty:
        dynamic_score = dynamic_wavelet_rank.iloc[0]['Total_Importance']
        if dynamic_score > global_importance['Total_Importance'].median():
            print("âœ… dynamic_wavelet_ratio_change å…¨å±€æ’åå¾ˆé«˜ - ä¿ç•™è¯¥ç‰¹å¾ï¼Œå®ƒæ˜¯å…³é”®æ”¯æŸ±ï¼")
        else:
            print("âš ï¸  dynamic_wavelet_ratio_change è¡¨ç°ä¸€èˆ¬ - è€ƒè™‘ä¼˜åŒ–æˆ–å¢å¼º")
    
    if not ll_distortion_rank.empty:
        ll_score = ll_distortion_rank.iloc[0]['Total_Importance']
        if ll_score > global_importance['Total_Importance'].median():
            print("âœ… ll_distortion å…¨å±€æ’åå¾ˆé«˜ - æ‚¨çš„'ç»“æ„å®¡è®¡å¸ˆ'æŒ‡çº¹éå¸¸æˆåŠŸï¼")
        else:
            print("âš ï¸  ll_distortion è¡¨ç°ä¸€èˆ¬ - éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # è¯†åˆ«éœ€è¦ç§»é™¤çš„æ½œåœ¨å™ªéŸ³ç‰¹å¾
    noise_threshold = global_importance['Total_Importance'].quantile(0.25)  # ä¸‹å››åˆ†ä½æ•°
    potential_noise_features = global_importance[global_importance['Total_Importance'] < noise_threshold]
    
    if not potential_noise_features.empty:
        print(f"\nğŸ” å»ºè®®è¿›è¡Œ'æ§åˆ¶å˜é‡å®éªŒ'çš„ç‰¹å¾ (é‡è¦æ€§ < {noise_threshold:.4f}):")
        for idx, row in potential_noise_features.iterrows():
            print(f"  {row['Feature_Name']}: {row['Total_Importance']:.4f}")
        print("  å»ºè®®ï¼šå»ºç«‹æ–°æ¨¡å‹ç‰ˆæœ¬ï¼Œç§»é™¤è¿™äº›ç‰¹å¾ï¼Œä¸åŸºçº¿æ¨¡å‹è¿›è¡Œæ€§èƒ½å¯¹æ¯”ã€‚")
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ
    print("\n--- ä¿å­˜åˆ†æç»“æœ ---")
    if os.path.exists(latest_run_dir):
        # ä¿å­˜è¯¦ç»†çš„ç‰¹å¾é‡è¦æ€§æ•°æ®
        importance_df.to_csv(os.path.join(latest_run_dir, 'feature_importance_analysis.csv'), index=False)
        global_importance.to_csv(os.path.join(latest_run_dir, 'global_feature_ranking.csv'), index=False)
        
        # ç”Ÿæˆç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
        plt.figure(figsize=(16, 12))
        
        # å­å›¾1ï¼šå„ä¸“å®¶çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯” (ä¼˜åŒ–ï¼šè°ƒæ•´å¸ƒå±€å’Œæ ‡ç­¾)
        plt.subplot(2, 2, 1)
        pivot_df = importance_df.pivot(index='Feature_Name', columns='Expert_Name', values='Importance_Score')
        pivot_df.plot(kind='bar', ax=plt.gca(), width=0.8)
        plt.title('å„ä¸“å®¶çœ¼ä¸­çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯”', fontsize=14, fontweight='bold')
        plt.xticks(rotation=45, ha='right')
        plt.ylabel('é‡è¦æ€§åˆ†æ•°', fontsize=12)
        plt.xlabel('ç‰¹å¾åç§°', fontsize=12)
        plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=10)
        plt.grid(axis='y', alpha=0.3)
        
        # å­å›¾2ï¼šå…¨å±€ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ (ä¼˜åŒ–ï¼šæ›´æ¸…æ™°çš„æ ‡ç­¾å’Œé¢œè‰²)
        plt.subplot(2, 2, 2)
        top_10_features = global_importance.head(10)
        colors = plt.cm.viridis(np.linspace(0, 1, len(top_10_features)))
        bars = plt.barh(range(len(top_10_features)), top_10_features['Total_Importance'], 
                        color=colors, alpha=0.8)
        plt.yticks(range(len(top_10_features)), top_10_features['Feature_Name'], fontsize=10)
        plt.xlabel('æ€»é‡è¦æ€§åˆ†æ•°', fontsize=12)
        plt.title('Top 10 ç‰¹å¾å…¨å±€é‡è¦æ€§æ’è¡Œæ¦œ', fontsize=14, fontweight='bold')
        plt.grid(axis='x', alpha=0.3)
        
        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars, top_10_features['Total_Importance'])):
            plt.text(value + 0.01, i, f'{value:.3f}', va='center', fontsize=9)
        
        # å­å›¾3ï¼šç‰¹å¾ä½¿ç”¨é¢‘ç‡åˆ†å¸ƒ (ä¼˜åŒ–ï¼šæ˜ç¡®æ ‡ç­¾å’Œå«ä¹‰)
        plt.subplot(2, 2, 3)
        feature_usage = global_importance['Expert_Count'].value_counts().sort_index()
        usage_labels = [f'è¢«{i}ä¸ªä¸“å®¶ä½¿ç”¨' for i in feature_usage.index]
        colors_usage = plt.cm.Set3(np.linspace(0, 1, len(feature_usage)))
        
        bars_usage = plt.bar(range(len(feature_usage)), feature_usage.values, 
                             color=colors_usage, alpha=0.8)
        plt.xlabel('ä¸“å®¶ä½¿ç”¨æ¬¡æ•°', fontsize=12)
        plt.ylabel('ç‰¹å¾æ•°é‡', fontsize=12)
        plt.title('ç‰¹å¾è¢«ä¸“å®¶ä½¿ç”¨çš„é¢‘ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        plt.xticks(range(len(feature_usage)), feature_usage.index)
        plt.grid(axis='y', alpha=0.3)
        
        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, value) in enumerate(zip(bars_usage, feature_usage.values)):
            plt.text(i, value + 0.1, str(value), ha='center', va='bottom', fontsize=10)
        
        # å­å›¾4ï¼šé‡è¦æ€§åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾ (ä¼˜åŒ–ï¼šæ›´æ¸…æ™°çš„æ ‡é¢˜å’Œæ ‡ç­¾)
        plt.subplot(2, 2, 4)
        plt.hist(importance_df['Importance_Score'], bins=15, alpha=0.7, 
                 edgecolor='black', color='skyblue', linewidth=1)
        plt.xlabel('ç‰¹å¾é‡è¦æ€§åˆ†æ•°', fontsize=12)
        plt.ylabel('ç‰¹å¾æ•°é‡', fontsize=12)
        plt.title('ç‰¹å¾é‡è¦æ€§åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾', fontsize=14, fontweight='bold')
        plt.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_score = importance_df['Importance_Score'].mean()
        median_score = importance_df['Importance_Score'].median()
        plt.axvline(mean_score, color='red', linestyle='--', linewidth=2, 
                    label=f'å¹³å‡å€¼: {mean_score:.3f}')
        plt.axvline(median_score, color='orange', linestyle='--', linewidth=2, 
                    label=f'ä¸­ä½æ•°: {median_score:.3f}')
        plt.legend(fontsize=10)
        
        # æ•´ä½“å¸ƒå±€ä¼˜åŒ–
        plt.tight_layout(pad=3.0)
        
        # ä¿å­˜å›¾åƒæ—¶ç¡®ä¿å®Œæ•´æ˜¾ç¤º
        plt.savefig(os.path.join(latest_run_dir, 'feature_importance_analysis.png'), 
                   dpi=300, bbox_inches='tight', facecolor='white')
        print(f"ç‰¹å¾é‡è¦æ€§åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {latest_run_dir}")
        
        # æ˜¾ç¤ºå›¾åƒ
        plt.show()
        
        # é¢å¤–ç”Ÿæˆä¸€ä¸ªç®€åŒ–çš„ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾
        print("\n--- ç”Ÿæˆç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾ ---")
        plt.figure(figsize=(12, 8))
        
        # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
        heatmap_data = importance_df.pivot(index='Feature_Name', columns='Expert_Name', values='Importance_Score')
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', 
                    linewidths=0.5, cbar_kws={'label': 'é‡è¦æ€§åˆ†æ•°'})
        plt.title('ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾ - å„ä¸“å®¶è§†è§’å¯¹æ¯”', fontsize=16, fontweight='bold')
        plt.xlabel('ä¸“å®¶ç±»å‹', fontsize=12)
        plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
        
        # è°ƒæ•´xè½´æ ‡ç­¾è§’åº¦
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        plt.tight_layout()
        plt.savefig(os.path.join(latest_run_dir, 'feature_importance_heatmap.png'), 
                   dpi=300, bbox_inches='tight', facecolor='white')
        print(f"ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {latest_run_dir}")
        plt.show()
    
    print("\nç‰¹å¾é‡è¦æ€§å®¡è®¯ä¸åˆ†æå®Œæˆï¼")
    print("åŸºäºåˆ†æç»“æœï¼Œæ‚¨å¯ä»¥åˆ¶å®šç²¾å‡†çš„æ¨¡å‹ä¼˜åŒ–ç­–ç•¥ã€‚")

    # =============================================================================
    # é˜¶æ®µä¸‰ï¼šå¯¹å®Œæ•´çš„"ä¸“å®¶ç»„å†³ç­–ç³»ç»Ÿ"è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    # =============================================================================
    # ... (æ­¤é˜¶æ®µä»£ç ä¸åŸè„šæœ¬å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨) ...
    print("\n" + "="*50)
    print("é˜¶æ®µä¸‰ï¼šå¼€å§‹å¯¹ä¸¤é˜¶æ®µç³»ç»Ÿè¿›è¡Œæœ€ç»ˆè¯„ä¼°")
    print("="*50)
    X_test_imputed = imputer1.transform(X_test_full)
    X_test_scaled = scaler1.transform(X_test_imputed)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_full.columns, index=X_test_full.index)
    print("æµ‹è¯•æ ·æœ¬è¿›å…¥ç³»ç»Ÿ...")
    print(" -> æ­¥éª¤1: 'åˆ†è¯Šå°' (æ¨¡å‹ä¸€) æ­£åœ¨è¿›è¡Œåˆæ­¥è¯Šæ–­...")
    pred1_test = model1.predict(X_test_scaled)
    final_predictions = np.zeros_like(y_test_full)
    non_drift_test_mask = (pred1_test == 0)
    X_test_for_experts = X_test_scaled_df[non_drift_test_mask]
    print(f" -> 'åˆ†è¯Šå°'è¯Šæ–­å®Œæ¯•: {len(X_test_for_experts)} ä¸ªæ ·æœ¬è¢«æäº¤è‡³'ä¸“å®¶ç»„'ã€‚")
    if len(X_test_for_experts) > 0:
        print(" -> æ­¥éª¤2: 'ä¸“å®¶ç»„'å¼€å§‹å¯¹ç–‘éš¾æ ·æœ¬è¿›è¡Œä¼šè¯Š...")
        dynamic_opinions_test = dynamic_expert.predict_proba(X_test_for_experts[features_dynamic])
        frequency_opinions_test = frequency_expert.predict_proba(X_test_for_experts[features_frequency])
        texture_opinions_test = texture_expert.predict_proba(X_test_for_experts[features_texture])
        sensitivity_opinions_test = sensitivity_expert.predict_proba(X_test_for_experts[features_sensitivity])
        X_test_meta = np.hstack([dynamic_opinions_test, frequency_opinions_test, texture_opinions_test, sensitivity_opinions_test])
        expert_predictions = meta_classifier.predict(X_test_meta)
        expert_predictions_original_labels = label_encoder2.inverse_transform(expert_predictions)
        final_predictions[non_drift_test_mask] = expert_predictions_original_labels
        print(" -> 'ä¸“å®¶ç»„'ä¼šè¯Šå®Œæ¯•ã€‚")
    drift_label_encoded = list(label_encoder.classes_).index('drift_parameter')
    final_predictions[pred1_test == 1] = drift_label_encoded
    print("...æ‰€æœ‰æ ·æœ¬é¢„æµ‹æµç¨‹ç»“æŸã€‚")
    print("\n--- ä¸¤é˜¶æ®µä¸“å®¶ç»„å†³ç­–ç³»ç»Ÿæœ€ç»ˆæ€§èƒ½è¯„ä¼° ---")
    print(f"æœ€ç»ˆå‡†ç¡®ç‡: {accuracy_score(y_test_full, final_predictions):.4f}")
    print("\næœ€ç»ˆåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test_full, final_predictions, target_names=label_encoder.classes_))
    print("\næœ€ç»ˆæ··æ·†çŸ©é˜µ:")
    final_cm = confusion_matrix(y_test_full, final_predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(final_cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.title('Final Confusion Matrix for the Two-Stage Expert System')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    # ä¿å­˜å›¾åƒåˆ°æœ€æ–°çš„è¿è¡Œæ–‡ä»¶å¤¹
    if os.path.exists(latest_run_dir):
        plt.savefig(os.path.join(latest_run_dir, 'final_confusion_matrix_expert_system.png'))
        print(f"\næ··æ·†çŸ©é˜µå›¾åƒå·²ä¿å­˜åˆ°: {latest_run_dir}")
    plt.show()
    print("\nè„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼")

    # é‡æ–°è®¾è®¡ç‰¹å¾é‡è¦æ€§å¯è§†åŒ– - æ›´æ¸…æ™°æ˜“æ‡‚
    print("\n--- ç”Ÿæˆæ¸…æ™°æ˜“æ‡‚çš„ç‰¹å¾é‡è¦æ€§åˆ†æå›¾è¡¨ ---")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']  # æ”¯æŒä¸­æ–‡æ˜¾ç¤º
    plt.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    
    # åˆ›å»ºä¸‰ä¸ªç‹¬ç«‹çš„å›¾è¡¨ï¼Œæ¯ä¸ªéƒ½æ¸…æ™°æ˜“æ‡‚
    
    # å›¾è¡¨1ï¼šç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ - æœ€ç›´è§‚çš„å±•ç¤º
    plt.figure(figsize=(14, 8))
    
    # æŒ‰æ€»é‡è¦æ€§æ’åºï¼Œåªæ˜¾ç¤ºå‰15ä¸ªæœ€é‡è¦çš„ç‰¹å¾
    top_features = global_importance.head(15)
    
    # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
    bars = plt.barh(range(len(top_features)), top_features['Total_Importance'], 
                    color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))
    
    # è®¾ç½®Yè½´æ ‡ç­¾ï¼ˆç‰¹å¾åç§°ï¼‰
    plt.yticks(range(len(top_features)), top_features['Feature_Name'], fontsize=11)
    
    # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, value) in enumerate(zip(bars, top_features['Total_Importance'])):
        plt.text(value + 0.01, i, f'{value:.3f}', va='center', fontsize=10, fontweight='bold')
    
    plt.xlabel('æ€»é‡è¦æ€§åˆ†æ•°', fontsize=14)
    plt.title('ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ - å“ªäº›ç‰¹å¾æœ€é‡è¦ï¼Ÿ', fontsize=16, fontweight='bold', pad=20)
    plt.grid(axis='x', alpha=0.3)
    
    # æ·»åŠ è¯´æ˜æ–‡å­—
    plt.figtext(0.02, 0.02, 
                'è¯´æ˜ï¼šåˆ†æ•°è¶Šé«˜ï¼Œè¯¥ç‰¹å¾åœ¨åŒºåˆ†"å¯¹æŠ—æ”»å‡»"ä¸"é«˜æ–¯å™ªå£°"æ—¶è¶Šé‡è¦', 
                fontsize=10, style='italic', color='gray')
    
    plt.tight_layout()
    plt.savefig(os.path.join(latest_run_dir, '01_feature_importance_ranking.png'), 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    # å›¾è¡¨2ï¼šå„ä¸“å®¶çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯” - æ¸…æ™°çš„ä¸“å®¶è§†è§’
    plt.figure(figsize=(16, 10))
    
    # é€‰æ‹©é‡è¦æ€§æœ€é«˜çš„8ä¸ªç‰¹å¾è¿›è¡Œå¯¹æ¯”
    top_8_features = global_importance.head(8)['Feature_Name'].tolist()
    expert_data = importance_df[importance_df['Feature_Name'].isin(top_8_features)]
    
    # åˆ›å»ºåˆ†ç»„æ¡å½¢å›¾
    pivot_data = expert_data.pivot(index='Feature_Name', columns='Expert_Name', values='Importance_Score')
    
    # è®¾ç½®é¢œè‰²æ–¹æ¡ˆ
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    # ç»˜åˆ¶åˆ†ç»„æ¡å½¢å›¾
    x = np.arange(len(top_8_features))
    width = 0.2
    
    for i, (expert_name, color) in enumerate(zip(pivot_data.columns, colors)):
        values = pivot_data[expert_name].values
        plt.bar(x + i*width, values, width, label=expert_name, color=color, alpha=0.8)
    
    plt.xlabel('ç‰¹å¾åç§°', fontsize=14)
    plt.ylabel('é‡è¦æ€§åˆ†æ•°', fontsize=14)
    plt.title('å„ä¸“å®¶çœ¼ä¸­çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
    plt.xticks(x + width*1.5, top_8_features, rotation=45, ha='right')
    plt.legend(title='ä¸“å®¶ç±»å‹', fontsize=12, title_fontsize=12)
    plt.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ è¯´æ˜æ–‡å­—
    plt.figtext(0.02, 0.02, 
                'è¯´æ˜ï¼šæ¯ä¸ªç‰¹å¾åœ¨ä¸åŒä¸“å®¶çœ¼ä¸­çš„é‡è¦æ€§åˆ†æ•°ï¼Œå¸®åŠ©ç†è§£å„ä¸“å®¶çš„å†³ç­–é€»è¾‘', 
                fontsize=10, style='italic', color='gray')
    
    plt.tight_layout()
    plt.savefig(os.path.join(latest_run_dir, '02_expert_comparison.png'), 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    # å›¾è¡¨3ï¼šç‰¹å¾ä½¿ç”¨æƒ…å†µåˆ†æ - ç†è§£ç‰¹å¾åˆ†å¸ƒ
    plt.figure(figsize=(12, 8))
    
    # åˆ›å»ºå­å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # å­å›¾1ï¼šç‰¹å¾è¢«ä¸“å®¶ä½¿ç”¨çš„æ¬¡æ•°
    feature_usage = global_importance['Expert_Count'].value_counts().sort_index()
    usage_labels = [f'è¢«{i}ä¸ªä¸“å®¶ä½¿ç”¨' for i in feature_usage.index]
    
    bars1 = ax1.bar(range(len(feature_usage)), feature_usage.values, 
                     color=['#FF9999', '#66B2FF'], alpha=0.8)
    ax1.set_xlabel('ä¸“å®¶ä½¿ç”¨æ¬¡æ•°', fontsize=12)
    ax1.set_ylabel('ç‰¹å¾æ•°é‡', fontsize=12)
    ax1.set_title('ç‰¹å¾è¢«ä¸“å®¶ä½¿ç”¨çš„é¢‘ç‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax1.set_xticks(range(len(feature_usage)))
    ax1.set_xticklabels(feature_usage.index)
    ax1.grid(axis='y', alpha=0.3)
    
    # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, value) in enumerate(zip(bars1, feature_usage.values)):
        ax1.text(i, value + 0.1, str(value), ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # å­å›¾2ï¼šé‡è¦æ€§åˆ†æ•°åˆ†å¸ƒ
    ax2.hist(importance_df['Importance_Score'], bins=12, alpha=0.7, 
              edgecolor='black', color='lightblue', linewidth=1)
    ax2.set_xlabel('ç‰¹å¾é‡è¦æ€§åˆ†æ•°', fontsize=12)
    ax2.set_ylabel('ç‰¹å¾æ•°é‡', fontsize=12)
    ax2.set_title('ç‰¹å¾é‡è¦æ€§åˆ†æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax2.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡å‚è€ƒçº¿
    mean_score = importance_df['Importance_Score'].mean()
    median_score = importance_df['Importance_Score'].median()
    ax2.axvline(mean_score, color='red', linestyle='--', linewidth=2, 
                 label=f'å¹³å‡å€¼: {mean_score:.3f}')
    ax2.axvline(median_score, color='orange', linestyle='--', linewidth=2, 
                 label=f'ä¸­ä½æ•°: {median_score:.3f}')
    ax2.legend(fontsize=10)
    
    plt.tight_layout()
    plt.savefig(os.path.join(latest_run_dir, '03_feature_analysis.png'), 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    # å›¾è¡¨4ï¼šæ”¹è¿›çš„çƒ­åŠ›å›¾ - æ¸…æ™°æ˜¾ç¤ºæ¯ä¸ªç‰¹å¾åœ¨å„ä¸“å®¶ä¸­çš„é‡è¦æ€§
    plt.figure(figsize=(14, 10))
    
    # é‡æ–°æ•´ç†çƒ­åŠ›å›¾æ•°æ®ï¼Œç¡®ä¿æ¯ä¸ªç‰¹å¾åœ¨æ¯ä¸ªä¸“å®¶ä¸‹éƒ½æœ‰å€¼
    heatmap_data = importance_df.pivot(index='Feature_Name', columns='Expert_Name', values='Importance_Score')
    
    # ä½¿ç”¨æ›´å¥½çš„é¢œè‰²æ–¹æ¡ˆå’Œæ ‡æ³¨
    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd', 
                linewidths=0.5, cbar_kws={'label': 'é‡è¦æ€§åˆ†æ•°'}, 
                square=True, annot_kws={'size': 9})
    
    plt.title('ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾ - æ¯ä¸ªç‰¹å¾åœ¨å„ä¸“å®¶çœ¼ä¸­çš„é‡è¦æ€§', fontsize=16, fontweight='bold')
    plt.xlabel('ä¸“å®¶ç±»å‹', fontsize=12)
    plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
    
    # è°ƒæ•´æ ‡ç­¾è§’åº¦ï¼Œç¡®ä¿å¯è¯»æ€§
    plt.xticks(rotation=0, ha='center')
    plt.yticks(rotation=0)
    
    plt.tight_layout()
    plt.savefig(os.path.join(latest_run_dir, '04_feature_heatmap.png'), 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print(f"\næ‰€æœ‰ä¼˜åŒ–åçš„å›¾è¡¨å·²ä¿å­˜åˆ°: {latest_run_dir}")
    print("ç°åœ¨æ¯ä¸ªå›¾è¡¨éƒ½æœ‰æ¸…æ™°çš„æ ‡é¢˜ã€æ ‡ç­¾å’Œè¯´æ˜ï¼Œåº”è¯¥æ›´å®¹æ˜“ç†è§£äº†ï¼")

if __name__ == '__main__':
    main()
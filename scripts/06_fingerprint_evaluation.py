#!/usr/bin/env python3
"""
è„šæœ¬ 06: æŒ‡çº¹åŒºåˆ†èƒ½åŠ›å…¨é¢è¯„ä¼°
ç›®æ ‡ï¼šå½»åº•è¯„ä¼°ç°æœ‰æŒ‡çº¹ç‰¹å¾å¯¹noise_gaussianå’Œadversarial_pgdçš„åŒºåˆ†èƒ½åŠ›
å¦‚æœç°æœ‰ç‰¹å¾æ— æ³•åŒºåˆ†ï¼Œåˆ™æä¾›å¯»æ‰¾æ–°ç‰¹å¾çš„æŒ‡å¯¼
"""
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import glob
import yaml
import sys
from datetime import datetime
from scipy import stats
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import f_classif, mutual_info_classif, SelectKBest
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc
from sklearn.model_selection import cross_val_score, StratifiedKFold
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class FingerprintEvaluator:
    """æŒ‡çº¹è¯„ä¼°å™¨ç±»"""
    
    def __init__(self, results_dir):
        self.results_dir = results_dir
        self.evaluation_results = {}
        self.data_quality_report = {}
        
    def comprehensive_evaluation(self, data, label_encoder):
        """å…¨é¢è¯„ä¼°æŒ‡çº¹ç‰¹å¾"""
        print("=== å¼€å§‹å…¨é¢æŒ‡çº¹è¯„ä¼° ===")
        
        # 1. æ•°æ®è´¨é‡æ£€æŸ¥
        print("\n1. æ•°æ®è´¨é‡æ£€æŸ¥...")
        self._check_data_quality(data, label_encoder)
        
        # 2. æ•°æ®æ¸…ç†
        print("\n2. æ•°æ®æ¸…ç†...")
        data_cleaned = self._clean_data(data)
        
        # 3. ç‰¹å¾åŒºåˆ†åº¦åˆ†æ
        print("\n3. ç‰¹å¾åŒºåˆ†åº¦åˆ†æ...")
        feature_discrimination = self._analyze_feature_discrimination(data_cleaned, label_encoder)
        
        # 4. ç±»åˆ«é—´å·®å¼‚æ·±åº¦åˆ†æ
        print("\n4. ç±»åˆ«é—´å·®å¼‚æ·±åº¦åˆ†æ...")
        class_differences = self._deep_class_difference_analysis(data_cleaned, label_encoder)
        
        # 5. ç‰¹å¾ç»„åˆåˆ†æ
        print("\n5. ç‰¹å¾ç»„åˆåˆ†æ...")
        feature_combinations = self._analyze_feature_combinations(data_cleaned, label_encoder)
        
        # 6. æœºå™¨å­¦ä¹ æ¨¡å‹éªŒè¯
        print("\n6. æœºå™¨å­¦ä¹ æ¨¡å‹éªŒè¯...")
        ml_validation = self._validate_with_ml_models(data_cleaned, label_encoder)
        
        # 7. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        print("\n7. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        self._generate_comprehensive_report(feature_discrimination, class_differences, 
                                         feature_combinations, ml_validation)
        
        return feature_discrimination, class_differences, feature_combinations, ml_validation
    
    def _clean_data(self, data):
        """æ¸…ç†æ•°æ®ä¸­çš„æ— ç©·å¤§å€¼ã€NaNå€¼å’Œå¼‚å¸¸å€¼"""
        print("æ¸…ç†æ•°æ®...")
        
        data_cleaned = data.copy()
        
        # è·å–æ•°å€¼åˆ—
        numeric_columns = data_cleaned.select_dtypes(include=[np.number]).columns
        if 'vulnerability_type' in numeric_columns:
            numeric_columns = numeric_columns.drop('vulnerability_type')
        
        print(f"å¤„ç† {len(numeric_columns)} ä¸ªæ•°å€¼åˆ—...")
        
        for col in numeric_columns:
            # 1. æ›¿æ¢æ— ç©·å¤§å€¼
            data_cleaned[col] = data_cleaned[col].replace([np.inf, -np.inf], np.nan)
            
            # 2. è®¡ç®—åˆ†ä½æ•°æ¥è¯†åˆ«å¼‚å¸¸å€¼
            Q1 = data_cleaned[col].quantile(0.01)  # 1%åˆ†ä½æ•°
            Q3 = data_cleaned[col].quantile(0.99)  # 99%åˆ†ä½æ•°
            IQR = Q3 - Q1
            
            # å°†å¼‚å¸¸å€¼æ›¿æ¢ä¸ºåˆ†ä½æ•°è¾¹ç•Œå€¼
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            data_cleaned[col] = data_cleaned[col].clip(lower=lower_bound, upper=upper_bound)
            
            # 3. ç”¨ä¸­ä½æ•°å¡«å……NaNå€¼
            if data_cleaned[col].isna().sum() > 0:
                median_val = data_cleaned[col].median()
                data_cleaned[col].fillna(median_val, inplace=True)
        
        # æœ€ç»ˆæ£€æŸ¥
        remaining_nan = data_cleaned.isnull().sum().sum()
        remaining_inf = np.isinf(data_cleaned.select_dtypes(include=[np.number])).sum().sum()
        
        print(f"æ¸…ç†å®Œæˆ: å‰©ä½™NaNå€¼ {remaining_nan}, å‰©ä½™æ— ç©·å¤§å€¼ {remaining_inf}")
        
        return data_cleaned
        
    def _check_data_quality(self, data, label_encoder):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        print("æ£€æŸ¥æ•°æ®è´¨é‡...")
        
        quality_report = {}
        
        # 1. æ ‡ç­¾åˆ†å¸ƒæ£€æŸ¥
        label_counts = data['vulnerability_type'].value_counts()
        quality_report['label_distribution'] = label_counts.to_dict()
        
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {label_counts.to_dict()}")
        
        # 2. é‡å¤æ ·æœ¬æ£€æŸ¥
        duplicate_samples = data.duplicated().sum()
        quality_report['duplicate_samples'] = duplicate_samples
        
        print(f"é‡å¤æ ·æœ¬æ•°é‡: {duplicate_samples}")
        
        # 3. ç¼ºå¤±å€¼æ£€æŸ¥
        missing_values = data.isnull().sum()
        quality_report['missing_values'] = missing_values.to_dict()
        
        print(f"ç¼ºå¤±å€¼ç»Ÿè®¡: {missing_values.sum()} ä¸ªæ€»ç¼ºå¤±å€¼")
        
        # 4. ç‰¹å¾å€¼èŒƒå›´æ£€æŸ¥
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        if 'vulnerability_type' in numeric_cols:
            numeric_cols = numeric_cols.drop('vulnerability_type')
        
        feature_ranges = {}
        for col in numeric_cols:
            col_data = data[col]
            feature_ranges[col] = {
                'min': col_data.min(),
                'max': col_data.max(),
                'mean': col_data.mean(),
                'std': col_data.std(),
                'has_inf': np.isinf(col_data).any(),
                'has_nan': col_data.isna().any()
            }
        
        quality_report['feature_ranges'] = feature_ranges
        
        # 5. å¼‚å¸¸å€¼æ£€æŸ¥
        outlier_report = {}
        for col in numeric_cols:
            col_data = data[col]
            Q1 = col_data.quantile(0.25)
            Q3 = col_data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = ((col_data < lower_bound) | (col_data > upper_bound)).sum()
            outlier_report[col] = {
                'outlier_count': outliers,
                'outlier_percentage': outliers / len(col_data) * 100
            }
        
        quality_report['outlier_report'] = outlier_report
        
        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        quality_df = pd.DataFrame(quality_report['feature_ranges']).T
        quality_df.to_csv(os.path.join(self.results_dir, 'data_quality_report.csv'))
        
        self.data_quality_report = quality_report
        
        print("âœ… æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ")
        return quality_report
    
    def _analyze_feature_discrimination(self, data, label_encoder):
        """åˆ†æç‰¹å¾åŒºåˆ†èƒ½åŠ›"""
        print("åˆ†æç‰¹å¾åŒºåˆ†èƒ½åŠ›...")
        
        # å‡†å¤‡æ•°æ®
        X = data.drop('vulnerability_type', axis=1)
        y = label_encoder.transform(data['vulnerability_type'])
        
        # æœ€ç»ˆæ£€æŸ¥æ•°æ®æ˜¯å¦å¹²å‡€
        if X.isnull().any().any() or np.isinf(X.values).any():
            print("è­¦å‘Šï¼šæ•°æ®ä¸­ä»åŒ…å«NaNæˆ–æ— ç©·å¤§å€¼ï¼Œè¿›è¡Œæœ€ç»ˆæ¸…ç†...")
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(X.median())
        
        print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
        print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
        
        # 1. Fç»Ÿè®¡é‡åˆ†æï¼ˆANOVAï¼‰
        f_scores, p_values = f_classif(X, y)
        
        # 2. äº’ä¿¡æ¯åˆ†æ
        mi_scores = mutual_info_classif(X, y, random_state=42)
        
        # 3. åˆ›å»ºç‰¹å¾åŒºåˆ†åº¦DataFrame
        feature_discrimination = pd.DataFrame({
            'Feature_Name': X.columns,
            'F_Score': f_scores,
            'P_Value': p_values,
            'Mutual_Info': mi_scores,
            'Significant_F': p_values < 0.05,
            'F_Rank': f_scores.argsort()[::-1].argsort() + 1,
            'MI_Rank': mi_scores.argsort()[::-1].argsort() + 1
        }).sort_values('F_Score', ascending=False)
        
        # 4. ä¿å­˜ç»“æœ
        feature_discrimination.to_csv(os.path.join(self.results_dir, 'feature_discrimination_analysis.csv'), index=False)
        
        # 5. æ˜¾ç¤ºç»“æœ
        print(f"\nï¿½ï¿½ ç‰¹å¾åŒºåˆ†èƒ½åŠ›æ’è¡Œæ¦œ (Top 20, æŒ‰Fç»Ÿè®¡é‡):")
        for i, (_, row) in enumerate(feature_discrimination.head(20).iterrows()):
            significance = "***" if row['Significant_F'] else ""
            print(f"   {i+1:2d}. {row['Feature_Name']:<25} : F={row['F_Score']:.4f}, "
                  f"p={row['P_Value']:.6f}, MI={row['Mutual_Info']:.4f} {significance}")
        
        # 6. åˆ›å»ºå¯è§†åŒ–
        self._create_discrimination_visualization(feature_discrimination)
        
        return feature_discrimination
    
    def _create_discrimination_visualization(self, feature_discrimination):
        """åˆ›å»ºåŒºåˆ†èƒ½åŠ›å¯è§†åŒ–"""
        # 1. Fç»Ÿè®¡é‡ vs äº’ä¿¡æ¯æ•£ç‚¹å›¾
        plt.figure(figsize=(12, 8))
        
        plt.scatter(feature_discrimination['F_Score'], feature_discrimination['Mutual_Info'], 
                   alpha=0.7, s=50)
        
        # æ ‡æ³¨å‰10ä¸ªç‰¹å¾
        top_10 = feature_discrimination.head(10)
        for _, row in top_10.iterrows():
            plt.annotate(row['Feature_Name'], 
                        (row['F_Score'], row['Mutual_Info']),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=8, alpha=0.8)
        
        plt.xlabel('Fç»Ÿè®¡é‡', fontsize=12, fontweight='bold')
        plt.ylabel('äº’ä¿¡æ¯', fontsize=12, fontweight='bold')
        plt.title('ç‰¹å¾åŒºåˆ†èƒ½åŠ›å¯¹æ¯”: Fç»Ÿè®¡é‡ vs äº’ä¿¡æ¯', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(os.path.join(self.results_dir, 'feature_discrimination_scatter.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Top 20ç‰¹å¾Fç»Ÿè®¡é‡æ¡å½¢å›¾
        plt.figure(figsize=(16, 10))
        
        top_20 = feature_discrimination.head(20)
        bars = plt.barh(range(len(top_20)), top_20['F_Score'], 
                        color=plt.cm.viridis(np.linspace(0, 1, len(top_20))))
        
        plt.yticks(range(len(top_20)), top_20['Feature_Name'], fontsize=10)
        
        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, f_score) in enumerate(zip(bars, top_20['F_Score'])):
            plt.text(f_score + 0.001, i, f'{f_score:.3f}', 
                    va='center', fontsize=9, fontweight='bold')
        
        plt.xlabel('Fç»Ÿè®¡é‡', fontsize=12, fontweight='bold')
        plt.title('Top 20ç‰¹å¾åŒºåˆ†èƒ½åŠ› (Fç»Ÿè®¡é‡)', fontsize=14, fontweight='bold')
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(os.path.join(self.results_dir, 'top_features_f_score.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _deep_class_difference_analysis(self, data, label_encoder):
        """æ·±åº¦åˆ†æç±»åˆ«é—´çš„ç‰¹å¾å·®å¼‚"""
        print("æ·±åº¦åˆ†æç±»åˆ«é—´å·®å¼‚...")
        
        classes = label_encoder.classes_
        target_classes = ['noise_gaussian', 'adversarial_pgd']
        
        # åˆ†ææ¯ä¸ªç‰¹å¾åœ¨ä¸¤ä¸ªç±»åˆ«ä¸Šçš„åˆ†å¸ƒ
        feature_analysis = {}
        
        for feature in data.columns:
            if feature == 'vulnerability_type':
                continue
                
            if data[feature].dtype in ['float64', 'int64']:
                class_stats = {}
                
                for class_name in target_classes:
                    if class_name in classes:
                        class_data = data[data['vulnerability_type'] == class_name][feature]
                        if len(class_data) > 0:
                            class_stats[class_name] = {
                                'mean': class_data.mean(),
                                'std': class_data.std(),
                                'median': class_data.median(),
                                'q25': class_data.quantile(0.25),
                                'q75': class_data.quantile(0.75),
                                'min': class_data.min(),
                                'max': class_data.max(),
                                'skewness': class_data.skew(),
                                'kurtosis': class_data.kurtosis()
                            }
                
                # è®¡ç®—ç±»åˆ«é—´å·®å¼‚
                if len(class_stats) == 2:
                    # Cohen's dæ•ˆåº”é‡
                    mean_diff = abs(class_stats[target_classes[0]]['mean'] - class_stats[target_classes[1]]['mean'])
                    pooled_std = np.sqrt((class_stats[target_classes[0]]['std']**2 + class_stats[target_classes[1]]['std']**2) / 2)
                    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0
                    
                    # é‡å åº¦ï¼ˆOverlapï¼‰
                    overlap = self._calculate_overlap(class_stats[target_classes[0]], class_stats[target_classes[1]])
                    
                    # è¿›è¡Œtæ£€éªŒ
                    class1_data = data[data['vulnerability_type'] == target_classes[0]][feature]
                    class2_data = data[data['vulnerability_type'] == target_classes[1]][feature]
                    
                    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
                    t_stat, p_value, mannwhitney_u, mw_p_value = 0, 1, 0, 1
                    
                    if len(class1_data) > 0 and len(class2_data) > 0:
                        try:
                            t_stat, p_value = stats.ttest_ind(class1_data, class2_data)
                            mannwhitney_u, mw_p_value = stats.mannwhitneyu(class1_data, class2_data, alternative='two-sided')
                        except Exception as e:
                            print(f"è­¦å‘Šï¼šç‰¹å¾ {feature} çš„ç»Ÿè®¡æ£€éªŒå¤±è´¥: {e}")
                            # ä½¿ç”¨é»˜è®¤å€¼
                            t_stat, p_value, mannwhitney_u, mw_p_value = 0, 1, 0, 1
                    
                    feature_analysis[feature] = {
                        'cohens_d': cohens_d,
                        'overlap': overlap,
                        't_statistic': t_stat,
                        't_p_value': p_value,
                        'mannwhitney_u': mannwhitney_u,
                        'mw_p_value': mw_p_value,
                        'class_stats': class_stats,
                        'effect_size': self._classify_effect_size(cohens_d)
                    }
        
        # æŒ‰æ•ˆåº”é‡æ’åº
        sorted_features = sorted(feature_analysis.items(), key=lambda x: x[1]['cohens_d'], reverse=True)
        
        print(f"\nğŸ“Š ç‰¹å¾åŒºåˆ†åº¦æ’åº (æŒ‰Cohen's dæ•ˆåº”é‡):")
        for i, (feature, stats) in enumerate(sorted_features[:20]):
            print(f"   {i+1:2d}. {feature:<25} : d={stats['cohens_d']:.4f} ({stats['effect_size']}), "
                  f"é‡å åº¦={stats['overlap']:.2%}, tæ£€éªŒp={stats['t_p_value']:.6f}")
        
        # ä¿å­˜ç»“æœ
        analysis_df = pd.DataFrame([
            {
                'Feature_Name': feature,
                'Cohens_d': stats['cohens_d'],
                'Effect_Size': stats['effect_size'],
                'Overlap': stats['overlap'],
                'T_Statistic': stats['t_statistic'],
                'T_P_Value': stats['t_p_value'],
                'MannWhitney_U': stats['mannwhitney_u'],
                'MW_P_Value': stats['mw_p_value']
            }
            for feature, stats in feature_analysis.items()
        ]).sort_values('Cohens_d', ascending=False)
        
        analysis_df.to_csv(os.path.join(self.results_dir, 'class_difference_analysis.csv'), index=False)
        
        # åˆ›å»ºå¯è§†åŒ–
        self._create_class_difference_visualization(analysis_df)
        
        return feature_analysis
    
    def _calculate_overlap(self, stats1, stats2):
        """è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒçš„é‡å åº¦"""
        # ä½¿ç”¨å››åˆ†ä½æ•°èŒƒå›´è®¡ç®—é‡å åº¦
        range1 = (stats1['q25'], stats1['q75'])
        range2 = (stats2['q25'], stats2['q75'])
        
        overlap_start = max(range1[0], range2[0])
        overlap_end = min(range1[1], range2[1])
        
        if overlap_start >= overlap_end:
            return 0.0
        
        overlap_length = overlap_end - overlap_start
        total_length = (range1[1] - range1[0]) + (range2[1] - range2[0])
        
        return overlap_length / total_length if total_length > 0 else 0.0
    
    def _classify_effect_size(self, cohens_d):
        """åˆ†ç±»æ•ˆåº”é‡å¤§å°"""
        if cohens_d < 0.2:
            return "å¾®å°"
        elif cohens_d < 0.5:
            return "å°"
        elif cohens_d < 0.8:
            return "ä¸­ç­‰"
        else:
            return "å¤§"
    
    def _create_class_difference_visualization(self, analysis_df):
        """åˆ›å»ºç±»åˆ«å·®å¼‚å¯è§†åŒ–"""
        # 1. Cohen's dæ•ˆåº”é‡åˆ†å¸ƒ
        plt.figure(figsize=(15, 10))
        
        # å­å›¾1ï¼šæ•ˆåº”é‡åˆ†å¸ƒç›´æ–¹å›¾
        ax1 = plt.subplot(2, 2, 1)
        plt.hist(analysis_df['Cohens_d'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        plt.xlabel("Cohen's dæ•ˆåº”é‡", fontsize=12, fontweight='bold')
        plt.ylabel('ç‰¹å¾æ•°é‡', fontsize=12, fontweight='bold')
        plt.title("æ•ˆåº”é‡åˆ†å¸ƒ", fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        
        # å­å›¾2ï¼šé‡å åº¦åˆ†å¸ƒ
        ax2 = plt.subplot(2, 2, 2)
        plt.hist(analysis_df['Overlap'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
        plt.xlabel('é‡å åº¦', fontsize=12, fontweight='bold')
        plt.ylabel('ç‰¹å¾æ•°é‡', fontsize=12, fontweight='bold')
        plt.title('ç±»åˆ«é‡å åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        
        # å­å›¾3ï¼šæ•ˆåº”é‡ vs é‡å åº¦æ•£ç‚¹å›¾
        ax3 = plt.subplot(2, 2, 3)
        plt.scatter(analysis_df['Cohens_d'], analysis_df['Overlap'], alpha=0.6, s=30)
        plt.xlabel("Cohen's dæ•ˆåº”é‡", fontsize=12, fontweight='bold')
        plt.ylabel('é‡å åº¦', fontsize=12, fontweight='bold')
        plt.title('æ•ˆåº”é‡ vs é‡å åº¦', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        
        # å­å›¾4ï¼šTop 15ç‰¹å¾æ•ˆåº”é‡
        ax4 = plt.subplot(2, 2, 4)
        top_15 = analysis_df.head(15)
        bars = plt.barh(range(len(top_15)), top_15['Cohens_d'], 
                        color=plt.cm.viridis(np.linspace(0, 1, len(top_15))))
        
        plt.yticks(range(len(top_15)), top_15['Feature_Name'], fontsize=8)
        plt.xlabel("Cohen's dæ•ˆåº”é‡", fontsize=12, fontweight='bold')
        plt.title('Top 15ç‰¹å¾æ•ˆåº”é‡', fontsize=14, fontweight='bold')
        plt.grid(axis='x', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'class_difference_analysis.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _analyze_feature_combinations(self, data, label_encoder):
        """åˆ†æç‰¹å¾ç»„åˆçš„åŒºåˆ†èƒ½åŠ›"""
        print("åˆ†æç‰¹å¾ç»„åˆ...")
        
        # è·å–Topç‰¹å¾
        X = data.drop('vulnerability_type', axis=1)
        y = label_encoder.transform(data['vulnerability_type'])
        
        # ä½¿ç”¨Fç»Ÿè®¡é‡é€‰æ‹©Topç‰¹å¾
        selector = SelectKBest(f_classif, k=min(20, X.shape[1]))
        X_selected = selector.fit_transform(X, y)
        selected_features = X.columns[selector.get_support()].tolist()
        
        print(f"é€‰æ‹©çš„Top {len(selected_features)} ç‰¹å¾: {selected_features}")
        
        # åˆ†æç‰¹å¾ç»„åˆ
        combination_results = []
        
        # 2ç‰¹å¾ç»„åˆ
        print("åˆ†æ2ç‰¹å¾ç»„åˆ...")
        for i in range(len(selected_features)):
            for j in range(i+1, len(selected_features)):
                feat1, feat2 = selected_features[i], selected_features[j]
                
                # åˆ›å»ºç»„åˆç‰¹å¾
                X_combined = X[[feat1, feat2]]
                
                # ä½¿ç”¨éšæœºæ£®æ—è¯„ä¼°ç»„åˆç‰¹å¾
                rf = RandomForestClassifier(n_estimators=50, random_state=42)
                cv_scores = cross_val_score(rf, X_combined, y, cv=5, scoring='accuracy')
                
                combination_results.append({
                    'Feature1': feat1,
                    'Feature2': feat2,
                    'Mean_Accuracy': cv_scores.mean(),
                    'Std_Accuracy': cv_scores.std(),
                    'Combination_Type': '2_features'
                })
        
        # 3ç‰¹å¾ç»„åˆï¼ˆé€‰æ‹©Top 10ä¸ª2ç‰¹å¾ç»„åˆï¼‰
        print("åˆ†æ3ç‰¹å¾ç»„åˆ...")
        top_2_combinations = sorted(combination_results, key=lambda x: x['Mean_Accuracy'], reverse=True)[:10]
        
        for combo in top_2_combinations:
            feat1, feat2 = combo['Feature1'], combo['Feature2']
            
            # ä¸ºæ¯ä¸ª2ç‰¹å¾ç»„åˆæ·»åŠ ä¸€ä¸ªé¢å¤–çš„ç‰¹å¾
            for feat3 in selected_features:
                if feat3 not in [feat1, feat2]:
                    X_combined = X[[feat1, feat2, feat3]]
                    
                    rf = RandomForestClassifier(n_estimators=50, random_state=42)
                    cv_scores = cross_val_score(rf, X_combined, y, cv=5, scoring='accuracy')
                    
                    combination_results.append({
                        'Feature1': feat1,
                        'Feature2': feat2,
                        'Feature3': feat3,
                        'Mean_Accuracy': cv_scores.mean(),
                        'Std_Accuracy': cv_scores.std(),
                        'Combination_Type': '3_features'
                    })
        
        # ä¿å­˜ç»“æœ
        combination_df = pd.DataFrame(combination_results)
        combination_df.to_csv(os.path.join(self.results_dir, 'feature_combination_analysis.csv'), index=False)
        
        # æ˜¾ç¤ºæœ€ä½³ç»„åˆ
        print(f"\nğŸ“Š æœ€ä½³ç‰¹å¾ç»„åˆ (Top 10):")
        best_combinations = combination_df.sort_values('Mean_Accuracy', ascending=False).head(10)
        for i, (_, row) in enumerate(best_combinations.iterrows()):
            if row['Combination_Type'] == '2_features':
                print(f"   {i+1:2d}. {row['Feature1']} + {row['Feature2']} : "
                      f"å‡†ç¡®ç‡={row['Mean_Accuracy']:.4f} Â± {row['Std_Accuracy']:.4f}")
            else:
                print(f"   {i+1:2d}. {row['Feature1']} + {row['Feature2']} + {row['Feature3']} : "
                      f"å‡†ç¡®ç‡={row['Mean_Accuracy']:.4f} Â± {row['Std_Accuracy']:.4f}")
        
        return combination_results
    
    def _validate_with_ml_models(self, data, label_encoder):
        """ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹éªŒè¯ç‰¹å¾åŒºåˆ†èƒ½åŠ›"""
        print("ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹éªŒè¯...")
        
        X = data.drop('vulnerability_type', axis=1)
        y = label_encoder.transform(data['vulnerability_type'])
        
        # æœ€ç»ˆæ£€æŸ¥æ•°æ®
        if X.isnull().any().any() or np.isinf(X.values).any():
            print("è­¦å‘Šï¼šæ¨¡å‹éªŒè¯å‰è¿›è¡Œæœ€ç»ˆæ•°æ®æ¸…ç†...")
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(X.median())
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ä½¿ç”¨å¤šä¸ªæ¨¡å‹éªŒè¯
        models = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
        }
        
        validation_results = {}
        
        for model_name, model in models.items():
            print(f"éªŒè¯ {model_name}...")
            
            # äº¤å‰éªŒè¯
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            
            # å‡†ç¡®ç‡
            accuracy_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')
            
            # ROC AUCï¼ˆäºŒåˆ†ç±»é—®é¢˜ï¼‰
            if len(np.unique(y)) == 2:
                roc_auc_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='roc_auc')
            else:
                roc_auc_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='roc_auc_ovr')
            
            # F1åˆ†æ•°
            f1_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='f1_weighted')
            
            validation_results[model_name] = {
                'Accuracy_Mean': accuracy_scores.mean(),
                'Accuracy_Std': accuracy_scores.std(),
                'ROC_AUC_Mean': roc_auc_scores.mean(),
                'ROC_AUC_Std': roc_auc_scores.std(),
                'F1_Mean': f1_scores.mean(),
                'F1_Std': f1_scores.std()
            }
        
        # ä¿å­˜éªŒè¯ç»“æœ
        validation_df = pd.DataFrame(validation_results).T
        validation_df.to_csv(os.path.join(self.results_dir, 'ml_validation_results.csv'))
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š æœºå™¨å­¦ä¹ æ¨¡å‹éªŒè¯ç»“æœ:")
        for model_name, results in validation_results.items():
            print(f"   {model_name}:")
            print(f"     å‡†ç¡®ç‡: {results['Accuracy_Mean']:.4f} Â± {results['Accuracy_Std']:.4f}")
            print(f"     ROC AUC: {results['ROC_AUC_Mean']:.4f} Â± {results['ROC_AUC_Std']:.4f}")
            print(f"     F1åˆ†æ•°: {results['F1_Mean']:.4f} Â± {results['F1_Std']:.4f}")
        
        return validation_results
    
    def _generate_comprehensive_report(self, feature_discrimination, class_differences, 
                                     feature_combinations, ml_validation):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print("ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
        
        report_path = os.path.join(self.results_dir, 'fingerprint_evaluation_report.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("æŒ‡çº¹ç‰¹å¾åŒºåˆ†èƒ½åŠ›å…¨é¢è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("è¯„ä¼°æ—¶é—´: " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            
            # æ•°æ®è´¨é‡æ€»ç»“
            f.write("1. æ•°æ®è´¨é‡æ€»ç»“\n")
            f.write("-" * 40 + "\n")
            if self.data_quality_report:
                f.write(f"æ€»æ ·æœ¬æ•°: {sum(self.data_quality_report['label_distribution'].values())}\n")
                f.write(f"æ ‡ç­¾åˆ†å¸ƒ: {self.data_quality_report['label_distribution']}\n")
                f.write(f"é‡å¤æ ·æœ¬: {self.data_quality_report['duplicate_samples']}\n")
                f.write(f"ç¼ºå¤±å€¼æ€»æ•°: {sum(self.data_quality_report['missing_values'].values())}\n\n")
            
            # ç‰¹å¾åŒºåˆ†èƒ½åŠ›æ€»ç»“
            f.write("2. ç‰¹å¾åŒºåˆ†èƒ½åŠ›æ€»ç»“\n")
            f.write("-" * 40 + "\n")
            if not feature_discrimination.empty:
                significant_features = feature_discrimination[feature_discrimination['Significant_F'] == True]
                f.write(f"æ€»ç‰¹å¾æ•°: {len(feature_discrimination)}\n")
                f.write(f"ç»Ÿè®¡æ˜¾è‘—ç‰¹å¾æ•°: {len(significant_features)}\n")
                f.write(f"æ˜¾è‘—ç‰¹å¾æ¯”ä¾‹: {len(significant_features)/len(feature_discrimination)*100:.2f}%\n\n")
                
                f.write("Top 10ç‰¹å¾ (æŒ‰Fç»Ÿè®¡é‡):\n")
                for i, (_, row) in enumerate(feature_discrimination.head(10).iterrows()):
                    f.write(f"   {i+1:2d}. {row['Feature_Name']:<25} : F={row['F_Score']:.4f}, "
                           f"p={row['P_Value']:.6f}\n")
                f.write("\n")
            
            # ç±»åˆ«å·®å¼‚æ€»ç»“
            f.write("3. ç±»åˆ«å·®å¼‚æ€»ç»“\n")
            f.write("-" * 40 + "\n")
            if class_differences:
                large_effect_features = [f for f, stats in class_differences.items() 
                                       if stats['effect_size'] in ['å¤§', 'ä¸­ç­‰']]
                f.write(f"å¤§/ä¸­ç­‰æ•ˆåº”é‡ç‰¹å¾æ•°: {len(large_effect_features)}\n")
                f.write(f"å¾®å°/å°æ•ˆåº”é‡ç‰¹å¾æ•°: {len(class_differences) - len(large_effect_features)}\n\n")
                
                f.write("Top 10ç‰¹å¾ (æŒ‰Cohen's dæ•ˆåº”é‡):\n")
                sorted_features = sorted(class_differences.items(), key=lambda x: x[1]['cohens_d'], reverse=True)
                for i, (feature, stats) in enumerate(sorted_features[:10]):
                    f.write(f"   {i+1:2d}. {feature:<25} : d={stats['cohens_d']:.4f} ({stats['effect_size']})\n")
                f.write("\n")
            
            # ç‰¹å¾ç»„åˆæ€»ç»“
            f.write("4. ç‰¹å¾ç»„åˆæ€»ç»“\n")
            f.write("-" * 40 + "\n")
            if feature_combinations:
                best_2_feature = max([c for c in feature_combinations if c['Combination_Type'] == '2_features'], 
                                   key=lambda x: x['Mean_Accuracy'])
                best_3_feature = max([c for c in feature_combinations if c['Combination_Type'] == '3_features'], 
                                   key=lambda x: x['Mean_Accuracy'])
                
                f.write(f"æœ€ä½³2ç‰¹å¾ç»„åˆ: {best_2_feature['Feature1']} + {best_2_feature['Feature2']}\n")
                f.write(f"å‡†ç¡®ç‡: {best_2_feature['Mean_Accuracy']:.4f} Â± {best_2_feature['Std_Accuracy']:.4f}\n\n")
                
                f.write(f"æœ€ä½³3ç‰¹å¾ç»„åˆ: {best_3_feature['Feature1']} + {best_3_feature['Feature2']} + {best_3_feature['Feature3']}\n")
                f.write(f"å‡†ç¡®ç‡: {best_3_feature['Mean_Accuracy']:.4f} Â± {best_3_feature['Std_Accuracy']:.4f}\n\n")
            
            # æœºå™¨å­¦ä¹ éªŒè¯æ€»ç»“
            f.write("5. æœºå™¨å­¦ä¹ éªŒè¯æ€»ç»“\n")
            f.write("-" * 40 + "\n")
            if ml_validation:
                for model_name, results in ml_validation.items():
                    f.write(f"{model_name}:\n")
                    f.write(f"  å‡†ç¡®ç‡: {results['Accuracy_Mean']:.4f} Â± {results['Accuracy_Std']:.4f}\n")
                    f.write(f"  ROC AUC: {results['ROC_AUC_Mean']:.4f} Â± {results['ROC_AUC_Std']:.4f}\n")
                    f.write(f"  F1åˆ†æ•°: {results['F1_Mean']:.4f} Â± {results['F1_Std']:.4f}\n\n")
            
            # ç»“è®ºå’Œå»ºè®®
            f.write("6. ç»“è®ºå’Œå»ºè®®\n")
            f.write("-" * 40 + "\n")
            
            # åŸºäºç»“æœç»™å‡ºå»ºè®®
            if feature_discrimination.empty or class_differences is None:
                f.write("æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç»™å‡ºæœ‰æ•ˆç»“è®ºã€‚\n")
            else:
                significant_count = len(feature_discrimination[feature_discrimination['Significant_F'] == True])
                large_effect_count = len([f for f, stats in class_differences.items() 
                                        if stats['effect_size'] in ['å¤§', 'ä¸­ç­‰']])
                
                if significant_count > len(feature_discrimination) * 0.3 and large_effect_count > 5:
                    f.write("âœ… ç°æœ‰ç‰¹å¾é›†åˆå…·æœ‰è¾ƒå¥½çš„åŒºåˆ†èƒ½åŠ›ï¼Œå»ºè®®:\n")
                    f.write("   - ä½¿ç”¨Topç‰¹å¾è¿›è¡Œæ¨¡å‹è®­ç»ƒ\n")
                    f.write("   - ä¼˜åŒ–ç‰¹å¾ç»„åˆ\n")
                    f.write("   - è°ƒæ•´æ¨¡å‹å‚æ•°\n")
                elif significant_count > 0 and large_effect_count > 0:
                    f.write("âš ï¸  ç°æœ‰ç‰¹å¾é›†åˆå…·æœ‰æœ‰é™çš„åŒºåˆ†èƒ½åŠ›ï¼Œå»ºè®®:\n")
                    f.write("   - ä½¿ç”¨æœ€ä½³ç‰¹å¾ç»„åˆ\n")
                    f.write("   - è€ƒè™‘ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–\n")
                    f.write("   - å¯»æ‰¾æ–°çš„åŒºåˆ†ç‰¹å¾\n")
                else:
                    f.write("âŒ ç°æœ‰ç‰¹å¾é›†åˆåŒºåˆ†èƒ½åŠ›ä¸è¶³ï¼Œå»ºè®®:\n")
                    f.write("   - é‡æ–°å®¡è§†æ•°æ®æ ‡æ³¨\n")
                    f.write("   - å¯»æ‰¾æ–°çš„ç‰¹å¾æ¥æº\n")
                    f.write("   - è€ƒè™‘ä¸åŒçš„åˆ†ç±»ç­–ç•¥\n")
            
            f.write("\nç”Ÿæˆçš„æ–‡ä»¶:\n")
            f.write("-" * 40 + "\n")
            for file in os.listdir(self.results_dir):
                if file.endswith(('.png', '.csv', '.txt')):
                    f.write(f"  {file}\n")
        
        print(f"âœ… ç»¼åˆè¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=== è„šæœ¬ 06: æŒ‡çº¹åŒºåˆ†èƒ½åŠ›å…¨é¢è¯„ä¼° ===")
    
    # åŠ è½½é…ç½®
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    try:
        # 1. åŠ è½½æ•°æ®
        print("åŠ è½½æ•°æ®...")
        runs_dir = config['output_paths']['runs_directory']
        candidate_csvs = glob.glob(os.path.join(runs_dir, '*/vulnerability_fingerprints.csv'))
        
        if not candidate_csvs:
            print("\né”™è¯¯ï¼šæ‰¾ä¸åˆ° vulnerability_fingerprints.csv æ–‡ä»¶ã€‚")
            sys.exit(1)
        
        fingerprint_file_path = max(candidate_csvs, key=os.path.getctime)
        print(f"ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶: {fingerprint_file_path}")
        
        data = pd.read_csv(fingerprint_file_path)
        print(f"æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ ·æœ¬")
        
        # 2. åˆ›å»ºç»“æœç›®å½•
        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        output_dir = os.path.join(runs_dir, 'output')
        os.makedirs(output_dir, exist_ok=True)
        
        results_dir = os.path.join(output_dir, f"fingerprint_evaluation_{current_time}")
        os.makedirs(results_dir, exist_ok=True)
        print(f"åˆ›å»ºç»“æœç›®å½•: {results_dir}")
        
        # 3. åˆ›å»ºæ ‡ç­¾ç¼–ç å™¨
        label_encoder = LabelEncoder()
        label_encoder.fit(data['vulnerability_type'])
        
        # 4. åˆ›å»ºæŒ‡çº¹è¯„ä¼°å™¨
        evaluator = FingerprintEvaluator(results_dir)
        
        # 5. æ‰§è¡Œå…¨é¢è¯„ä¼°
        print("\nå¼€å§‹å…¨é¢æŒ‡çº¹è¯„ä¼°...")
        
        feature_discrimination, class_differences, feature_combinations, ml_validation = \
            evaluator.comprehensive_evaluation(data, label_encoder)
        
        print(f"\nâœ… æŒ‡çº¹è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {results_dir}")
        print("\nğŸ“Š ä¸»è¦è¯„ä¼°å†…å®¹:")
        print("   - æ•°æ®è´¨é‡æ£€æŸ¥")
        print("   - æ•°æ®æ¸…ç†")
        print("   - ç‰¹å¾åŒºåˆ†èƒ½åŠ›åˆ†æ")
        print("   - ç±»åˆ«é—´å·®å¼‚æ·±åº¦åˆ†æ")
        print("   - ç‰¹å¾ç»„åˆåˆ†æ")
        print("   - æœºå™¨å­¦ä¹ æ¨¡å‹éªŒè¯")
        print("   - ç»¼åˆè¯„ä¼°æŠ¥å‘Š")
        
    except Exception as e:
        print(f"\nâŒ è„šæœ¬æ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
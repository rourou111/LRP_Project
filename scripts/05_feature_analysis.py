#!/usr/bin/env python3
"""
è„šæœ¬ 05: æ·±åº¦ç‰¹å¾åˆ†æ - å¯»æ‰¾noise_gaussianå’Œadversarial_pgdçš„åŒºåˆ†ç‰¹å¾
ç›®æ ‡ï¼šåˆ†æå…³é”®ç‰¹å¾çš„åˆ†å¸ƒå·®å¼‚ï¼Œå‘ç°æ–°çš„ç‰¹å¾ç»„åˆï¼Œæå‡åˆ†ç±»æ€§èƒ½
"""
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import glob
import yaml
import sys
from datetime import datetime
from scipy import stats
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class FeatureAnalyzer:
    """ç‰¹å¾åˆ†æå™¨ç±»"""
    
    def __init__(self, results_dir):
        self.results_dir = results_dir
        self.analysis_results = {}
        
    def clean_data(self, data):
        """æ¸…ç†æ•°æ®ä¸­çš„æ— ç©·å¤§å€¼å’Œå¼‚å¸¸å€¼"""
        print("æ¸…ç†æ•°æ®ä¸­çš„æ— ç©·å¤§å€¼å’Œå¼‚å¸¸å€¼...")
        
        # è·å–æ•°å€¼åˆ—
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        if 'vulnerability_type' in numeric_columns:
            numeric_columns = numeric_columns.drop('vulnerability_type')
        
        # å¤„ç†æ— ç©·å¤§å€¼
        data_cleaned = data.copy()
        for col in numeric_columns:
            # æ›¿æ¢æ— ç©·å¤§å€¼
            data_cleaned[col] = data_cleaned[col].replace([np.inf, -np.inf], np.nan)
            
            # è®¡ç®—åˆ†ä½æ•°æ¥è¯†åˆ«å¼‚å¸¸å€¼
            Q1 = data_cleaned[col].quantile(0.01)  # 1%åˆ†ä½æ•°
            Q3 = data_cleaned[col].quantile(0.99)  # 99%åˆ†ä½æ•°
            IQR = Q3 - Q1
            
            # å°†å¼‚å¸¸å€¼æ›¿æ¢ä¸ºåˆ†ä½æ•°è¾¹ç•Œå€¼
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            data_cleaned[col] = data_cleaned[col].clip(lower=lower_bound, upper=upper_bound)
            
            # ç”¨ä¸­ä½æ•°å¡«å……NaNå€¼
            if data_cleaned[col].isna().sum() > 0:
                median_val = data_cleaned[col].median()
                data_cleaned[col].fillna(median_val, inplace=True)
        
        print(f"âœ… æ•°æ®æ¸…ç†å®Œæˆï¼Œå¤„ç†äº† {len(numeric_columns)} ä¸ªæ•°å€¼åˆ—")
        return data_cleaned
        
    def analyze_feature_distributions(self, data, label_encoder):
        """åˆ†æå…³é”®ç‰¹å¾åœ¨ä¸åŒç±»åˆ«ä¸Šçš„åˆ†å¸ƒå·®å¼‚"""
        print("=== åˆ†æå…³é”®ç‰¹å¾åˆ†å¸ƒå·®å¼‚ ===")
        
        # è·å–å…³é”®ç‰¹å¾ï¼ˆåŸºäºä¹‹å‰çš„é‡è¦æ€§åˆ†æï¼‰
        key_features = ['ratio_zscore', 'high_freq_ratio', 'std_dev_diff', 
                       'correlation', 'energy', 'homogeneity', 'contrast', 'll_distortion']
        
        # è¿‡æ»¤å‡ºå­˜åœ¨çš„ç‰¹å¾
        existing_features = [f for f in key_features if f in data.columns]
        print(f"åˆ†æçš„ç‰¹å¾: {existing_features}")
        
        # åˆ›å»ºåˆ†å¸ƒå¯¹æ¯”å›¾
        self._create_distribution_comparison(data, existing_features, label_encoder)
        
        # è®¡ç®—ç»Ÿè®¡å·®å¼‚
        self._calculate_statistical_differences(data, existing_features, label_encoder)
        
        return existing_features
    
    def _create_distribution_comparison(self, data, features, label_encoder):
        """åˆ›å»ºç‰¹å¾åˆ†å¸ƒå¯¹æ¯”å›¾"""
        print("åˆ›å»ºç‰¹å¾åˆ†å¸ƒå¯¹æ¯”å›¾...")
        
        # è·å–ç±»åˆ«æ ‡ç­¾
        classes = label_encoder.classes_
        target_classes = ['noise_gaussian', 'adversarial_pgd']
        
        # ä¸ºæ¯ä¸ªç‰¹å¾åˆ›å»ºåˆ†å¸ƒå¯¹æ¯”å›¾
        for feature in features:
            if feature not in data.columns:
                continue
                
            # åˆ›å»ºå­å›¾
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # å­å›¾1ï¼šç®±çº¿å›¾å¯¹æ¯”
            plot_data = []
            labels = []
            colors = []
            
            for i, class_name in enumerate(target_classes):
                if class_name in classes:
                    class_idx = list(classes).index(class_name)
                    class_data = data[data['vulnerability_type'] == class_name][feature]
                    if len(class_data) > 0:
                        plot_data.append(class_data)
                        labels.append(class_name)
                        colors.append(['#FF6B6B', '#4ECDC4'][i])
            
            if plot_data:
                bp = ax1.boxplot(plot_data, labels=labels, patch_artist=True)
                for patch, color in zip(bp['boxes'], colors):
                    patch.set_facecolor(color)
                    patch.set_alpha(0.7)
                
                ax1.set_title(f'{feature} - ç®±çº¿å›¾å¯¹æ¯”', fontsize=14, fontweight='bold')
                ax1.set_ylabel('ç‰¹å¾å€¼', fontsize=12)
                ax1.grid(True, alpha=0.3)
            
            # å­å›¾2ï¼šå¯†åº¦å›¾å¯¹æ¯”
            for i, class_name in enumerate(target_classes):
                if class_name in classes:
                    class_data = data[data['vulnerability_type'] == class_name][feature]
                    if len(class_data) > 0:
                        ax2.hist(class_data, bins=30, alpha=0.7, density=True, 
                                label=class_name, color=['#FF6B6B', '#4ECDC4'][i])
            
            ax2.set_title(f'{feature} - å¯†åº¦åˆ†å¸ƒå¯¹æ¯”', fontsize=14, fontweight='bold')
            ax2.set_xlabel('ç‰¹å¾å€¼', fontsize=12)
            ax2.set_ylabel('å¯†åº¦', fontsize=12)
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(os.path.join(self.results_dir, f'distribution_{feature}.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"âœ… åˆ†å¸ƒå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {self.results_dir}")
    
    def _calculate_statistical_differences(self, data, features, label_encoder):
        """è®¡ç®—ç»Ÿè®¡å·®å¼‚æŒ‡æ ‡"""
        print("è®¡ç®—ç»Ÿè®¡å·®å¼‚æŒ‡æ ‡...")
        
        classes = label_encoder.classes_
        target_classes = ['noise_gaussian', 'adversarial_pgd']
        
        stats_results = []
        
        for feature in features:
            if feature not in data.columns:
                continue
                
            feature_stats = {'feature': feature}
            
            for class_name in target_classes:
                if class_name in classes:
                    class_data = data[data['vulnerability_type'] == class_name][feature]
                    if len(class_data) > 0:
                        feature_stats[f'{class_name}_mean'] = class_data.mean()
                        feature_stats[f'{class_name}_std'] = class_data.std()
                        feature_stats[f'{class_name}_median'] = class_data.median()
                        feature_stats[f'{class_name}_q25'] = class_data.quantile(0.25)
                        feature_stats[f'{class_name}_q75'] = class_data.quantile(0.75)
            
            # è®¡ç®—ä¸¤ä¸ªç±»åˆ«ä¹‹é—´çš„å·®å¼‚
            if (f'noise_gaussian_mean' in feature_stats and 
                f'adversarial_pgd_mean' in feature_stats):
                
                # å‡å€¼å·®å¼‚
                mean_diff = feature_stats['adversarial_pgd_mean'] - feature_stats['noise_gaussian_mean']
                feature_stats['mean_difference'] = mean_diff
                
                # ç›¸å¯¹å·®å¼‚
                if feature_stats['noise_gaussian_mean'] != 0:
                    relative_diff = abs(mean_diff) / abs(feature_stats['noise_gaussian_mean'])
                    feature_stats['relative_difference'] = relative_diff
                
                # è¿›è¡Œtæ£€éªŒ
                noise_data = data[data['vulnerability_type'] == 'noise_gaussian'][feature]
                adv_data = data[data['vulnerability_type'] == 'adversarial_pgd'][feature]
                
                if len(noise_data) > 0 and len(adv_data) > 0:
                    t_stat, p_value = stats.ttest_ind(noise_data, adv_data)
                    feature_stats['t_statistic'] = t_stat
                    feature_stats['p_value'] = p_value
                    feature_stats['significant'] = p_value < 0.05
            
            stats_results.append(feature_stats)
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats_df = pd.DataFrame(stats_results)
        stats_df.to_csv(os.path.join(self.results_dir, 'feature_statistical_analysis.csv'), index=False)
        
        # æ˜¾ç¤ºæ˜¾è‘—å·®å¼‚çš„ç‰¹å¾
        significant_features = stats_df[stats_df.get('significant', False) == True]
        print(f"\nğŸ“Š ç»Ÿè®¡æ˜¾è‘—å·®å¼‚çš„ç‰¹å¾ ({len(significant_features)} ä¸ª):")
        for _, row in significant_features.iterrows():
            print(f"   {row['feature']}: på€¼={row['p_value']:.6f}, tç»Ÿè®¡é‡={row['t_statistic']:.3f}")
        
        self.analysis_results['statistical_analysis'] = stats_df
        return stats_df
    
    def create_new_features(self, data):
        """åˆ›å»ºæ–°çš„ç‰¹å¾ç»„åˆ"""
        print("\n=== åˆ›å»ºæ–°ç‰¹å¾ ===")
        
        new_features = {}
        
        # 1. æ¯”ç‡ç‰¹å¾
        print("åˆ›å»ºæ¯”ç‡ç‰¹å¾...")
        if 'ratio_zscore' in data.columns and 'std_dev_diff' in data.columns:
            new_features['ratio_std_ratio'] = data['ratio_zscore'] / (data['std_dev_diff'] + 1e-8)
        
        if 'high_freq_ratio' in data.columns and 'energy' in data.columns:
            new_features['freq_energy_ratio'] = data['high_freq_ratio'] / (data['energy'] + 1e-8)
        
        # 2. å·®å¼‚ç‰¹å¾
        print("åˆ›å»ºå·®å¼‚ç‰¹å¾...")
        if 'ratio_zscore' in data.columns and 'high_freq_ratio' in data.columns:
            new_features['ratio_freq_diff'] = data['ratio_zscore'] - data['high_freq_ratio']
        
        if 'std_dev_diff' in data.columns and 'correlation' in data.columns:
            new_features['std_corr_diff'] = data['std_dev_diff'] - data['correlation']
        
        # 3. ä¹˜ç§¯ç‰¹å¾
        print("åˆ›å»ºä¹˜ç§¯ç‰¹å¾...")
        if 'ratio_zscore' in data.columns and 'std_dev_diff' in data.columns:
            new_features['ratio_std_product'] = data['ratio_zscore'] * data['std_dev_diff']
        
        if 'high_freq_ratio' in data.columns and 'energy' in data.columns:
            new_features['freq_energy_product'] = data['high_freq_ratio'] * data['energy']
        
        # 4. å¤šé¡¹å¼ç‰¹å¾
        print("åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾...")
        if 'ratio_zscore' in data.columns:
            new_features['ratio_zscore_squared'] = data['ratio_zscore'] ** 2
        
        if 'high_freq_ratio' in data.columns:
            new_features['high_freq_ratio_squared'] = data['high_freq_ratio'] ** 2
        
        # 5. ç»Ÿè®¡ç‰¹å¾
        print("åˆ›å»ºç»Ÿè®¡ç‰¹å¾...")
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        numeric_columns = [col for col in numeric_columns if col != 'vulnerability_type']
        
        if len(numeric_columns) > 0:
            # è¡Œå‡å€¼
            new_features['row_mean'] = data[numeric_columns].mean(axis=1)
            # è¡Œæ ‡å‡†å·®
            new_features['row_std'] = data[numeric_columns].std(axis=1)
            # è¡Œæœ€å¤§å€¼
            new_features['row_max'] = data[numeric_columns].max(axis=1)
            # è¡Œæœ€å°å€¼
            new_features['row_min'] = data[numeric_columns].min(axis=1)
            # è¡ŒèŒƒå›´
            new_features['row_range'] = new_features['row_max'] - new_features['row_min']
        
        print(f"âœ… åˆ›å»ºäº† {len(new_features)} ä¸ªæ–°ç‰¹å¾")
        
        # å°†æ–°ç‰¹å¾æ·»åŠ åˆ°æ•°æ®ä¸­
        for feature_name, feature_values in new_features.items():
            data[feature_name] = feature_values
        
        return data, new_features
    
    def evaluate_new_features(self, data, label_encoder):
        """è¯„ä¼°æ–°ç‰¹å¾çš„é‡è¦æ€§"""
        print("\n=== è¯„ä¼°æ–°ç‰¹å¾é‡è¦æ€§ ===")
        
        # å‡†å¤‡æ•°æ®
        X = data.drop('vulnerability_type', axis=1)
        y = label_encoder.transform(data['vulnerability_type'])
        
        # å†æ¬¡æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«æ— ç©·å¤§å€¼
        print("æ£€æŸ¥æ•°æ®ä¸­çš„æ— ç©·å¤§å€¼...")
        if np.any(np.isinf(X.values)) or np.any(np.isnan(X.values)):
            print("è­¦å‘Šï¼šæ•°æ®ä¸­ä»åŒ…å«æ— ç©·å¤§å€¼æˆ–NaNå€¼ï¼Œè¿›è¡Œæœ€ç»ˆæ¸…ç†...")
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(X.median())
        
        print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
        print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
        
        # ä½¿ç”¨éšæœºæ£®æ—è¯„ä¼°ç‰¹å¾é‡è¦æ€§
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'Feature_Name': X.columns,
            'Importance': rf.feature_importances_
        }).sort_values('Importance', ascending=False)
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        feature_importance.to_csv(os.path.join(self.results_dir, 'new_feature_importance.csv'), index=False)
        
        # æ˜¾ç¤ºå‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        print("\nğŸ“Š æ–°ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ (Top 20):")
        for i, (_, row) in enumerate(feature_importance.head(20).iterrows()):
            print(f"   {i+1:2d}. {row['Feature_Name']:<25} : {row['Importance']:.4f}")
        
        # åˆ›å»ºæ–°ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
        self._create_new_feature_importance_plot(feature_importance)
        
        return feature_importance
    
    def _create_new_feature_importance_plot(self, feature_importance):
        """åˆ›å»ºæ–°ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–"""
        plt.figure(figsize=(16, 10))
        
        # é€‰æ‹©å‰25ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        top_25 = feature_importance.head(25)
        
        # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
        bars = plt.barh(range(len(top_25)), top_25['Importance'], 
                        color=plt.cm.viridis(np.linspace(0, 1, len(top_25))))
        
        plt.yticks(range(len(top_25)), top_25['Feature_Name'], fontsize=10)
        
        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, importance) in enumerate(zip(bars, top_25['Importance'])):
            plt.text(importance + 0.001, i, f'{importance:.4f}', 
                    va='center', fontsize=9, fontweight='bold')
        
        plt.xlabel('ç‰¹å¾é‡è¦æ€§å¾—åˆ†', fontsize=12, fontweight='bold')
        plt.title('æ–°ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ - åŒ…å«æ–°åˆ›å»ºçš„ç‰¹å¾', fontsize=14, fontweight='bold')
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plt.savefig(os.path.join(self.results_dir, 'new_feature_importance_ranking.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_feature_correlation_analysis(self, data):
        """åˆ›å»ºç‰¹å¾ç›¸å…³æ€§åˆ†æ"""
        print("\n=== ç‰¹å¾ç›¸å…³æ€§åˆ†æ ===")
        
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_data = data.select_dtypes(include=[np.number])
        if 'vulnerability_type' in numeric_data.columns:
            numeric_data = numeric_data.drop('vulnerability_type', axis=1)
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = numeric_data.corr()
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(20, 16))
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        
        sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.3f', 
                   cmap='RdBu_r', center=0, square=True, linewidths=0.5)
        
        plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plt.savefig(os.path.join(self.results_dir, 'feature_correlation_heatmap.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix.to_csv(os.path.join(self.results_dir, 'feature_correlation_matrix.csv'))
        
        # æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„ç‰¹å¾å¯¹
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.8:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                    high_corr_pairs.append({
                        'feature1': correlation_matrix.columns[i],
                        'feature2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        if high_corr_pairs:
            print(f"\nğŸ” å‘ç° {len(high_corr_pairs)} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾:")
            for pair in high_corr_pairs:
                print(f"   {pair['feature1']} <-> {pair['feature2']} : {pair['correlation']:.3f}")
        
        return correlation_matrix
    
    def create_dimensionality_reduction_analysis(self, data, label_encoder):
        """åˆ›å»ºé™ç»´åˆ†æ"""
        print("\n=== é™ç»´åˆ†æ ===")
        
        # å‡†å¤‡æ•°æ®
        X = data.drop('vulnerability_type', axis=1)
        y = label_encoder.transform(data['vulnerability_type'])
        
        # å†æ¬¡æ£€æŸ¥æ•°æ®
        if np.any(np.isinf(X.values)) or np.any(np.isnan(X.values)):
            print("è­¦å‘Šï¼šé™ç»´åˆ†æå‰è¿›è¡Œæ•°æ®æ¸…ç†...")
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(X.median())
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # PCAåˆ†æ
        print("è¿›è¡ŒPCAåˆ†æ...")
        pca = PCA(n_components=min(10, X.shape[1]))
        X_pca = pca.fit_transform(X_scaled)
        
        # åˆ›å»ºPCAå¯è§†åŒ–
        self._create_pca_visualization(X_pca, y, label_encoder, pca)
        
        # t-SNEåˆ†æ
        print("è¿›è¡Œt-SNEåˆ†æ...")
        if X_scaled.shape[1] > 50:  # å¦‚æœç‰¹å¾å¤ªå¤šï¼Œå…ˆé™ç»´
            X_reduced = PCA(n_components=50).fit_transform(X_scaled)
        else:
            X_reduced = X_scaled
        
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_reduced)//4))
        X_tsne = tsne.fit_transform(X_reduced)
        
        # åˆ›å»ºt-SNEå¯è§†åŒ–
        self._create_tsne_visualization(X_tsne, y, label_encoder)
        
        return pca, tsne
    
    def _create_pca_visualization(self, X_pca, y, label_encoder, pca):
        """åˆ›å»ºPCAå¯è§†åŒ–"""
        plt.figure(figsize=(15, 6))
        
        # å­å›¾1ï¼šå‰ä¸¤ä¸ªä¸»æˆåˆ†çš„æ•£ç‚¹å›¾
        ax1 = plt.subplot(1, 2, 1)
        classes = label_encoder.classes_
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        
        for i, class_name in enumerate(classes):
            if i < len(colors):
                mask = y == i
                ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                           c=colors[i], label=class_name, alpha=0.7, s=30)
        
        ax1.set_xlabel(f'ç¬¬ä¸€ä¸»æˆåˆ† (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[0]:.3f})')
        ax1.set_ylabel(f'ç¬¬äºŒä¸»æˆåˆ† (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[1]:.3f})')
        ax1.set_title('PCAé™ç»´ç»“æœ - å‰ä¸¤ä¸ªä¸»æˆåˆ†')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å­å›¾2ï¼šè§£é‡Šæ–¹å·®æ¯”ä¾‹
        ax2 = plt.subplot(1, 2, 2)
        explained_variance_ratio = pca.explained_variance_ratio_
        cumulative_variance_ratio = np.cumsum(explained_variance_ratio)
        
        ax2.plot(range(1, len(explained_variance_ratio) + 1), 
                cumulative_variance_ratio, 'bo-', linewidth=2, markersize=6)
        ax2.set_xlabel('ä¸»æˆåˆ†æ•°é‡')
        ax2.set_ylabel('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹')
        ax2.set_title('PCAè§£é‡Šæ–¹å·®ç´¯ç§¯æ¯”ä¾‹')
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95%æ–¹å·®')
        ax2.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'pca_analysis.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_tsne_visualization(self, X_tsne, y, label_encoder):
        """åˆ›å»ºt-SNEå¯è§†åŒ–"""
        plt.figure(figsize=(10, 8))
        
        classes = label_encoder.classes_
# åœ¨t-SNEå¯è§†åŒ–ä¸­ä½¿ç”¨ä¸åŒé¢œè‰²
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']  # çº¢è‰²ã€é’è‰²ã€è“è‰²
# ç¡®ä¿æ¯ä¸ªç±»åˆ«éƒ½æœ‰ç‹¬ç‰¹çš„é¢œè‰²
        
        for i, class_name in enumerate(classes):
            if i < len(colors):
                mask = y == i
                plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], 
                           c=colors[i], label=class_name, alpha=0.7, s=30)
        
        plt.xlabel('t-SNEç»´åº¦1')
        plt.ylabel('t-SNEç»´åº¦2')
        plt.title('t-SNEé™ç»´ç»“æœ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, 'tsne_analysis.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_analysis_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
        
        report_path = os.path.join(self.results_dir, 'feature_analysis_report.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("ç‰¹å¾åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("åˆ†ææ—¶é—´: " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            
            if 'statistical_analysis' in self.analysis_results:
                f.write("ç»Ÿè®¡åˆ†æç»“æœ:\n")
                f.write("-" * 30 + "\n")
                stats_df = self.analysis_results['statistical_analysis']
                significant_features = stats_df[stats_df.get('significant', False) == True]
                f.write(f"æ˜¾è‘—å·®å¼‚ç‰¹å¾æ•°é‡: {len(significant_features)}\n")
                for _, row in significant_features.iterrows():
                    f.write(f"  {row['feature']}: på€¼={row['p_value']:.6f}\n")
                f.write("\n")
            
            f.write("ç”Ÿæˆçš„æ–‡ä»¶:\n")
            f.write("-" * 30 + "\n")
            for file in os.listdir(self.results_dir):
                if file.endswith(('.png', '.csv')):
                    f.write(f"  {file}\n")
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=== è„šæœ¬ 05: æ·±åº¦ç‰¹å¾åˆ†æ ===")
    
    # åŠ è½½é…ç½®
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    try:
        # 1. åŠ è½½æ•°æ®
        print("åŠ è½½æ•°æ®...")
        runs_dir = config['output_paths']['runs_directory']
        candidate_csvs = glob.glob(os.path.join(runs_dir, '*/vulnerability_fingerprints.csv'))
        
        if not candidate_csvs:
            print("\né”™è¯¯ï¼šæ‰¾ä¸åˆ° vulnerability_fingerprints.csv æ–‡ä»¶ã€‚")
            sys.exit(1)
        
        fingerprint_file_path = max(candidate_csvs, key=os.path.getctime)
        print(f"ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶: {fingerprint_file_path}")
        
        data = pd.read_csv(fingerprint_file_path)
        print(f"æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ ·æœ¬")
        
        # 2. åˆ›å»ºç»“æœç›®å½• - ä¿®æ”¹ä¸ºruns/outputç›®å½•
        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        output_dir = os.path.join(runs_dir, 'output')  # æ–°å¢outputå­ç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºæ–°çš„ç‰¹å¾åˆ†æç›®å½•ï¼Œåç§°ä¸ä¹‹å‰åŒºåˆ†
        results_dir = os.path.join(output_dir, f"feature_analysis_{current_time}")
        os.makedirs(results_dir, exist_ok=True)
        print(f"åˆ›å»ºç»“æœç›®å½•: {results_dir}")
        
        # 3. åˆ›å»ºæ ‡ç­¾ç¼–ç å™¨
        label_encoder = LabelEncoder()
        label_encoder.fit(data['vulnerability_type'])
        
        # 4. åˆ›å»ºç‰¹å¾åˆ†æå™¨
        analyzer = FeatureAnalyzer(results_dir)
        
        # 5. æ•°æ®æ¸…ç†
        print("\nå¼€å§‹æ•°æ®æ¸…ç†...")
        data_cleaned = analyzer.clean_data(data)
        
        # 6. æ‰§è¡Œåˆ†æ
        print("\nå¼€å§‹æ·±åº¦ç‰¹å¾åˆ†æ...")
        
        # åˆ†æç°æœ‰ç‰¹å¾åˆ†å¸ƒ
        existing_features = analyzer.analyze_feature_distributions(data_cleaned, label_encoder)
        
        # åˆ›å»ºæ–°ç‰¹å¾
        data_with_new_features, new_features = analyzer.create_new_features(data_cleaned)
        
        # è¯„ä¼°æ–°ç‰¹å¾
        feature_importance = analyzer.evaluate_new_features(data_with_new_features, label_encoder)
        
        # ç›¸å…³æ€§åˆ†æ
        correlation_matrix = analyzer.create_feature_correlation_analysis(data_with_new_features)
        
        # é™ç»´åˆ†æ
        pca, tsne = analyzer.create_dimensionality_reduction_analysis(data_with_new_features, label_encoder)
        
        # ç”ŸæˆæŠ¥å‘Š
        analyzer.generate_analysis_report()
        
        print(f"\nâœ… ç‰¹å¾åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {results_dir}")
        print("\nğŸ“Š ä¸»è¦å‘ç°:")
        print("   - ç‰¹å¾åˆ†å¸ƒå·®å¼‚åˆ†æ")
        print("   - æ–°ç‰¹å¾åˆ›å»ºå’Œè¯„ä¼°")
        print("   - ç‰¹å¾ç›¸å…³æ€§åˆ†æ")
        print("   - é™ç»´åˆ†æç»“æœ")
        
    except Exception as e:
        print(f"\nâŒ è„šæœ¬æ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
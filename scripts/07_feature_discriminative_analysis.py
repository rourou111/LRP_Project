#!/usr/bin/env python3
"""
è„šæœ¬ 07: ç‰¹å¾åŒºåˆ†åº¦æ·±åº¦æµ‹è¯„
ç›®æ ‡ï¼šä¸“é—¨æµ‹è¯„feature_extractor.pyä¸­å®šä¹‰çš„ç‰¹å¾çš„åŒºåˆ†èƒ½åŠ›
å¸®åŠ©å†³å®šæ˜¯å¦æ”¾å¼ƒç°æœ‰ç‰¹å¾ï¼Œè¿˜æ˜¯å¯ä»¥åŸºäºå®ƒä»¬ä¼˜åŒ–
"""
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import glob
import yaml
import sys
from datetime import datetime
from scipy import stats
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import f_classif, mutual_info_classif, SelectKBest
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class FeatureDiscriminativeAnalyzer:
    """ç‰¹å¾åŒºåˆ†åº¦åˆ†æå™¨"""
    
    def __init__(self, results_dir):
        self.results_dir = results_dir
        self.analysis_results = {}
        
    def comprehensive_feature_analysis(self, data, label_encoder):
        """å…¨é¢åˆ†æç‰¹å¾åŒºåˆ†åº¦"""
        print("=== å¼€å§‹ç‰¹å¾åŒºåˆ†åº¦æ·±åº¦æµ‹è¯„ ===")
        
        # 1. æ•°æ®è´¨é‡æ£€æŸ¥
        print("\n1. æ•°æ®è´¨é‡æ£€æŸ¥...")
        self._check_data_quality(data, label_encoder)
        
        # 2. æ•°æ®æ¸…ç†
        print("\n2. æ•°æ®æ¸…ç†...")
        data_cleaned = self._clean_data(data)
        
        # 3. ç‰¹å¾åˆ†ç±»åˆ†æ
        print("\n3. ç‰¹å¾åˆ†ç±»åˆ†æ...")
        feature_categories = self._categorize_features(data_cleaned)
        
        # 4. å•ç‰¹å¾åŒºåˆ†åº¦åˆ†æ
        print("\n4. å•ç‰¹å¾åŒºåˆ†åº¦åˆ†æ...")
        single_feature_analysis = self._analyze_single_features(data_cleaned, label_encoder)
        
        # 5. ç‰¹å¾ç»„åˆåˆ†æ
        print("\n5. ç‰¹å¾ç»„åˆåˆ†æ...")
        feature_combinations = self._analyze_feature_combinations(data_cleaned, label_encoder)
        
        # 6. ç‰¹å¾é‡è¦æ€§åˆ†æ
        print("\n6. ç‰¹å¾é‡è¦æ€§åˆ†æ...")
        feature_importance = self._analyze_feature_importance(data_cleaned, label_encoder)
        
        # 7. ç”Ÿæˆæµ‹è¯„æŠ¥å‘Š
        print("\n7. ç”Ÿæˆæµ‹è¯„æŠ¥å‘Š...")
        self._generate_discriminative_report(feature_categories, single_feature_analysis, 
                                          feature_combinations, feature_importance)
        
        return single_feature_analysis, feature_combinations, feature_importance
    
    def _clean_data(self, data):
        """æ¸…ç†æ•°æ®"""
        print("æ¸…ç†æ•°æ®...")
        
        data_cleaned = data.copy()
        
        # è·å–æ•°å€¼åˆ—
        numeric_columns = data_cleaned.select_dtypes(include=[np.number]).columns
        if 'vulnerability_type' in numeric_columns:
            numeric_columns = numeric_columns.drop('vulnerability_type')
        
        print(f"å¤„ç† {len(numeric_columns)} ä¸ªæ•°å€¼åˆ—...")
        
        for col in numeric_columns:
            # æ›¿æ¢æ— ç©·å¤§å€¼
            data_cleaned[col] = data_cleaned[col].replace([np.inf, -np.inf], np.nan)
            
            # å¤„ç†å¼‚å¸¸å€¼
            Q1 = data_cleaned[col].quantile(0.01)
            Q3 = data_cleaned[col].quantile(0.99)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            data_cleaned[col] = data_cleaned[col].clip(lower=lower_bound, upper=upper_bound)
            
            # å¡«å……NaNå€¼
            if data_cleaned[col].isna().sum() > 0:
                median_val = data_cleaned[col].median()
                data_cleaned[col].fillna(median_val, inplace=True)
        
        # æœ€ç»ˆæ£€æŸ¥
        remaining_nan = data_cleaned.isnull().sum().sum()
        remaining_inf = np.isinf(data_cleaned.select_dtypes(include=[np.number])).sum().sum()
        
        print(f"æ¸…ç†å®Œæˆ: å‰©ä½™NaNå€¼ {remaining_nan}, å‰©ä½™æ— ç©·å¤§å€¼ {remaining_inf}")
        
        return data_cleaned
    
    def _check_data_quality(self, data, label_encoder):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        print("æ£€æŸ¥æ•°æ®è´¨é‡...")
        
        quality_report = {}
        
        # æ ‡ç­¾åˆ†å¸ƒ
        label_counts = data['vulnerability_type'].value_counts()
        quality_report['label_distribution'] = label_counts.to_dict()
        
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {label_counts.to_dict()}")
        
        # ç¼ºå¤±å€¼
        missing_values = data.isnull().sum()
        quality_report['missing_values'] = missing_values.to_dict()
        
        print(f"ç¼ºå¤±å€¼ç»Ÿè®¡: {missing_values.sum()} ä¸ªæ€»ç¼ºå¤±å€¼")
        
        # æ— ç©·å¤§å€¼
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        if 'vulnerability_type' in numeric_cols:
            numeric_cols = numeric_cols.drop('vulnerability_type')
        
        inf_counts = {}
        for col in numeric_cols:
            inf_count = np.isinf(data[col]).sum()
            inf_counts[col] = inf_count
        
        quality_report['infinity_values'] = inf_counts
        total_inf = sum(inf_counts.values())
        print(f"æ— ç©·å¤§å€¼ç»Ÿè®¡: {total_inf} ä¸ªæ€»æ— ç©·å¤§å€¼")
        
        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        quality_df = pd.DataFrame(quality_report['infinity_values'], index=['infinity_count']).T
        quality_df.to_csv(os.path.join(self.results_dir, 'data_quality_report.csv'))
        
        self.data_quality_report = quality_report
        
        return quality_report
    
    def _categorize_features(self, data):
        """å¯¹ç‰¹å¾è¿›è¡Œåˆ†ç±»"""
        print("å¯¹ç‰¹å¾è¿›è¡Œåˆ†ç±»...")
        
        # åŸºäºfeature_extractor.pyä¸­çš„ç‰¹å¾å®šä¹‰è¿›è¡Œåˆ†ç±»
        feature_categories = {
            'KLæ•£åº¦ç‰¹å¾': ['kl_divergence_pos', 'kl_divergence_neg'],
            'ç›¸ä¼¼åº¦ç‰¹å¾': ['cosine_similarity'],
            'çº¹ç†ç‰¹å¾': ['contrast', 'homogeneity', 'energy', 'correlation'],
            'ç»Ÿè®¡ç‰¹å¾': ['std_dev_diff', 'kurtosis_diff'],
            'é¢‘åŸŸç‰¹å¾': ['high_freq_ratio'],
            'å°æ³¢ç‰¹å¾': ['dynamic_wavelet_ratio_change', 'll_distortion'],
            'å…¶ä»–ç‰¹å¾': ['ratio_zscore', 'super_fingerprint']
        }
        
        # æ£€æŸ¥å“ªäº›ç‰¹å¾å®é™…å­˜åœ¨äºæ•°æ®ä¸­
        existing_categories = {}
        for category, features in feature_categories.items():
            existing_features = [f for f in features if f in data.columns]
            if existing_features:
                existing_categories[category] = existing_features
        
        print(f"ç‰¹å¾åˆ†ç±»ç»“æœ:")
        for category, features in existing_categories.items():
            print(f"  {category}: {features}")
        
        # ä¿å­˜ç‰¹å¾åˆ†ç±»
        category_df = pd.DataFrame([
            {'Category': cat, 'Features': ', '.join(feats), 'Count': len(feats)}
            for cat, feats in existing_categories.items()
        ])
        category_df.to_csv(os.path.join(self.results_dir, 'feature_categories.csv'), index=False)
        
        return existing_categories
    
    def _analyze_single_features(self, data, label_encoder):
        """åˆ†æå•ç‰¹å¾çš„åŒºåˆ†åº¦"""
        print("åˆ†æå•ç‰¹å¾åŒºåˆ†åº¦...")
        
        X = data.drop('vulnerability_type', axis=1)
        y = label_encoder.transform(data['vulnerability_type'])
        
        # æœ€ç»ˆæ£€æŸ¥æ•°æ®
        if X.isnull().any().any() or np.isinf(X.values).any():
            print("è­¦å‘Šï¼šå•ç‰¹å¾åˆ†æå‰è¿›è¡Œæœ€ç»ˆæ•°æ®æ¸…ç†...")
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(X.median())
        
        # 1. Fç»Ÿè®¡é‡åˆ†æ
        f_scores, p_values = f_classif(X, y)
        
        # 2. äº’ä¿¡æ¯åˆ†æ
        mi_scores = mutual_info_classif(X, y, random_state=42)
        
        # 3. ç±»åˆ«é—´å·®å¼‚åˆ†æ
        class_differences = self._calculate_class_differences(data, label_encoder)
        
        # 4. åˆ›å»ºç»¼åˆåˆ†æDataFrame
        feature_analysis = pd.DataFrame({
            'Feature_Name': X.columns,
            'F_Score': f_scores,
            'P_Value': p_values,
            'Mutual_Info': mi_scores,
            'Significant_F': p_values < 0.05,
            'F_Rank': f_scores.argsort()[::-1].argsort() + 1,
            'MI_Rank': mi_scores.argsort()[::-1].argsort() + 1
        })
        
        # æ·»åŠ ç±»åˆ«å·®å¼‚ä¿¡æ¯
        for feature in X.columns:
            if feature in class_differences:
                feature_analysis.loc[feature_analysis['Feature_Name'] == feature, 'Cohens_d'] = \
                    class_differences[feature]['cohens_d']
                feature_analysis.loc[feature_analysis['Feature_Name'] == feature, 'Effect_Size'] = \
                    class_differences[feature]['effect_size']
                feature_analysis.loc[feature_analysis['Feature_Name'] == feature, 'Overlap'] = \
                    class_differences[feature]['overlap']
        
        # æŒ‰Fç»Ÿè®¡é‡æ’åº
        feature_analysis = feature_analysis.sort_values('F_Score', ascending=False)
        
        # ä¿å­˜ç»“æœ
        feature_analysis.to_csv(os.path.join(self.results_dir, 'single_feature_analysis.csv'), index=False)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nï¿½ï¿½ å•ç‰¹å¾åŒºåˆ†åº¦æ’è¡Œæ¦œ (Top 20, æŒ‰Fç»Ÿè®¡é‡):")
        for i, (_, row) in enumerate(feature_analysis.head(20).iterrows()):
            significance = "***" if row['Significant_F'] else ""
            cohens_d_info = f", d={row.get('Cohens_d', 'N/A'):.4f}" if 'Cohens_d' in row and not pd.isna(row['Cohens_d']) else ""
            print(f"   {i+1:2d}. {row['Feature_Name']:<25} : F={row['F_Score']:.4f}, "
                  f"p={row['P_Value']:.6f}, MI={row['Mutual_Info']:.4f}{cohens_d_info} {significance}")
        
        # åˆ›å»ºå¯è§†åŒ–
        self._create_single_feature_visualization(feature_analysis)
        
        return feature_analysis
    
    def _calculate_class_differences(self, data, label_encoder):
        """è®¡ç®—ç±»åˆ«é—´å·®å¼‚"""
        classes = label_encoder.classes_
        target_classes = ['noise_gaussian', 'adversarial_pgd']
        
        feature_analysis = {}
        
        for feature in data.columns:
            if feature == 'vulnerability_type':
                continue
                
            if data[feature].dtype in ['float64', 'int64']:
                class_stats = {}
                
                for class_name in target_classes:
                    if class_name in classes:
                        class_data = data[data['vulnerability_type'] == class_name][feature]
                        if len(class_data) > 0:
                            class_stats[class_name] = {
                                'mean': class_data.mean(),
                                'std': class_data.std(),
                                'median': class_data.median(),
                                'q25': class_data.quantile(0.25),
                                'q75': class_data.quantile(0.75)
                            }
                
                # è®¡ç®—ç±»åˆ«é—´å·®å¼‚
                if len(class_stats) == 2:
                    # Cohen's dæ•ˆåº”é‡
                    mean_diff = abs(class_stats[target_classes[0]]['mean'] - class_stats[target_classes[1]]['mean'])
                    pooled_std = np.sqrt((class_stats[target_classes[0]]['std']**2 + class_stats[target_classes[1]]['std']**2) / 2)
                    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0
                    
                    # é‡å åº¦
                    overlap = self._calculate_overlap(class_stats[target_classes[0]], class_stats[target_classes[1]])
                    
                    feature_analysis[feature] = {
                        'cohens_d': cohens_d,
                        'overlap': overlap,
                        'effect_size': self._classify_effect_size(cohens_d)
                    }
        
        return feature_analysis
    
    def _calculate_overlap(self, stats1, stats2):
        """è®¡ç®—é‡å åº¦"""
        range1 = (stats1['q25'], stats1['q75'])
        range2 = (stats2['q25'], stats2['q75'])
        
        overlap_start = max(range1[0], range2[0])
        overlap_end = min(range1[1], range2[1])
        
        if overlap_start >= overlap_end:
            return 0.0
        
        overlap_length = overlap_end - overlap_start
        total_length = (range1[1] - range1[0]) + (range2[1] - range2[0])
        
        return overlap_length / total_length if total_length > 0 else 0.0
    
    def _classify_effect_size(self, cohens_d):
        """åˆ†ç±»æ•ˆåº”é‡"""
        if cohens_d < 0.2:
            return "å¾®å°"
        elif cohens_d < 0.5:
            return "å°"
        elif cohens_d < 0.8:
            return "ä¸­ç­‰"
        else:
            return "å¤§"
    
    def _create_single_feature_visualization(self, feature_analysis):
        """åˆ›å»ºå•ç‰¹å¾åˆ†æå¯è§†åŒ–"""
        # 1. Fç»Ÿè®¡é‡ vs äº’ä¿¡æ¯æ•£ç‚¹å›¾
        plt.figure(figsize=(12, 8))
        
        plt.scatter(feature_analysis['F_Score'], feature_analysis['Mutual_Info'], 
                   alpha=0.7, s=50)
        
        # æ ‡æ³¨å‰10ä¸ªç‰¹å¾
        top_10 = feature_analysis.head(10)
        for _, row in top_10.iterrows():
            plt.annotate(row['Feature_Name'], 
                        (row['F_Score'], row['Mutual_Info']),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=8, alpha=0.8)
        
        plt.xlabel('Fç»Ÿè®¡é‡', fontsize=12, fontweight='bold')
        plt.ylabel('äº’ä¿¡æ¯', fontsize=12, fontweight='bold')
        plt.title('ç‰¹å¾åŒºåˆ†èƒ½åŠ›å¯¹æ¯”: Fç»Ÿè®¡é‡ vs äº’ä¿¡æ¯', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(os.path.join(self.results_dir, 'single_feature_discrimination.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Top 20ç‰¹å¾Fç»Ÿè®¡é‡æ¡å½¢å›¾
        plt.figure(figsize=(16, 10))
        
        top_20 = feature_analysis.head(20)
        bars = plt.barh(range(len(top_20)), top_20['F_Score'], 
                        color=plt.cm.viridis(np.linspace(0, 1, len(top_20))))
        
        plt.yticks(range(len(top_20)), top_20['Feature_Name'], fontsize=10)
        
        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, f_score) in enumerate(zip(bars, top_20['F_Score'])):
            plt.text(f_score + 0.001, i, f'{f_score:.3f}', 
                    va='center', fontsize=9, fontweight='bold')
        
        plt.xlabel('Fç»Ÿè®¡é‡', fontsize=12, fontweight='bold')
        plt.title('Top 20ç‰¹å¾åŒºåˆ†èƒ½åŠ› (Fç»Ÿè®¡é‡)', fontsize=14, fontweight='bold')
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        
        plt.savefig(os.path.join(self.results_dir, 'top_features_f_score.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _analyze_feature_combinations(self, data, label_encoder):
        """åˆ†æç‰¹å¾ç»„åˆ"""
        print("åˆ†æç‰¹å¾ç»„åˆ...")
        
        X = data.drop('vulnerability_type', axis=1)
        y = label_encoder.transform(data['vulnerability_type'])
        
        # ä½¿ç”¨Fç»Ÿè®¡é‡é€‰æ‹©Topç‰¹å¾
        selector = SelectKBest(f_classif, k=min(15, X.shape[1]))
        X_selected = selector.fit_transform(X, y)
        selected_features = X.columns[selector.get_support()].tolist()
        
        print(f"é€‰æ‹©çš„Top {len(selected_features)} ç‰¹å¾: {selected_features}")
        
        # åˆ†æç‰¹å¾ç»„åˆ
        combination_results = []
        
        # 2ç‰¹å¾ç»„åˆ
        print("åˆ†æ2ç‰¹å¾ç»„åˆ...")
        for i in range(len(selected_features)):
            for j in range(i+1, len(selected_features)):
                feat1, feat2 = selected_features[i], selected_features[j]
                
                X_combined = X[[feat1, feat2]]
                
                rf = RandomForestClassifier(n_estimators=50, random_state=42)
                cv_scores = cross_val_score(rf, X_combined, y, cv=5, scoring='accuracy')
                
                combination_results.append({
                    'Feature1': feat1,
                    'Feature2': feat2,
                    'Mean_Accuracy': cv_scores.mean(),
                    'Std_Accuracy': cv_scores.std(),
                    'Combination_Type': '2_features'
                })
        
        # 3ç‰¹å¾ç»„åˆ
        print("åˆ†æ3ç‰¹å¾ç»„åˆ...")
        top_2_combinations = sorted(combination_results, key=lambda x: x['Mean_Accuracy'], reverse=True)[:10]
        
        for combo in top_2_combinations:
            feat1, feat2 = combo['Feature1'], combo['Feature2']
            
            for feat3 in selected_features:
                if feat3 not in [feat1, feat2]:
                    X_combined = X[[feat1, feat2, feat3]]
                    
                    rf = RandomForestClassifier(n_estimators=50, random_state=42)
                    cv_scores = cross_val_score(rf, X_combined, y, cv=5, scoring='accuracy')
                    
                    combination_results.append({
                        'Feature1': feat1,
                        'Feature2': feat2,
                        'Feature3': feat3,
                        'Mean_Accuracy': cv_scores.mean(),
                        'Std_Accuracy': cv_scores.std(),
                        'Combination_Type': '3_features'
                    })
        
        # ä¿å­˜ç»“æœ
        combination_df = pd.DataFrame(combination_results)
        combination_df.to_csv(os.path.join(self.results_dir, 'feature_combination_analysis.csv'), index=False)
        
        # æ˜¾ç¤ºæœ€ä½³ç»„åˆ
        print(f"\nğŸ“Š æœ€ä½³ç‰¹å¾ç»„åˆ (Top 10):")
        best_combinations = combination_df.sort_values('Mean_Accuracy', ascending=False).head(10)
        for i, (_, row) in enumerate(best_combinations.iterrows()):
            if row['Combination_Type'] == '2_features':
                print(f"   {i+1:2d}. {row['Feature1']} + {row['Feature2']} : "
                      f"å‡†ç¡®ç‡={row['Mean_Accuracy']:.4f} Â± {row['Std_Accuracy']:.4f}")
            else:
                print(f"   {i+1:2d}. {row['Feature1']} + {row['Feature2']} + {row['Feature3']} : "
                      f"å‡†ç¡®ç‡={row['Mean_Accuracy']:.4f} Â± {row['Std_Accuracy']:.4f}")
        
        return combination_results
    
    def _analyze_feature_importance(self, data, label_encoder):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        print("åˆ†æç‰¹å¾é‡è¦æ€§...")
        
        X = data.drop('vulnerability_type', axis=1)
        y = label_encoder.transform(data['vulnerability_type'])
        
        # æœ€ç»ˆæ£€æŸ¥æ•°æ®
        if X.isnull().any().any() or np.isinf(X.values).any():
            print("è­¦å‘Šï¼šç‰¹å¾é‡è¦æ€§åˆ†æå‰è¿›è¡Œæœ€ç»ˆæ•°æ®æ¸…ç†...")
            X = X.replace([np.inf, -np.inf], np.nan)
            X = X.fillna(X.median())
        
        # ä½¿ç”¨éšæœºæ£®æ—è¯„ä¼°ç‰¹å¾é‡è¦æ€§
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'Feature_Name': X.columns,
            'Importance': rf.feature_importances_
        }).sort_values('Importance', ascending=False)
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        feature_importance.to_csv(os.path.join(self.results_dir, 'feature_importance_analysis.csv'), index=False)
        
        # æ˜¾ç¤ºå‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        print("\nï¿½ï¿½ ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ (Top 20):")
        for i, (_, row) in enumerate(feature_importance.head(20).iterrows()):
            print(f"   {i+1:2d}. {row['Feature_Name']:<25} : {row['Importance']:.4f}")
        
        # åˆ›å»ºå¯è§†åŒ–
        self._create_feature_importance_visualization(feature_importance)
        
        return feature_importance
    
    def _create_feature_importance_visualization(self, feature_importance):
        """åˆ›å»ºç‰¹å¾é‡è¦æ€§å¯è§†åŒ–"""
        plt.figure(figsize=(16, 10))
        
        # é€‰æ‹©å‰25ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        top_25 = feature_importance.head(25)
        
        # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
        bars = plt.barh(range(len(top_25)), top_25['Importance'], 
                        color=plt.cm.viridis(np.linspace(0, 1, len(top_25))))
        
        plt.yticks(range(len(top_25)), top_25['Feature_Name'], fontsize=10)
        
        # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, importance) in enumerate(zip(bars, top_25['Importance'])):
            plt.text(importance + 0.001, i, f'{importance:.4f}', 
                    va='center', fontsize=9, fontweight='bold')
        
        plt.xlabel('ç‰¹å¾é‡è¦æ€§å¾—åˆ†', fontsize=12, fontweight='bold')
        plt.title('ç‰¹å¾é‡è¦æ€§æ’è¡Œæ¦œ - åŸºäºéšæœºæ£®æ—', fontsize=14, fontweight='bold')
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plt.savefig(os.path.join(self.results_dir, 'feature_importance_ranking.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_discriminative_report(self, feature_categories, single_feature_analysis, 
                                      feature_combinations, feature_importance):
        """ç”ŸæˆåŒºåˆ†åº¦æµ‹è¯„æŠ¥å‘Š"""
        print("ç”ŸæˆåŒºåˆ†åº¦æµ‹è¯„æŠ¥å‘Š...")
        
        report_path = os.path.join(self.results_dir, 'feature_discriminative_report.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("ç‰¹å¾åŒºåˆ†åº¦æ·±åº¦æµ‹è¯„æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("æµ‹è¯„æ—¶é—´: " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            
            # ç‰¹å¾åˆ†ç±»æ€»ç»“
            f.write("1. ç‰¹å¾åˆ†ç±»æ€»ç»“\n")
            f.write("-" * 40 + "\n")
            for category, features in feature_categories.items():
                f.write(f"{category}: {len(features)} ä¸ªç‰¹å¾\n")
                f.write(f"  ç‰¹å¾åˆ—è¡¨: {', '.join(features)}\n\n")
            
            # å•ç‰¹å¾åŒºåˆ†åº¦æ€»ç»“
            f.write("2. å•ç‰¹å¾åŒºåˆ†åº¦æ€»ç»“\n")
            f.write("-" * 40 + "\n")
            if not single_feature_analysis.empty:
                significant_features = single_feature_analysis[single_feature_analysis['Significant_F'] == True]
                f.write(f"æ€»ç‰¹å¾æ•°: {len(single_feature_analysis)}\n")
                f.write(f"ç»Ÿè®¡æ˜¾è‘—ç‰¹å¾æ•°: {len(significant_features)}\n")
                f.write(f"æ˜¾è‘—ç‰¹å¾æ¯”ä¾‹: {len(significant_features)/len(single_feature_analysis)*100:.2f}%\n\n")
                
                f.write("Top 10ç‰¹å¾ (æŒ‰Fç»Ÿè®¡é‡):\n")
                for i, (_, row) in enumerate(single_feature_analysis.head(10).iterrows()):
                    f.write(f"   {i+1:2d}. {row['Feature_Name']:<25} : F={row['F_Score']:.4f}, "
                           f"p={row['P_Value']:.6f}\n")
                f.write("\n")
            
            # ç‰¹å¾ç»„åˆæ€»ç»“
            f.write("3. ç‰¹å¾ç»„åˆæ€»ç»“\n")
            f.write("-" * 40 + "\n")
            if feature_combinations:
                best_2_feature = max([c for c in feature_combinations if c['Combination_Type'] == '2_features'], 
                                   key=lambda x: x['Mean_Accuracy'])
                best_3_feature = max([c for c in feature_combinations if c['Combination_Type'] == '3_features'], 
                                   key=lambda x: x['Mean_Accuracy'])
                
                f.write(f"æœ€ä½³2ç‰¹å¾ç»„åˆ: {best_2_feature['Feature1']} + {best_2_feature['Feature2']}\n")
                f.write(f"å‡†ç¡®ç‡: {best_2_feature['Mean_Accuracy']:.4f} Â± {best_2_feature['Std_Accuracy']:.4f}\n\n")
                
                f.write(f"æœ€ä½³3ç‰¹å¾ç»„åˆ: {best_3_feature['Feature1']} + {best_3_feature['Feature2']} + {best_3_feature['Feature3']}\n")
                f.write(f"å‡†ç¡®ç‡: {best_3_feature['Mean_Accuracy']:.4f} Â± {best_3_feature['Std_Accuracy']:.4f}\n\n")
            
            # ç‰¹å¾é‡è¦æ€§æ€»ç»“
            f.write("4. ç‰¹å¾é‡è¦æ€§æ€»ç»“\n")
            f.write("-" * 40 + "\n")
            if not feature_importance.empty:
                f.write("Top 10ç‰¹å¾ (æŒ‰é‡è¦æ€§):\n")
                for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
                    f.write(f"   {i+1:2d}. {row['Feature_Name']:<25} : {row['Importance']:.4f}\n")
                f.write("\n")
            
            # ç»“è®ºå’Œå»ºè®®
            f.write("5. ç»“è®ºå’Œå»ºè®®\n")
            f.write("-" * 40 + "\n")
            
            # åŸºäºç»“æœç»™å‡ºå»ºè®®
            if single_feature_analysis.empty:
                f.write("æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç»™å‡ºæœ‰æ•ˆç»“è®ºã€‚\n")
            else:
                significant_count = len(single_feature_analysis[single_feature_analysis['Significant_F'] == True])
                total_features = len(single_feature_analysis)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤§æ•ˆåº”é‡ç‰¹å¾
                large_effect_features = 0
                if 'Cohens_d' in single_feature_analysis.columns:
                    large_effect_features = len(single_feature_analysis[
                        single_feature_analysis['Cohens_d'] > 0.8
                    ])
                
                if significant_count > total_features * 0.8 and large_effect_features > 3:
                    f.write("âœ… ç°æœ‰ç‰¹å¾é›†åˆå…·æœ‰è¾ƒå¥½çš„åŒºåˆ†èƒ½åŠ›ï¼Œå»ºè®®:\n")
                    f.write("   - ä¿ç•™ç°æœ‰ç‰¹å¾ï¼Œè¿›è¡Œä¼˜åŒ–ç»„åˆ\n")
                    f.write("   - ä½¿ç”¨æœ€ä½³ç‰¹å¾ç»„åˆè¿›è¡Œæ¨¡å‹è®­ç»ƒ\n")
                    f.write("   - è°ƒæ•´æ¨¡å‹å‚æ•°å’Œé˜ˆå€¼\n")
                    f.write("   - è€ƒè™‘ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–\n")
                elif significant_count > total_features * 0.5 and large_effect_features > 0:
                    f.write("âš ï¸  ç°æœ‰ç‰¹å¾é›†åˆå…·æœ‰æœ‰é™çš„åŒºåˆ†èƒ½åŠ›ï¼Œå»ºè®®:\n")
                    f.write("   - ä¿ç•™æœ‰åŒºåˆ†èƒ½åŠ›çš„ç‰¹å¾\n")
                    f.write("   - å¯»æ‰¾æ–°çš„è¡¥å……ç‰¹å¾\n")
                    f.write("   - ä¼˜åŒ–ç‰¹å¾ç»„åˆç­–ç•¥\n")
                    f.write("   - è€ƒè™‘ç‰¹å¾è½¬æ¢å’Œç»„åˆ\n")
                else:
                    f.write("âŒ ç°æœ‰ç‰¹å¾é›†åˆåŒºåˆ†èƒ½åŠ›ä¸è¶³ï¼Œå»ºè®®:\n")
                    f.write("   - é‡æ–°å®¡è§†ç‰¹å¾è®¾è®¡\n")
                    f.write("   - å¯»æ‰¾æ–°çš„ç‰¹å¾æ¥æº\n")
                    f.write("   - è€ƒè™‘ä¸åŒçš„åˆ†ç±»ç­–ç•¥\n")
                    f.write("   - å¯èƒ½éœ€è¦é‡æ–°è®¾è®¡æ•´ä¸ªç‰¹å¾æå–æµç¨‹\n")
            
            f.write("\nç”Ÿæˆçš„æ–‡ä»¶:\n")
            f.write("-" * 40 + "\n")
            for file in os.listdir(self.results_dir):
                if file.endswith(('.png', '.csv', '.txt')):
                    f.write(f"  {file}\n")
        
        print(f"âœ… åŒºåˆ†åº¦æµ‹è¯„æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=== è„šæœ¬ 07: ç‰¹å¾åŒºåˆ†åº¦æ·±åº¦æµ‹è¯„ ===")
    
    # åŠ è½½é…ç½®
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    try:
        # 1. åŠ è½½æ•°æ®
        print("åŠ è½½æ•°æ®...")
        runs_dir = config['output_paths']['runs_directory']
        candidate_csvs = glob.glob(os.path.join(runs_dir, '*/vulnerability_fingerprints.csv'))
        
        if not candidate_csvs:
            print("\né”™è¯¯ï¼šæ‰¾ä¸åˆ° vulnerability_fingerprints.csv æ–‡ä»¶ã€‚")
            sys.exit(1)
        
        fingerprint_file_path = max(candidate_csvs, key=os.path.getctime)
        print(f"ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶: {fingerprint_file_path}")
        
        data = pd.read_csv(fingerprint_file_path)
        print(f"æˆåŠŸåŠ è½½ {len(data)} ä¸ªæ ·æœ¬")
        
        # 2. åˆ›å»ºç»“æœç›®å½•
        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        output_dir = os.path.join(runs_dir, 'output')
        os.makedirs(output_dir, exist_ok=True)
        
        results_dir = os.path.join(output_dir, f"feature_discriminative_analysis_{current_time}")
        os.makedirs(results_dir, exist_ok=True)
        print(f"åˆ›å»ºç»“æœç›®å½•: {results_dir}")
        
        # 3. åˆ›å»ºæ ‡ç­¾ç¼–ç å™¨
        label_encoder = LabelEncoder()
        label_encoder.fit(data['vulnerability_type'])
        
        # 4. åˆ›å»ºç‰¹å¾åŒºåˆ†åº¦åˆ†æå™¨
        analyzer = FeatureDiscriminativeAnalyzer(results_dir)
        
        # 5. æ‰§è¡Œå…¨é¢åˆ†æ
        print("\nå¼€å§‹ç‰¹å¾åŒºåˆ†åº¦æ·±åº¦æµ‹è¯„...")
        
        single_feature_analysis, feature_combinations, feature_importance = \
            analyzer.comprehensive_feature_analysis(data, label_encoder)
        
        print(f"\nâœ… ç‰¹å¾åŒºåˆ†åº¦æµ‹è¯„å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {results_dir}")
        print("\nğŸ“Š ä¸»è¦æµ‹è¯„å†…å®¹:")
        print("   - æ•°æ®è´¨é‡æ£€æŸ¥")
        print("   - ç‰¹å¾åˆ†ç±»åˆ†æ")
        print("   - å•ç‰¹å¾åŒºåˆ†åº¦åˆ†æ")
        print("   - ç‰¹å¾ç»„åˆåˆ†æ")
        print("   - ç‰¹å¾é‡è¦æ€§åˆ†æ")
        print("   - åŒºåˆ†åº¦æµ‹è¯„æŠ¥å‘Š")
        
    except Exception as e:
        print(f"\nâŒ è„šæœ¬æ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()